
# LLM合集-多模态

LLM合集-多模态模块构建了涵盖30+个前沿多模态大模型的完整技术生态，专注于视觉-语言、音频-语言等跨模态AI技术的创新应用。该模块系统性地整理了OpenAI GPT-4V、Google Gemini Vision、Anthropic Claude 3、Meta LLaVA系列等国际领先的视觉语言模型，以及阿里通义千问VL、百度文心一言4.0、腾讯混元多模态、字节豆包视觉版、智谱GLM-4V、月之暗面Kimi视觉等国产优秀多模态模型。技术特色涵盖了图像理解、视频分析、音频处理、3D感知等多维度感知能力，详细解析了Vision Transformer、CLIP、DALL-E、Stable Diffusion等核心技术架构，以及视觉编码器、跨模态注意力、多模态融合等关键技术机制。

模块深入介绍了图像描述生成、视觉问答、图文检索、视频理解、音频转录、语音合成等典型应用场景，以及多模态数据预处理、模型训练策略、推理优化、部署方案等工程化实践。内容还包括多模态评测基准（VQA、COCO Caption、BLIP）、开源项目生态（LLaVA、MiniGPT-4、InstructBLIP）、商业API服务、性能对比分析等实用资源，以及最新技术突破、应用创新、发展趋势等前沿洞察，帮助开发者构建具备丰富感知能力的下一代AI应用，实现文本、图像、音频、视频等多模态信息的智能理解和生成。

## 目录
1. 1.Nexus-Gen魔塔文生图
2. 1.Seed1.5-VL字节
3. 2.Matrix-Game空间大模型-昆仑万维
4. 2.Step1X-3D 阶跃星辰 3D生成模型
5. 2.bytedance ContentV文生视频
6. 2.字节BAGEL多模态模型
7. 2.谷歌系列
8. 2.谷歌系列/Graphiti-AI动态知识图谱
9. 2.通义万相-开源视频模型
10. 3.MoviiGen 1.1
11. 3.meta-vjepa世界模型
12. 4.3D生成模型Partcrafter
13. 4.MAGREF多主体视频生成框架-字节
14. 4.OmniAudio空间音频生成
15. 4.PlayDiffusion音频编辑
16. 4.SeedVR2-字节视频修复
17. 4.美团-肖像生成llia
18. 5.多人对话视频框架
19. 5.趣丸科技-人脸动画生成
21. Kwai Keye VL 快手
22. OmniAvatar浙大阿里视频生成

## Qwen-Image – 阿里通义千问开源的文生图模型

#### 简介
通义千问视觉基础模型（Qwen-Image）是由阿里云QwenLM团队开发的一款20亿参数的MMDiT（Multi-Modal Diffusion Transformer）图像基础模型。该模型在复杂的文本渲染和精准的图像编辑方面取得了显著进展，旨在提供高质量的图文生成与编辑能力。

![qwem-image.png](https://free.picui.cn/free/2025/08/06/6892c75fc0309.png)

![qwen-image1.png](https://free.picui.cn/free/2025/08/06/6892c7607b344.png)

#### 核心功能
*   **高保真文本渲染：** 能够在生成的图像中实现高精度的文本呈现，无论是英文字母还是中文字符，都能保持排版细节、布局一致性和上下文和谐性，实现文本与图像的无缝融合。
*   **精准图像编辑：** 提供强大的图像编辑能力，包括但不限于图像生成、内容估计、新视角合成和超分辨率等。
*   **复杂场景生成：** 支持根据复杂的文本描述生成视觉上连贯且高质量的图像。
*   **跨语言文本支持：** 能够处理并生成包含多种语言文本的图像。

#### 技术原理
Qwen-Image是一个基于MMDiT架构的20亿参数基础模型。MMDiT（Multi-Modal Diffusion Transformer）结合了扩散模型（Diffusion Model）的图像生成能力和Transformer架构处理序列数据的优势。其核心原理可能涉及：
*   **多模态融合：** 有效地将文本提示信息与视觉特征相结合，指导图像生成和编辑过程。
*   **扩散模型：** 逐步去噪生成图像，从而实现高保真和细节丰富的输出。
*   **Transformer结构：** 用于捕捉长距离依赖关系和处理复杂的语义信息，尤其在文本理解和图像内容布局方面发挥关键作用。
*   **参数量与训练：** 20亿参数表明模型规模庞大，通过大规模数据集训练，赋予其强大的泛化能力和对图像复杂性的理解。

#### 应用场景
*   **创意内容生成：** 广泛应用于广告、设计、媒体等领域，用于快速生成包含定制文本和视觉效果的图片。
*   **智能图像编辑：** 辅助专业设计师或普通用户进行图像的精细化修改、内容添加或修复。
*   **多语言本地化：** 帮助企业和创作者生成带有不同语言文本的图像，以适应全球市场需求。
*   **自动化设计工具：** 集成到自动化设计平台中，实现文本到图像的智能转换，提高工作效率。
*   **虚拟现实与游戏：** 用于快速生成场景、道具或角色纹理，包含特定文字元素。

Qwen-Image的项目地址
* GitHub仓库：https://github.com/QwenLM/Qwen-Image
* HuggingFace模型库：https://huggingface.co/Qwen/Qwen-Image
* 技术论文：https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf
* 在线体验Demo：https://huggingface.co/spaces/Qwen/Qwen-Image

# 1.Nexus-Gen魔塔文生图

#### 简介
Nexus - Gen是一个统一模型，将大语言模型（LLM）的语言推理能力与扩散模型的图像合成能力相结合，通过双阶段对齐训练过程，使模型具备处理图像理解、生成和编辑任务的综合能力。2025年5月27日使用BLIP - 3o - 60k数据集微调后，提升了图像生成对文本提示的鲁棒性。

![Snipaste_2025-07-19_15-18-13.png](https://free.picui.cn/free/2025/07/19/687b46f19ba1c.png)

#### 核心功能
- 图像理解：能对图像进行理解分析。
- 图像生成：依据文本提示生成高保真图像，可使用预填充自回归策略提升生成质量，还支持使用较少显存生成图像。
- 图像编辑：对已有图像进行编辑处理。

#### 技术原理
通过双阶段对齐训练过程，使大语言模型和扩散模型的嵌入空间对齐。一是自回归大语言模型学习基于多模态输入预测图像嵌入；二是视觉解码器训练从这些嵌入中重建高保真图像。训练大语言模型时，引入预填充自回归策略避免连续嵌入空间中误差积累导致的生成质量下降问题。

#### 应用场景
- 图像创作：根据文字描述生成艺术作品、设计素材等。
- 图像编辑：对照片、设计图等进行修改完善。
- 图像分析：辅助理解图像内容，如智能图像标注等。 


- [modelscope/Nexus-Gen](https://github.com/modelscope/Nexus-Gen)

# 1.Seed1.5-VL字节

#### 简介
Seed1.5-VL 是由字节跳动发布的一款视觉 - 语言基础模型，专注于提升多模态理解与推理能力。它采用 5.32 亿参数的视觉编码器和 200 亿激活参数的混合专家（MoE）大语言模型，在 60 项公开评测基准中的 38 项取得 SOTA 表现。

![Snipaste_2025-07-19_15-18-28.png](https://free.picui.cn/free/2025/07/19/687b46f1e04a7.png)

#### 核心功能
- **多模态理解**：支持图像、视频等多模态输入，能处理复杂推理、OCR、图表理解、视觉定位等任务。
- **智能体交互**：在 GUI 控制、游戏玩法等交互式智能体任务中表现出色。
- **格式转换**：可将 txt 文件转换为 epub 格式。

#### 技术原理
- **模型架构**：由 SeedViT 编码图像和视频、MLP 适配器投射视觉特征、大语言模型处理多模态输入并推理。
- **图像与视频处理**：支持多分辨率图像输入，用原生分辨率变换保留细节；视频处理采用动态帧分辨率采样策略，引入时间戳标记增强时间信息感知。
- **预训练**：使用 3 万亿源标记的预训练语料库，多数子类数据训练损失与训练标记数遵循幂律关系。
- **后训练**：结合拒绝采样和在线强化学习迭代更新，监督信号仅作用于最终输出结果。

#### 应用场景
- **内容识别**：识别图片中的地点、物体等。
- **视觉谜题解答**：根据表情符号等视觉元素联想电影等。
- **文件格式转换**：将 txt 文件转换为 epub 格式。
- **智能体操作**：进行 GUI 智能体任务，如设置软件全局热键等。 


- [ByteDance-Seed/Seed1.5-VL: Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks.](https://github.com/ByteDance-Seed/Seed1.5-VL)
- [字节跳动Seed](https://seed.bytedance.com/zh/tech/seed1_5_vl)

# 2.谷歌系列

#### 简介
Imagen图像生成模型，它是目前最佳的图像生成模型，可生成具有逼真细节、清晰画质、改进拼写和排版的图像，涵盖风景、人物、动物、漫画、包装等多种场景，能将用户的想象快速转化为生动的视觉呈现。谷歌DeepMind推出的视频生成模型Veo，包括最新的Veo 3及Veo 2的新创意功能。Veo 3具有更高的真实感、更好的指令遵循能力，能原生生成音频；Veo 2在控制、一致性和创造性方面有新提升，还具备参考图像生成视频、匹配风格等功能。该模型在视频生成领域表现出色，但在自然连贯的语音音频方面仍在发展。此外，还介绍了其在影视、游戏等行业的应用案例。
#### 核心功能
- **图像生成**：依据用户输入的文本描述，生成具有高度真实感的图像，包括不同场景、生物、物品等。
- **细节捕捉**：能够捕捉极端特写，呈现丰富的颜色、纹理和渐变，使图像具有可触摸感。
- **拼写排版优化**：在漫画、包装和收藏品等方面，实现改进的拼写、更长的文本字符串以及新的布局和样式。
- **视频生成**：根据文本描述生成具有高真实感和细节的视频，涵盖多种场景和风格。
- **音频生成**：Veo 3可原生生成音效、环境噪音和对话等音频。
- **视频编辑**：包括相机控制、添加或移除物体、角色控制、运动控制、外绘画、首末帧过渡等功能。

#### 应用场景
- **艺术创作**：帮助艺术家、设计师快速将创意转化为图像，用于绘画、插画、漫画等创作。
- **商业设计**：如产品包装设计、广告宣传海报、品牌形象设计等。
- **娱乐行业**：为游戏、影视制作生成场景、角色、道具等概念图。
- **影视制作**：从脚本到故事板制作，提升电影制作流程效率。
- **游戏开发**：为游戏提供视觉体验，如在地下城爬行游戏中充当主持人、引导者和互动角色。
- **创意工具开发**：开发者可将Veo与其他生成媒体技术结合，创造新型创意工具。 


- [Veo - Google DeepMind-视频生成](https://deepmind.google/models/veo/)
- [Imagen - 图像生成](https://deepmind.google/models/imagen/)
- [FLow-电影制作](https://labs.google/flow/about)
- [Sparkify-AI动画生成](https://sparkify.withgoogle.com/explore)
- [MedGemma  |  Health AI 谷歌医疗模型](https://developers.google.com/health-ai-developer-foundations/medgemma)
- [Stitch - Design with AI-UI设计](https://stitch.withgoogle.com/)
- [Gemini Diffusion扩散模型 - Google DeepMind](https://deepmind.google/models/gemini-diffusion/)

## Graphiti-AI动态知识图谱

#### 简介
Graphiti是一个用于构建和查询时态感知知识图谱的框架，专为动态环境中的AI智能体设计。它能将用户交互、企业数据和外部信息整合到可查询的图谱中，支持增量数据更新、高效检索和精确历史查询，适用于开发交互式、上下文感知的AI应用。

#### 核心功能
- 数据整合：集成和维护动态用户交互与业务数据。
- 推理与自动化：促进智能体基于状态的推理和任务自动化。
- 数据查询：支持语义、关键字和基于图的搜索方法，查询复杂且不断演变的数据。
- 管理操作：具备事件管理、实体管理、关系处理、语义和混合搜索、组管理、图维护等功能。

#### 技术原理
- 双时态数据模型：显式跟踪事件发生和摄取时间，实现精确的时间点查询。
- 高效混合检索：结合语义嵌入、关键字（BM25）和图遍历，实现低延迟查询。
- 自定义实体定义：通过Pydantic模型灵活创建本体并支持开发者定义实体。
- 可扩展性：利用并行处理高效管理大型数据集。

#### 应用场景
- 销售与客服：支持智能客服学习用户交互，融合个人知识与业务系统动态数据。
- 医疗健康：辅助医疗智能体执行复杂任务，基于多源动态数据进行推理。
- 金融领域：助力金融智能应用进行长期记忆和基于状态的推理。 


- [getzep/graphiti: Build Real-Time Knowledge Graphs for AI Agents](https://github.com/getzep/graphiti/)
- [Overview | Zep Documentation](https://help.getzep.com/graphiti/graphiti/overview)


# 2.通义万相-开源视频模型


#### 简介
该链接指向ModelScope主页，提供模型、数据集、工作室等资源。包含快速入门指引，有本周热门模型、数据集和工作室展示。平台提供Studio用于构建和展示AI应用，还有开源框架辅助模型开发和应用构建，以及评估、训练推理等工具。

#### 核心功能
- 模型与数据集展示：展示行业最新模型和数据集。
- 应用展示空间：Studio提供免费灵活的AI应用展示与构建空间。
- 框架工具支持：有Eval - Scope用于大模型评估，Swift用于大模型训练推理，ModelScope - Agent连接模型与外界。

#### 技术原理
- 统一框架：ModelScope Library作为统一网关，实现高效模型推理、微调与评估。
- 模型托管：ModelHub作为开源中心，托管AI模型和数据集。
- 评估定制：Eval - Scope通过可定制框架进行大模型性能基准测试。
- 多模型支持：Swift支持多种模型和训练方式。

#### 应用场景
- 模型开发：开发者基于平台模型和工具开发新AI模型。
- 应用构建：利用Studio构建和展示不同AI应用。
- 模型评估：使用Eval - Scope评估大模型性能。
- 模型比较：通过CompassArena体验和比较多款主流大模型效果。 


- [通义万相Wan2.1视频生成合集详情-来自Wan-AI · 魔搭社区](https://modelscope.cn/collections/tongyiwanxiang-Wan21-shipinshengcheng-67ec9b23fd8d4f)


# 2.字节BAGEL多模态模型

#### 简介
BAGEL是字节跳动开源的多模态基础模型，拥有140亿参数（70亿活跃参数）。采用混合变换器专家架构（MoT），通过双编码器分别捕捉图像像素级和语义级特征，经海量多模态标记数据预训练。在多模态理解基准测试中超越部分顶级开源视觉语言模型，具备图像与文本融合理解、视频内容理解、文本到图像生成等多种功能。

![Snipaste_2025-07-19_15-19-04.png](https://free.picui.cn/free/2025/07/19/687b46f2008d3.png)

#### 核心功能
1. **多模态交互**：处理图像和文本的混合输入输出，支持对话交流。
2. **内容生成**：生成高质量图像、视频帧或图文交织内容。
3. **图像编辑**：进行自由形式的图像编辑，保留视觉细节。
4. **风格迁移**：转换图像风格。
5. **导航功能**：在不同环境中进行路径规划和导航。
6. **多模态推理**：融合多模态数据进行推理，完成复杂任务。

#### 技术原理
- **架构设计**：采用混合变换器专家架构（MoT），含两个独立编码器，分别处理图像像素级和语义级特征。
- **专家混合机制**：编码器内有多个专家模块，动态选择合适专家组合处理多模态数据。
- **标记化处理**：将多模态数据转化为标记，图像分割成小块，文本按单词或子词处理。
- **训练范式**：遵循“下一个标记组预测”范式，用海量多模态标记数据预训练。

#### 应用场景
1. **内容创作**：生成图像、视频，进行创意广告制作。
2. **教育领域**：可视化复杂概念，辅助学习。
3. **电商平台**：生成产品3D模型和虚拟展示，提升购物体验。
4. **智能交互**：支持图像和文本混合的对话交流。 


- [BAGEL - 字节跳动开源的多模态基础模型 | AI工具集](https://ai-bot.cn/bagel/)
- [ByteDance-Seed/BAGEL-7B-MoT · Hugging Face](https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT)
- [ByteDance-Seed/Bagel](https://github.com/bytedance-seed/BAGEL)
- [BAGEL: The Open-Source Unified Multimodal Model](https://bagel-ai.org/)

# 2.bytedance ContentV文生视频

#### 简介
ContentV 是字节跳动推出的一个高效视频生成模型框架，其 8B 开源模型基于 Stable Diffusion 3.5 Large 和 Wan - VAE，在 256×64GB NPUs 上仅训练 4 周就在 VBench 测试中取得 85.14 的成绩。它通过创新架构、训练策略和反馈框架，能根据文本提示生成多分辨率、多时长的高质量视频。

#### 核心功能
- 文本到视频生成：依据文本提示生成多样化、高质量视频。
- 保证视频质量：确保生成视频具有高时间一致性和视觉质量。
- 内容多样：可生成涵盖人类、动物、场景等多类别的视频。

#### 技术原理
- 架构设计：采用简约架构，最大程度复用预训练图像生成模型进行视频合成。
- 训练策略：运用基于流匹配的系统多阶段训练策略，提升训练效率。
- 反馈框架：采用具成本效益的基于人类反馈的强化学习框架，在无需额外人工标注的情况下提高生成质量。

#### 应用场景
- 影视制作：为影视创作提供视频素材和创意灵感。
- 广告宣传：快速生成各类产品宣传视频。
- 教育领域：制作教学视频，丰富教学内容。 


- [bytedance/ContentV](https://github.com/bytedance/ContentV)
- [ByteDance/ContentV-8B · Hugging Face](https://huggingface.co/ByteDance/ContentV-8B)
- [ContentV: Efficient Training of Video Generation Models with Limited Compute](https://contentv.github.io/)

# 2.Matrix-Game空间大模型-昆仑万维

#### 简介
Matrix - Game是一个拥有170亿参数的交互式世界基础模型，用于可控的游戏世界生成。它采用两阶段训练流程，先进行无标签预训练理解环境，再进行有动作标签的微调以实现交互式视频生成。配套有含细粒度动作注释的大规模Minecraft数据集Matrix - Game - MC。通过GameWorld Score基准评估，在多个指标上优于先前的开源Minecraft世界模型。

#### 核心功能
1. 交互式生成：基于扩散的图像到世界模型，根据键盘和鼠标输入生成高质量视频，实现细粒度控制和动态场景演变。
2. 游戏世界评估：GameWorld Score基准，从视觉质量、时间质量、动作可控性和物理规则理解四个关键维度评估Minecraft世界模型。
3. 支持大规模训练：Matrix - Game数据集支持交互式和基于物理的世界建模的可扩展训练。
4. 精准控制：能精确控制角色动作和相机移动，保持高视觉质量和时间连贯性。

#### 技术原理
Matrix - Game采用图像到世界的生成范式，以单张参考图像作为世界理解和视频生成的主要先验。使用自回归策略保持片段间的局部时间一致性，实现长时间视频生成。模型训练分两阶段，先无标签预训练理解环境，后有动作标签微调实现交互式视频生成。

#### 应用场景
1. 游戏开发：用于生成多样化Minecraft场景及其他虚幻引擎构建的游戏场景的高质量交互式视频。
2. 游戏测试：通过评估模型的可控性和物理合理性，辅助测试游戏的操作体验和物理规则表现。 


- [SkyworkAI/Matrix-Game: Matrix-Game: Interactive World Foundation Model](https://github.com/SkyworkAI/Matrix-Game)
- [Matrix-Game: Interactive World Foundation Model](https://matrix-game-homepage.github.io/)

# 2.Step1X-3D 阶跃星辰 3D生成模型

#### 简介
Step1X-3D 是一个专注于高质量、可控生成带纹理三维资产的开源框架。它旨在解决当前三维生成领域数据稀缺、算法局限性以及生态系统碎片化等挑战，通过先进的技术实现高保真几何和多样化纹理贴图的生成，并确保几何与纹理之间的高度对齐。

#### 核心功能
- 高保真3D资产生成： 能够从单张图像生成具有高精细几何和多样纹理贴图的3D模型。
- 可控的几何与纹理： 支持对生成的3D模型进行控制，例如调整对称性、边缘类型（尖锐、普通、平滑）以及纹理风格（卡通、素描、照片级真实感）。
- 纹理与几何对齐： 确保生成的纹理与模型几何形状精确匹配，避免错位或不协调。
- 开放框架： 提供完整的模型、训练代码和适配模块，促进3D生成领域的开放研究和复现性。

#### 技术原理
Step1X-3D 采用两阶段3D原生架构：
- 几何生成阶段： 使用混合3D VAE-DiT（变分自编码器-扩散Transformer）扩散模型生成截断有符号距离函数 (TSDF)，随后通过行进立方体算法将其网格化，以构建高质量的3D几何。
- 纹理合成阶段： 利用经过微调的 SD-XL（Stable Diffusion XL）多视图生成器。该生成器以生成的几何和输入图像为条件，产生视图一致的纹理，并烘焙到3D模型上。
此外，该框架还包括一个严格的数据整理管道，处理并筛选出高质量的3D资产数据集，以支持模型训练。

#### 应用场景
- 游戏开发： 快速生成游戏角色、道具和环境的3D模型，提高开发效率。
- 虚拟现实 (VR) / 增强现实 (AR)： 创建沉浸式VR/AR体验所需的丰富3D内容。
- 数字艺术与设计： 为艺术家和设计师提供强大的工具，快速实现3D创意构想。
- 元宇宙构建： 自动化生成元宇宙中的各类数字资产。
- 工业设计与原型： 辅助产品设计与原型制作，实现快速迭代。


- [stepfun-ai/Step1X-3D: Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets](https://github.com/stepfun-ai/Step1X-3D)
- [Step1X 3D - a Hugging Face Space by stepfun-ai](https://huggingface.co/spaces/stepfun-ai/Step1X-3D)

# 3.MoviiGen 1.1
#### 简介
MoviiGen 1.1 是基于 Wan2.1 的尖端视频生成模型，在电影美学和视觉质量上表现出色。经专业人士评估，它在氛围营造、镜头运动和物体细节保留等关键电影维度表现卓越，能生成清晰度和真实感高的视频，适用于专业视频制作和创意应用。

#### 核心功能
- **高质量视频生成**：生成具有出色清晰度和细节的 720P、1080P 视频，保持视觉质量一致。
- **专业电影美学呈现**：在氛围营造、镜头运动和物体细节保留方面表现优异。
- **支持提示扩展推理**：可使用提示扩展模型优化生成效果。

#### 技术原理
- **训练框架**：基于 FastVideo，采用序列并行优化内存使用和训练效率，通过自定义实现将时间维度分配到多个 GPU，减少单设备内存需求。
- **数据处理**：将视频和文本提示缓存为潜在变量和文本嵌入，减少训练阶段计算开销。
- **混合精度训练**：支持 BF16/FP16 训练加速计算。

#### 应用场景
- **专业影视制作**：用于创作具有电影质感的视频内容。
- **创意设计领域**：辅助设计师实现高保真的视觉创意。
- **虚拟现实和游戏**：生成高质量的场景和角色动画。 


- [ZulutionAI/MoviiGen1.1: MoviiGen 1.1: Towards Cinematic-Quality Video Generative Models](https://github.com/ZulutionAI/MoviiGen1.1)
- [ZuluVision/MoviiGen1.1 · Hugging Face](https://huggingface.co/ZuluVision/MoviiGen1.1)


# 4.3D生成模型Partcrafter

#### 简介
PartCrafter是首个结构化3D生成模型，可从单张RGB图像联合合成多个语义有意义且几何不同的3D网格。它基于预训练的3D网格扩散变压器，引入组合潜在空间和分层注意力机制，无需预分割输入，能同时对多个3D部分去噪，实现端到端的部件感知生成。研究还整理了新数据集支持部件级监督，实验表明其在生成可分解3D网格方面优于现有方法。

#### 核心功能
- **多部件3D网格生成**：从单张RGB图像一次性联合生成多个语义和几何不同的3D网格，可用于生成单个物体或复杂多物体场景。
- **端到端部件感知生成**：无需图像分割输入，同时对多个3D部分去噪，实现端到端的部件感知生成。
- **支持不可见部件生成**：能够自动推断输入图像中不可见的3D结构。

#### 技术原理
- **基于预训练模型**：构建于在完整物体上训练的预训练3D网格扩散变压器（DiT），继承预训练权重、编码器和解码器。
- **组合潜在空间**：每个3D部分由一组解纠缠的潜在令牌表示，添加可学习的部件身份嵌入区分不同部件。
- **分层注意力机制**：包括局部注意力和全局注意力，局部注意力捕获每个部件内的局部特征，全局注意力实现所有部件间的信息流动，确保生成过程中的全局一致性和部件级细节。
- **训练目标**：通过整流流匹配训练，将噪声高斯分布映射到数据分布。

#### 应用场景
- **3D建模与设计**：辅助设计师快速从2D图像生成具有多个部件的3D模型，提高设计效率。
- **游戏与动画制作**：用于生成游戏和动画中的3D场景和物体，丰富场景内容和物体细节。
- **机器人训练**：提供准确和部件感知的3D表示，帮助机器人更好地理解和与环境交互。
- **虚拟和增强现实**：为VR/AR应用生成高质量的3D场景和物体，提升用户体验。 


- [wgsxm/PartCrafter: PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers](https://github.com/wgsxm/PartCrafter)
- [PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers](https://arxiv.org/pdf/2506.05573)
- [PartCrafter](https://wgsxm.github.io/projects/partcrafter/)

# 4.OmniAudio空间音频生成

#### 简介
OmniAudio是一个用于从360度视频生成一阶Ambisonics（FOA）空间音频的框架。论文作者提出了360V2SA新任务，构建了大规模数据集Sphere360，采用自监督预训练和双分支架构，在客观和主观指标上均达到了最先进的性能。项目网站提供相关信息，GitHub仓库提供代码。

#### 核心功能
- **空间音频生成**：从360度视频中生成FOA格式的空间音频，准确捕捉声音的方向和空间信息。
- **数据处理**：构建并清理Sphere360数据集，包含103,000个真实世界的视频片段，涵盖288种音频事件。
- **模型训练**：采用自监督的粗到细预训练策略，结合非空间和空间音频数据进行预训练，然后进行视频引导的微调。

#### 技术原理
- **条件流匹配**：使用时间相关的速度向量场，通过最小化目标函数来训练模型，生成从噪声到数据的概率路径。
- **变分自编码器（VAE）**：对FOA音频进行编码和解码，通过修改Stable Audio框架，适应四通道FOA音频。
- **双分支视频表示**：利用冻结的预训练MetaCLIP - Huge图像编码器，分别处理全景视频和透视视频，融合两种表示以捕捉局部和全局信息。

#### 应用场景
- **虚拟现实和增强现实**：为VR/AR体验提供更真实的音频环境，增强沉浸感。
- **内容创作**：帮助创作者轻松为视频添加音效、背景音乐或旁白。
- **教育和培训**：为教育视频自动生成音频解释，使学习体验更具吸引力。
- **历史保护**：为无声的历史镜头添加生成的音频，重现历史场景。 


- [liuhuadai/OmniAudio: [ICML 2025] PyTorch Implementation of "OmniAudio: Generating Spatial Audio from 360-Degree Video"](https://github.com/liuhuadai/OmniAudio)
- [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://omniaudio-360v2sa.github.io/)
- [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://arxiv.org/pdf/2504.14906)

# 4.PlayDiffusion音频编辑

#### 简介
PlayDiffusion是Play.AI推出的一款基于扩散模型的音频编辑模型，已开源。它解决了传统自回归模型在音频编辑时的局限性，能实现高质量、连贯的音频编辑，还可作为高效的文本转语音系统。

#### 核心功能
- **音频编辑**：对音频特定部分进行修改、替换，且能保持音频连贯性和自然度。
- **文本转语音**：在全音频波形被掩码的极端情况下，可作为高效的TTS系统。

#### 技术原理
- **编码与掩码**：将音频序列编码为离散空间的令牌，修改时掩码相应部分。
- **扩散模型去噪**：以更新后的文本为条件，用扩散模型对掩码区域去噪。
- **解码转换**：通过BigVGAN解码器将输出令牌序列转换为语音波形。
- **训练优化**：采用非因果掩码、自定义分词器和嵌入缩减、说话人条件等技术训练模型。

#### 应用场景
- **音频内容创作**：对已有音频进行精细修改和调整。
- **有声读物制作**：将文本高效转换为语音。
- **语音交互系统**：优化语音合成的质量和效率。 


- [playht/PlayDiffusion](https://github.com/playht/PlayDiffusion)
- [Meet PlayDiffusion – our newest voice model for inpainting](https://blog.play.ai/blog/play-diffusion)
- [PlayDiffusion - a Hugging Face Space by PlayHT](https://huggingface.co/spaces/PlayHT/PlayDiffusion)

# 5.多人对话视频框架

#### 简介
MultiTalk是用于音频驱动的多人对话视频生成的开源框架。输入多流音频、参考图像和提示词，可生成包含符合提示的互动且嘴唇动作与音频一致的视频。它支持单人和多人视频生成、交互式角色控制、卡通角色和唱歌视频生成，具有分辨率灵活、可生成长达15秒视频等特点。

#### 核心功能
1. **生成对话视频**：支持单人和多人对话视频生成，实现嘴唇动作与音频同步。
2. **角色交互控制**：通过提示词直接控制虚拟人物动作。
3. **泛化性能佳**：支持卡通角色和唱歌视频生成。
4. **灵活分辨率**：可输出480p和720p任意宽高比视频。
5. **长视频生成**：支持长达15秒视频生成。 

#### 技术原理
1. **基础架构**：采用基于DiT的视频扩散模型，结合3D变分自编码器（VAE），利用文本编码器生成文本条件输入，通过解耦交叉注意力将CLIP图像编码器提取的全局上下文注入DiT模型。
2. **音频嵌入提取**：使用Wav2Vec提取声学音频嵌入，并将当前帧附近的音频嵌入拼接。
3. **音频适配压缩**：在音频交叉注意力层，通过音频适配器对音频进行压缩，使视频潜在帧长度与音频嵌入匹配。
4. **多流音频注入**：提出四种注入方案，引入自适应人物定位方法和L - RoPE方法解决音频与人物绑定问题。
5. **训练策略**：采用两阶段训练、部分参数训练和多任务训练，保留基础模型的指令跟随能力。
6. **长视频生成**：采用基于自回归的方法，将先前生成视频的最后5帧作为额外条件进行推理。

#### 应用场景
1. **影视制作**：用于制作多角色电影场景。
2. **电商直播**：生成主播与观众互动的视频。
3. **动画创作**：创作卡通角色对话、唱歌的动画视频。 


- [MeiGen-AI/MultiTalk: Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation](https://github.com/MeiGen-AI/MultiTalk)
- [MeiGen-AI/MeiGen-MultiTalk · Hugging Face](https://huggingface.co/MeiGen-AI/MeiGen-MultiTalk)
- [arxiv.org/pdf/2505.22647](https://arxiv.org/pdf/2505.22647)
- [multi-talk官网](https://meigen-ai.github.io/multi-talk/)

# 5.趣丸科技-人脸动画生成

#### 简介
Playmate 是一个通过3D隐式空间引导扩散模型实现人像动画灵活控制的框架。它旨在解决现有口型同步、头部姿态不准确以及缺乏精细表情控制等挑战，从而生成高质量、可控的逼真说话人脸视频。

#### 核心功能
- 逼真说话人脸生成： 能够根据音频输入生成高度逼真的说话人脸视频。
- 精准唇形同步： 显著提高视频中人物唇部动作与语音的同步准确性。
- 灵活头部姿态控制： 允许用户对生成视频中的头部姿态进行精确调整和控制。
- 精细情感表达控制： 通过情感控制模块，实现对生成视频中人物情感的细粒度控制。
- 两阶段训练框架： 采用分阶段训练方法，优化属性解耦和运动序列生成。

#### 技术原理
Playmate 采用一个两阶段训练框架，核心是3D隐式空间引导扩散模型。

- 第一阶段： 利用**运动解耦模块（motion-decoupled module）增强属性解耦的准确性，并训练一个扩散变换器（diffusion transformer）**直接从音频线索生成运动序列。这一阶段专注于运动的生成和解耦。
- 第二阶段： 引入一个情感控制模块（emotion-control module），将情感控制信息编码到潜在空间中。这使得系统能够对表情进行精细控制，从而生成具有所需情感的说话视频。整个过程结合了扩散模型的强大生成能力与3D隐式空间的精细表征能力，以实现高度可控的视频合成。
#### 应用场景
- 虚拟数字人/AI主播： 创建具有高度表现力和情感的虚拟数字人形象，用于新闻播报、直播、客户服务等。
- 影视动画制作： 辅助电影、电视或游戏中的角色口型动画和表情生成，提高制作效率和真实感。
- 个性化内容创作： 用户可以根据需求，生成特定人物、声音和情感的个性化视频内容。
- 教育培训： 制作生动形象的教育讲解视频，提升学习体验。
- 语音助手/虚拟客服： 为语音助手或虚拟客服系统提供更加拟人化的视觉交互体验。

- [Playmate111/Playmate: Playmate: Flexible Control of Portrait Animation via 3D-Implicit Space Guided Diffusion](https://github.com/Playmate111/Playmate)
- [PlayMate111 Homepage](https://playmate111.github.io/Playmate/)

# Kwai Keye VL 快手
#### 简介
Kwai-Keye专注于多模态大语言模型的前沿探索与创新，致力于推动视频理解、多模态大语言模型等领域的发展。其Keye-VL-8B模型基于Qwen3 - 8B语言模型和SigLIP视觉编码器，在视频理解、复杂逻辑推理等方面表现出色。此外，团队还有多项研究成果，如MM - RLHF、VLM as Policy等。

#### 核心功能
- **多模态交互**：支持图像、视频和文本的交互，能对图像和视频进行描述等操作。
- **视频理解**：在多个视频基准测试中超越同规模顶级模型。
- **复杂推理**：在需要复杂逻辑推理和数学问题解决的评估集中有良好表现。
- **内容审核**：如提出的KuaiMod框架可用于短视频平台内容审核。

#### 技术原理
- **模型架构**：基于Qwen3 - 8B语言模型，结合从开源SigLIP初始化的视觉编码器，采用3D RoPE统一处理文本、图像和视频信息。
- **预训练**：采用四阶段渐进策略，包括图像 - 文本匹配、ViT - LLM对齐、多任务预训练和模型合并退火。
- **后训练**：分两个阶段五个步骤，引入混合思维链（CoT）和多思维模式强化学习（RL）机制。

#### 应用场景
- **智能客服**：处理包含图像和视频的用户咨询。
- **视频平台**：视频内容理解、审核和推荐。
- **教育领域**：辅助解决复杂的数学和科学问题。
- **内容创作**：根据图像和视频生成描述和创意内容。 


- [Kwai-Keye/Keye](https://github.com/Kwai-Keye/Keye/tree/main)
- [Kwai-Keye (Kwai-Keye)](https://huggingface.co/Kwai-Keye)
- [Kwai Keye](https://kwai-keye.github.io/)


# OmniAvatar浙大阿里视频生成
#### 简介
OmniAvatar是一种创新的音频驱动全身视频生成模型，旨在解决现有音频驱动人类动画方法在创建自然同步、流畅的全身动画以及精确提示控制方面的挑战。它引入像素级多层次音频嵌入策略，结合基于LoRA的训练方法，提高了唇同步准确性和身体动作的自然度，在面部和半身视频生成方面超越现有模型，可用于播客、人类交互、动态场景和唱歌等多种领域。

#### 核心功能
- **高质量视频生成**：根据单张参考图像、音频和提示，生成具有自适应和自然身体动画的逼真会说话的头像视频。
- **精准唇同步**：通过像素级多层次音频嵌入策略，实现音频与唇部动作的精确同步。
- **文本控制**：支持精确的基于文本的控制，可控制角色的动作、表情、背景等。
- **身份和时间一致性**：利用参考图像嵌入策略保持身份一致，使用潜在重叠策略确保长时间视频生成的时间连续性。

#### 技术原理
- **扩散模型**：采用潜在扩散模型（LDM）进行高效视频生成，在潜在空间中逆转扩散过程，将噪声逐步转化为数据。
- **音频嵌入**：提出像素级音频嵌入策略，使用Wav2Vec2提取音频特征，通过音频包模块将音频特征映射到潜在空间，并在像素级别嵌入到视频潜在空间。
- **LoRA训练**：对DiT模型的层应用基于LoRA的训练，在保留基础模型强大功能的同时，有效结合音频作为新条件。
- **长视频生成**：利用帧重叠和参考图像嵌入策略，保持长视频生成的一致性和时间连续性。

#### 应用场景
- **播客制作**：生成主播的视频内容，提升播客的视觉效果。
- **虚拟社交**：用于虚拟人物的交互视频，增强社交体验。
- **动态场景模拟**：如游戏、影视等领域，生成角色的动画视频。
- **音乐表演**：生成歌手唱歌的视频，实现音频与动作的同步。 

* 项目链接：
    - [OmniAvatar/OmniAvatar-14B · Hugging Face](https://huggingface.co/OmniAvatar/OmniAvatar-14B)
    - [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/pdf/2506.18866)
    - [Omni-Avatar/OmniAvatar](https://github.com/Omni-Avatar/OmniAvatar)
    - [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://omni-avatar.github.io/)

    
# OmniGen2 智源研究院
#### 简介
OmniGen2 是一款多功能开源生成模型，旨在为文本到图像、图像编辑和上下文生成等多种生成任务提供统一解决方案。它具有两条不同的文本和图像模态解码路径，采用非共享参数和分离的图像分词器。研究团队开发了全面的数据构建管道，引入了用于图像生成任务的反射机制和专门的反射数据集。此外，还推出了 OmniContext 基准测试，用于评估模型的上下文生成能力。

#### 核心功能
1. **多模态生成**：支持文本到图像生成、图像编辑和上下文生成等多种任务。
2. **反射机制**：能够对生成的图像进行反思，识别不足并迭代优化输出。
3. **数据构建**：开发了用于图像编辑和上下文生成的数据构建管道，生成高质量数据集。
4. **基准测试**：引入 OmniContext 基准测试，评估模型在上下文图像生成中的一致性。

#### 技术原理
1. **架构设计**：采用独立的自回归和扩散变压器架构，使用 ViT 和 VAE 两种不同的图像编码器。
2. **位置嵌入**：引入 Omni - RoPE 多模态旋转位置嵌入，分解为序列和模态标识符、2D 空间高度和宽度坐标三个组件。
3. **训练策略**：MLLM 大部分参数冻结，仅更新特殊令牌；扩散模型从头开始训练，采用混合任务训练策略。
4. **数据处理**：使用多种开源图像数据集进行训练，通过视频数据构建上下文生成和图像编辑数据集。

#### 应用场景
1. **创意设计**：根据文本描述生成图像，为设计师提供灵感和素材。
2. **图像编辑**：对现有图像进行修改，如改变背景、移除对象、调整风格等。
3. **虚拟现实/增强现实**：生成与上下文相关的图像，用于 VR/AR 场景的内容创建。
4. **智能客服**：根据用户描述生成图像，更直观地回答问题。 

- [OmniGen2-github](https://github.com/VectorSpaceLab/OmniGen2)
- [项目官网](https://vectorspacelab.github.io/OmniGen2/)
- [论文：OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/pdf/2506.18871)

# FairyGen 动画生成

#### 简介
FairyGen是一个用于从单个儿童手绘角色生成动画故事视频的新颖框架。它借助多模态大语言模型进行故事规划，通过风格传播适配器确保视觉一致性，利用3D代理生成物理上合理的运动序列，并采用两阶段运动定制适配器实现多样化和连贯的视频场景渲染，在风格、叙事和运动方面表现出色。

#### 核心功能
- **故事与分镜规划**：利用MLLM从给定角色草图推断结构化分镜，进行时空分镜规划。
- **风格一致背景合成**：通过定制风格传播模块，将角色的审美特征传播到背景，生成风格一致的背景。
- **3D代理角色视频序列生成**：从2D角色图像重建3D代理，导出物理上合理的运动序列，用于微调图像到视频的扩散模型。
- **两阶段运动定制**：第一阶段从无序帧学习外观特征，分离身份和运动；第二阶段使用时间步移策略建模时间动态，生成多样化和连贯的视频场景。

#### 技术原理
- **故事规划**：使用MLLM生成结构化分镜，包含全局叙事概述和详细的分镜描述。
- **风格传播**：基于预训练的文本到图像扩散模型，采用基于传播的定制策略，在训练时学习角色视觉风格，推理时将其传播到背景。
- **3D重建**：从2D草图重建角色的3D代理，导出运动序列，作为训练数据微调MMDiT-based图像到视频扩散模型。
- **两阶段运动定制**：第一阶段去除时间偏置，学习空间特征；第二阶段使用时间步移采样策略，增加采样高噪声训练步骤的概率，学习鲁棒的运动表示。

#### 应用场景
- **教育领域**：将儿童绘画转化为动画故事，激发儿童想象力和创造力。
- **数字艺术治疗**：帮助患者通过绘画和动画表达情感和想法。
- **个性化内容创作**：为用户生成个性化的动画故事视频。
- **互动娱乐**：为游戏、动漫等娱乐产业提供创意内容。 

- [FairyGen-github](https://github.com/GVCLab/FairyGen)
- [项目官网](https://jayleejia.github.io/FairyGen/)
- [论文：FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/pdf/2506.21272)


# HumanOmniV2-阿里

#### 简介
HumanOmniV2 是阿里通义实验室开源的多模态推理模型，解决了多模态推理中全局上下文理解不足和推理路径简单的问题。它能在生成答案前分析视觉、听觉和语言信号，构建场景背景，精准捕捉隐藏逻辑和深层意图。该模型在 IntentBench 等基准测试中表现出色，现已开源。

#### 核心功能
- 全面理解多模态信息：综合分析图像、视频、音频等多模态输入，捕捉隐藏信息和深层逻辑。
- 精准推理人类意图：基于上下文背景，准确理解对话或场景中的真实意图。
- 生成结构化推理路径：输出详细的上下文总结和推理步骤，确保推理可解释。
- 应对复杂社交场景：识别理解人物情绪、动机及社会关系，提供符合人类认知的判断。

#### 技术原理
- 强制上下文总结机制：输出 <context> 标签内的上下文概括，构建完整场景背景。
- 大模型驱动的多维度奖励体系：包含上下文、格式、准确性和逻辑奖励，激励模型准确推理。
- 基于 GRPO 的优化训练方法：引入词元级损失，移除问题级归一化项，应用动态 KL 散度机制。
- 高质量的全模态推理训练数据集：包含图像、视频和音频任务，附带详细标注。
- 全新的评测基准 IntentBench：评估模型对人类行为动机、情感状态和社会互动的理解能力。

#### 应用场景
- 视频内容理解与推荐：为视频平台提供精准推荐。
- 智能客服与客户体验优化：帮助客服人员应对客户问题。
- 情感识别与心理健康支持：辅助心理健康应用提供情绪支持。
- 社交互动分析与优化：优化社交推荐和用户互动体验。
- 教育与个性化学习：为在线教育平台提供个性化学习建议。 


[HumanOmniV2-coder](https://github.com/HumanMLLM/HumanOmniV2)

[HumanOmniV2-模型](https://huggingface.co/PhilipC/HumanOmniV2)

[HumanOmniV2-论文](https://arxiv.org/pdf/2506.21277)

[HumanOmniV2](https://www.aliyun.com/product/ai-studio)


# 通义万相2.2 – 阿里开源AI视频生成模型

#### 简介
通义万相（Tongyi Wanxiang）是阿里云通义旗下的人工智能创意创作平台。其中，通义万相2.2（Wan2.2）是阿里巴巴开源的先进AI视频生成模型，旨在降低创意工作的门槛。该平台集成了多种AI生成能力，包括视频和图像内容的创作。

![通义.png](https://free.picui.cn/free/2025/07/29/6888b2c857b67.png)

![通义1.png](https://free.picui.cn/free/2025/07/29/6888b2c23803a.png)


#### 核心功能
*   **文生视频（Text-to-Video, T2V）**: 根据文本描述生成视频内容。
*   **图生视频（Image-to-Video, I2V）**: 根据静态图片生成动态视频。
*   **统一视频生成（Unified Video Generation, IT2V）**: 整合文本和图像输入进行视频生成。
*   **图像生成**: 支持文生图、图生图、涂鸦作画、虚拟模特、个人写真等多种图像创作模式。
*   **高级特效与LoRA训练**: 提供增强的图像/视频生成效果和更智能的LoRA模型训练功能。
*   **电影级控制**: 首次实现MoE架构的视频生成模型，具备电影级控制能力。

![通义2.png](https://free.picui.cn/free/2025/07/29/6888b2c2e4673.png)

#### 技术原理
通义万相2.2基于先进的AI视频生成模型，采用大规模模型（Large-Scale Video Generative Models）和混合专家架构（MoE-Architecture）来提升视频生成质量和控制力。其核心技术可能包括深度学习、扩散模型（如Hugging Face上提及的Diffusers）以及复杂的神经网络结构，以实现从文本或图像到高质量视频的转换。

#### 应用场景
*   **内容创作**: 为电影、广告、短视频等领域的创作者提供高效的视频和图像生成工具，降低制作成本和时间。
*   **数字营销**: 快速生成个性化、吸引人的宣传视频和图片，用于产品推广和品牌建设。
*   **艺术设计**: 辅助艺术家和设计师进行概念验证和视觉化创作，拓宽创意边界。
*   **个人娱乐**: 满足普通用户生成有趣视频和图片的个性化需求，如社交媒体分享等。
*   **教育与培训**: 制作教学动画、演示视频，提升学习体验。

* https://tongyi.aliyun.com/wanxiang/welcome
* https://github.com/Wan-Video/Wan2.2
* https://tongyi.aliyun.com/wanxiang


# 腾讯混元3D世界模型开源

#### 简介
Hunyuan3D是腾讯研发的大规模三维生成模型，基于先进的扩散（Diffusion）技术。它能够通过文本描述或图像输入，快速、高效地生成高质量、逼真的3D资产。HunyuanWorld-1.0作为其重要组成部分，更是首个开源的三维世界生成模型，旨在革新3D内容创作流程。

![腾讯.png](https://free.picui.cn/free/2025/07/29/6888b2c534813.png)

#### 核心功能
*   **文本到三维生成 (Text-to-3D Generation)**：依据自然语言描述直接生成多样化的3D模型，涵盖物体、角色、环境等。
*   **图像到三维重建 (Image-to-3D Reconstruction)**：将2D图像转换为具有精确几何和纹理的高质量3D模型。
*   **高分辨率与高质量输出 (High-Resolution & High-Quality Output)**：生成细节丰富、视觉效果出众的3D资产。
*   **多视图生成与重建 (Multi-view Generation & Reconstruction)**：支持从多角度图像输入进行3D模型的生成与精细重建。
*   **场景与世界生成 (Scene & World Generation)**：尤其通过HunyuanWorld-1.0，实现复杂三维场景乃至整个虚拟世界的自动化生成。

#### 技术原理
Hunyuan3D的核心技术是**扩散模型 (Diffusion Models)**，这是一种生成式AI模型，通过逐步去噪过程从随机噪声中学习数据分布并生成新样本。其架构包含：
*   **文本编码器 (Text Encoders)**：负责将输入的文本描述转换为高维语义特征。
*   **图像编码器 (Image Encoders)**：处理输入的2D图像，提取视觉特征。
*   **扩散模型 (Diffusion Models)**：接收编码后的文本或图像特征，通过多步迭代逆向扩散过程，逐步从噪声中恢复并生成3D的潜在表示。
*   **3D解码器 (3D Decoders)**：将扩散模型输出的3D潜在表示解码为最终的3D几何（如网格或体素）和纹理信息，完成从2D/文本到3D的映射。
整个系统通过大规模3D数据集进行训练，以确保生成内容的质量和多样性。

#### 应用场景
*   **游戏开发 (Game Development)**：快速生成游戏角色、道具、场景环境，显著提升内容创作效率。
*   **影视动画制作 (Film & Animation Production)**：辅助制作复杂的3D场景、特效及角色，降低生产成本和周期。
*   **虚拟现实 (VR) 与增强现实 (AR)**：为VR/AR应用提供丰富的3D内容库，构建沉浸式体验。
*   **数字内容创作 (Digital Content Creation)**：赋能设计师、艺术家和内容创作者，快速实现3D视觉创意。
*   **元宇宙构建 (Metaverse Construction)**：大规模生成虚拟世界的3D资产和环境，加速元宇宙生态的建设。
*   **产品设计与可视化 (Product Design & Visualization)**：将概念图或文本描述迅速转化为3D模型进行展示、原型制作和评估。


腾讯混元 3D 世界模型 1.0：

* 项目主页：https://3d-models.hunyuan.tencent.com/world/

* 体验地址：https://3d.hunyuan.tencent.com/sceneTo3D

* Hugging Face 模型地址：https://huggingface.co/tencent/HunyuanWorld-1

* Github 项目地址：https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0


# 书生浦语-科学多模态大模型Intern-S1

#### 简介
Intern-S1 是上海人工智能实验室 (Shanghai AI Laboratory) 开发的 InternLM 系列大型语言模型 (LLM) 中的一员。它旨在提供高质量的语言模型和全栈工具链，尤其强调其强大的推理能力和与外部工具的交互能力。

#### 核心功能
*   **工具调用 (Tool Calling)**：能够调用外部工具，例如获取实时信息、执行代码或调用其他应用程序内的函数，从而扩展模型的能力边界。
*   **思维模式 (Thinking Mode)**：默认开启的思维模式显著增强了模型的推理能力，使其能够生成更高质量的响应。此模式可根据需要进行配置。
*   **多模态能力 (Multimodal Capabilities)**：支持多模态输入，进一步拓宽了模型的应用范围。

#### 技术原理
Intern-S1 基于 InternLM 的基础架构，并在此基础上进行了优化：
*   **函数调用集成**：通过特定的工具调用机制，模型能够解析用户意图并自动选择、调用外部函数或API，将外部世界的信息和能力整合到其响应生成过程中。
*   **FP8 量化**：提供 FP8 (8位浮点) 量化版本，这意味着模型在保持较高性能的同时，大幅降低了显存占用和计算资源需求，提高了推理效率。
*   **高级推理机制**：内置“思维模式”，通过引导模型进行类似人类的逐步思考过程，提升其复杂问题的推理和解决能力。
*   **部署灵活性**：支持通过 SGLang 和 Ollama 等多种方式进行本地部署，为开发者和研究人员提供了便捷的实验和应用环境。

#### 应用场景
*   **智能代理 (AI Agents)**：构建能够自主获取最新信息、执行复杂任务或与外部系统交互的智能代理。
*   **代码辅助与自动化**：在软件开发中，辅助代码生成、调试，或自动化某些编程任务。
*   **实时信息查询**：结合外部搜索引擎或数据库，提供即时、准确的信息服务。
*   **复杂问题解决**：在需要多步推理和外部工具协助的领域（如科学计算、金融分析）提供解决方案。
*   **大模型研究与开发**：作为开放的模型和工具链，为研究人员和开发者提供强大的基础模型和开发平台，用于探索新的应用和优化。


* Github仓库：https://github.com/InternLM/Intern-S1
* HuggingFace模型库：https://huggingface.co/internlm/Intern-S1-FP8
* 官网：https://intern-ai.org.cn/home



# Step 3 – 阶跃星辰多模态推理模型

#### 简介
Step 3 是阶跃星辰（StepFun AI）发布的新一代基础大模型，专为推理时代设计。它集高性能与极致成本效益于一体，具备强大的视觉感知和复杂推理能力，旨在成为SOTA（State-of-the-Art）水平的开放生态基础模型。

#### 核心功能
*   **多模态感知与理解：** 具备强大的视觉感知能力，能处理和理解多种模态的信息。
*   **复杂推理：** 针对推理时代的需求，擅长进行复杂的逻辑推理。
*   **高性能与高效率：** 在保证卓越性能的同时，注重极致的成本效益。
*   **开放生态支持：** 作为开放模型，支持开发者和研究人员在其基础上进行创新。

#### 技术原理
Step 3 采用了先进的**MoE（Mixture-of-Experts）架构**，这使得模型能够在大参数量下实现高效的激活参数量，从而平衡性能与计算资源消耗。总参数量达到321B，激活参数量为38B。这种架构有助于模型在不同任务上动态激活最相关的专家网络，优化资源利用和推理效率。其技术报告进一步详细阐述了Attention-FFN解耦等优化技术，以实现高吞吐量解码。

#### 应用场景
*   **智能助理与对话系统：** 提供更自然、智能的多模态交互体验。
*   **视觉内容理解与生成：** 应用于图像识别、视频分析、内容创作等领域。
*   **复杂问题解决：** 在科研、工程、医疗等需要强大推理能力的场景中提供辅助。
*   **开发者工具与平台：** 作为基础模型，可供开发者在其上构建各类AI应用和服务。

* https://mp.weixin.qq.com/s/SuAfRxg-GpTz_OIeDzd9-w

* Github仓库：https://github.com/stepfun-ai/Step3


# Higgs Audio V2 – 开源语音大模型

#### 简介
Higgs Audio V2是由李沐及其团队Boson AI开发并开源的语音大模型。它是一个强大的音频基础模型，经过超过1000万小时的音频数据和多样化文本数据的预训练，旨在模拟自然流畅的多人互动场景，并具备生成高质量音频的能力。

#### 核心功能
*   **多语言对话生成：** 能够生成自然流畅的多语言对话。
*   **自动韵律调整：** 自动匹配说话者的韵律和情感。
*   **语音克隆：** 具备语音克隆能力，可以复制特定人的声音。
*   **歌声合成：** 支持歌声合成功能。
*   **零样本文本到语音（Zero-shot TTS）：** 能够在没有特定训练数据的情况下，通过参考文本、参考音频和目标文本进行文本到语音的转换。
*   **多人对话生成：** 特别优化了模拟多人互动对话场景的能力。

#### 技术原理
Higgs Audio V2采用统一的音频语言建模方法（unified audio language modeling at scale）。它是一个基于超过1000万小时的音频数据和多样化文本数据进行预训练的音频基础模型。这种大规模的预训练使其能够捕捉复杂的语音特征、韵律变化以及多语言和多说话人的交互模式，从而实现自然度高、表现力强的语音生成。模型在多个基准测试中展现出高性能，包括Seed-TTS Eval、Emotional Speech Dataset (ESD)、EmergentTTS-Eval和Multi-speaker Eval。

#### 应用场景
*   **虚拟助手与客服系统：** 创建更自然、更具交互性的AI虚拟助手和智能客服。
*   **内容创作：** 用于播客、有声读物、游戏配音、动画等领域，快速生成高质量的语音和对话内容。
*   **教育与培训：** 制作多语言教学材料、模拟对话练习，提升学习体验。
*   **电影与电视制作：** 辅助角色配音，实现不同角色间的自然对话，甚至进行歌声合成。
*   **无障碍辅助：** 为有视力障碍的用户提供文本转语音服务，或为特定应用提供定制化语音交互。
*   **社交媒体与娱乐：** 生成个性化的音频内容，增强用户互动体验，如语音社交、虚拟偶像歌唱等。

* Github仓库：https://github.com/boson-ai/higgs-audio
* 在线体验Demo：https://huggingface.co/spaces/smola/higgs_audio_v2





## AudioGen-Omni – 快手推出的多模态音频生成框架

#### 简介
AudioGen-Omni是快手推出的一款多模态音频生成框架，能够基于视频、文本等多种输入，高效生成高质量的音频、语音和歌曲。它旨在提供一个统一的解决方案，以满足不同形式的音频内容创作需求。

#### 核心功能
*   **多模态输入支持：** 能够接受视频和文本作为输入，生成相应的音频内容。
*   **高保真音频生成：** 可生成高质量的背景音乐、语音、环境音效以及完整歌曲。
*   **统一生成能力：** 框架实现了对不同类型音频（如语音、音乐、音效）的统一生成，简化了工作流程。

#### 技术原理
AudioGen-Omni基于多模态扩散Transformer (MMDit) 架构，通过联合训练大规模的视频-文本-音频语料库进行学习。其核心技术包括统一的歌词-文本编码器，以及用于相位对齐的先进机制（如AdaLN），确保生成音频的连贯性和质量。这种架构使其能够理解复杂的跨模态信息，并生成与输入高度相关的音频。

#### 应用场景
*   **短视频及直播内容创作：** 为视频自动配乐、生成旁白或特效声音，提升内容丰富度。
*   **音乐制作与歌曲创作：** 从文本歌词或意图描述生成歌曲旋律、伴奏及人声。
*   **智能语音助手与虚拟人：** 生成自然流畅的语音对话，增强人机交互体验。
*   **多媒体内容编辑：** 作为AI辅助工具，帮助用户快速生成所需的音频素材。


AudioGen-Omni的项目地址
* 项目官网：https://ciyou2.github.io/AudioGen-Omni/
* arXiv技术论文：https://ciyou2.github.io/AudioGen-Omni/


## MiDashengLM – 小米开源的高效声音理解大模型

#### 简介
MiDaShengLM-7B是小米研究（Xiaomi Research）开源的多模态语音AI模型，参数规模为70亿，专注于音频理解和推理。该模型旨在通过整合先进的音频编码器和大型语言模型，实现对语音、环境声音和音乐元素的全面理解。它代表了小米在语音AI领域的重要进展，并已面向全球社区开放。

#### 核心功能
*   **通用音频理解与推理：** 能够综合理解和推理各种音频内容，包括人类语音、环境声音（如硬币掉落、水滴声）和音乐元素。
*   **跨模态融合：** 有效结合音频输入与文本提示，生成相关的文本响应，支持音频到文本的理解。
*   **高效推理：** 相较于同类模型（如Qwen2.5-Omni-7B），展现出卓越的推理效率和更快的响应时间，即使在处理较长音频输入时也能保持性能。
*   **情绪与音乐理解：** 具备理解说话者情绪和音乐的能力，超越了传统语音识别的范畴。

#### 技术原理
MiDaShengLM-7B的核心技术原理是其独特的集成架构：
*   **Dasheng音频编码器：** 采用了小米自研的Dasheng开源音频编码器，该编码器以其在通用音频理解方面的先进性能而闻名。
*   **Qwen2.5-Omni-7B Thinker解码器：** 与阿里巴巴的Qwen2.5-Omni-7B Thinker解码器进行集成，实现强大的语言理解和生成能力。
*   **基于字幕的对齐策略（Caption-based Alignment Strategy）：** 模型采用独特的基于字幕的对齐策略，利用通用音频字幕来捕捉全面的音频表示。这种方法不同于传统的ASR驱动方法，能更有效地整合语音、环境声音和音乐，形成统一的文本表示。
*   **端到端自主信息检索与多步推理：** 结合了类似AI Agent的能力，使其能够在复杂的音频环境中进行主动感知、决策和行动，进行深度的信息检索和多步推理。

#### 应用场景
*   **智能家居：** 作为智能设备的核心语音交互模块，实现更自然、智能的语音控制和环境感知。
*   **汽车领域：** 在智能驾驶舱中提供高级语音助手功能，包括语音命令识别、环境噪音过滤和情绪识别。
*   **通用AI应用：** 广泛应用于需要音频理解和跨模态交互的各种AI产品和框架中，如智能助手、内容创作、安防监控等。
*   **音频内容分析：** 对播客、音乐、环境录音等进行深度分析，提取关键信息和情感。
*   **残障辅助技术：** 通过更准确地理解和响应音频输入，提升相关辅助设备的性能。

MiDashengLM的项目地址
* GitHub仓库：https://github.com/xiaomi-research/dasheng-lm
* HuggingFace模型库：https://huggingface.co/mispeech/midashenglm-7b
* 技术论文：https://github.com/xiaomi-research/dasheng-lm/blob/main/technical_report/MiDashengLM_techreport.pdf
* 在线体验Demo：https://huggingface.co/spaces/mispeech/MiDashengLM-7B


## RedOne – 小红书推出的社交大模型
#### 简介
根据提供的链接，ai-bot.cn 是一个创新型人工智能平台，提供一系列AI驱动的工具和解决方案，旨在提升生产力、优化流程并提供数据分析。同时，arXiv.org 是一个开放获取的学术论文预印本库，涵盖物理学、数学、计算机科学等多个领域，是研究人员分享最新研究成果的重要平台，尽管其内容未经同行评审。

![小红书.png](https://free.picui.cn/free/2025/08/06/6892c72f9b7bc.png)

#### 核心功能
*   **AI工具与服务提供：** ai-bot.cn 提供AI驱动的生产力工具、流程自动化工具和数据分析服务，以及用于电子商务、客户支持的AI代理（如WhatsApp、Facebook和Instagram机器人）。
*   **学术研究成果共享：** arXiv.org 作为一个免费的学术文献分发服务和开放获取档案库，其核心功能是提供物理学、数学、计算机科学等领域约240万篇学术文章的存储和访问，促进前沿研究的快速传播。

#### 技术原理
ai-bot.cn 提供的AI服务很可能基于**机器学习(ML)** 和**自然语言处理(NLP)** 等技术，通过训练模型实现自动化、数据分析和智能交互。其中可能涉及**神经网络结构设计**、**激活函数选择**、**梯度优化技术**以及**损失函数构建**等机器学习核心原理。针对特定应用，如聊天机器人，可能运用到**对话管理系统**和**意图识别**等技术。虽然具体论文内容未直接获取，但arXiv上相关的AI研究广泛涉及**梯度下降**等优化算法，这是深度学习训练的基础。

#### 应用场景
*   **商业运营与效率提升：** 企业可以通过 ai-bot.cn 的AI工具集成AI，以提高运营效率，如自动化销售、优化客户服务（电商AI客服机器人），进行数据驱动的决策分析。
*   **个人项目与创新：** 个人用户可以利用 ai-bot.cn 的AI技术实现其项目目标。
*   **学术研究与教育：** arXiv.org 作为学术资料库，为物理学、数学、计算机科学（包括机器学习、机器人技术、多智能体系统）、定量生物学、统计学等领域的学者和学生提供最新的研究论文，支持学术交流、前沿探索和教育学习。


* arXiv技术论文：https://www.arxiv.org/pdf/2507.10605


## RynnVLA-001 – 阿里达摩院开源的视觉-语言-动作模型

RynnVLA-001是阿里巴巴达摩院开发的一种视觉-语言-动作（Vision-Language-Action, VLA）模型。该模型通过大规模第一人称视角的视频进行预训练，旨在从人类示范中学习操作技能，并能够将这些技能隐式地迁移到机器人手臂的控制中，使其能够理解高层语言指令并执行复杂的任务。

![rynnvla.png](https://free-img.400040.xyz/4/2025/08/12/689ab9c10ab37.png)

![rynnvla1.png](https://free-img.400040.xyz/4/2025/08/12/689ab9bf6c436.png)

#### 核心功能
*   **视觉-语言理解与融合：** 能够理解视觉输入和自然语言指令，并将两者关联起来。
*   **技能学习与迁移：** 从人类第一人称视角的视频示范中学习复杂的操作技能，并将其迁移到机器人平台。
*   **机器人操作控制：** 使机器人手臂能够精确执行抓取-放置（pick-and-place）等复杂操作任务。
*   **长程任务执行：** 支持机器人根据高级语言指令完成需要多步操作的长程任务。

#### 技术原理
RynnVLA-001的核心技术原理是基于生成式先验（generative priors）构建的。它是一个简单而有效的VLA模型，其基础是一个预训练的视频生成模型。具体流程包括：
1.  **第一阶段：自我中心视频生成模型（Ego-centric Video Generation Model）：** 利用大量第一人称视频数据训练一个视频生成模型，捕捉人类操作的视觉规律。
2.  **第二阶段：机器人动作块的VAE压缩（VAE for Compressing Robot Action Chunks）：** 使用变分自编码器（VAE）对机器人动作序列进行高效压缩，提取核心动作特征。
3.  **第三阶段：视觉-语言-动作模型构建（Vision-Language-Action Model）：** 将视频生成模型和动作VAE结合，构建一个端到端的VLA模型，使其能够从视觉和语言输入中直接生成机器人动作。

#### 应用场景
*   **机器人工业自动化：** 用于训练工业机器人执行精细装配、分拣和搬运等任务，提高生产线自动化水平。
*   **服务机器人：** 赋予服务机器人更强的环境感知和人机交互能力，使其能更好地理解并执行用户指令，例如家庭助手、医疗辅助机器人。
*   **人机协作：** 促进机器人更自然地与人类协作，通过观察人类操作学习新技能，提高协作效率。
*   **智能家居：** 应用于智能家电和自动化系统中，实现更智能、更人性化的设备控制和任务执行。
*   **教育与研究：** 为机器人学习、多模态AI等领域提供一个强大的研究平台和教学工具。

* 项目官网：https://huggingface.co/blog/Alibaba-DAMO-Academy/rynnvla-001
* GitHub仓库：https://github.com/alibaba-damo-academy/RynnVLA-001
* HuggingFace模型库：https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-001-7B-Base


## RynnEC – 阿里达摩院世界理解模型

RynnEC是阿里巴巴达摩院推出的一种世界理解模型（MLLM），专为具身认知任务设计。它旨在赋予人工智能系统对物理世界及其环境中物体深入的理解能力。

![RynnEC.png](https://free-img.400040.xyz/4/2025/08/12/689ada9783045.png)

#### 核心功能
RynnEC的核心功能在于能够从多达11个维度全面解析场景中的物体，这些维度包括但不限于物体的位置、功能和数量。模型支持对物体的精确理解以及对空间关系的深入感知。

![RynnEC1.png](https://free-img.400040.xyz/4/2025/08/12/689ada965aad6.png)

#### 技术原理
RynnEC基于多模态大语言模型（MLLM）架构，其技术原理涉及融合视觉与语言信息，以构建对真实世界的丰富表征。通过对场景中物体在位置、功能、数量等多个维度进行精细化分析，RynnEC能够实现高维度的场景理解和物体属性识别，从而支持复杂的具身智能决策和交互。

#### 应用场景
RynnEC主要应用于需要具身认知能力的领域，包括但不限于：
*   **智能机器人与自动化：** 帮助机器人在复杂环境中理解并操作物体，执行抓取、导航等任务。
*   **虚拟现实（VR）/增强现实（AR）：** 提升虚拟/增强环境中对现实物体的识别和交互能力，提供更真实的沉浸式体验。
*   **智能家居：** 赋能智能设备更准确地感知和响应用户指令及环境变化。
*   **自动驾驶：** 辅助车辆更好地理解道路环境、交通参与者和障碍物，提升决策安全性。


* GitHub仓库：https://github.com/alibaba-damo-academy/RynnEC/

##  RynnRCP – 阿里达摩院机器人上下文协议

RynnRCP（Robotics Context Protocol）是阿里巴巴达摩院开源的一套机器人上下文协议及框架，旨在打通具身智能（Embodied Intelligence）的开发全流程，提供标准化的机器人服务协议和开发框架。

#### 核心功能
*   **标准化协议与框架：** 提供一套完整的机器人服务协议和开发框架，促进具身智能开发流程的标准化。
*   **模块化组件：** 主要由RCP框架和RobotMotion两大核心模块组成，分别负责协议定义与机器人运动控制。
*   **全流程打通：** 旨在整合具身智能从感知、认知到行动的开发链路，提升开发效率和兼容性。

#### 技术原理
RynnRCP的核心技术原理基于**机器人上下文协议（Robotics Context Protocol）**，该协议定义了机器人系统间进行任务、数据和状态交互的标准化接口和规范。其内部包含：
*   **RCP 框架：** 负责定义具身智能任务的描述、分解、执行状态以及环境上下文信息的传递机制，确保不同模块和设备间的协同工作。
*   **RobotMotion 模块：** 专注于机器人的运动控制，可能涉及高级运动规划、力控、轨迹生成以及与机器人硬件接口的集成，实现精确且鲁棒的物理世界操作。
*   **具身智能理论：** 结合AI与机器人技术，使机器人能够像人类一样感知环境、理解意图、作出决策并在物理世界中执行任务，强调实体与环境的交互。

#### 应用场景
*   **通用具身智能机器人开发：** 适用于各类服务机器人、工业机器人、物流机器人等具身智能设备的快速开发与部署。
*   **机器人系统集成：** 作为统一的通信和控制协议，便于集成不同厂商的机器人硬件、传感器和执行器。
*   **智能工厂与自动化：** 在工业自动化领域，用于实现机器人协同作业、产线柔性制造和智能巡检。
*   **智慧生活与服务：** 在家庭、医疗、零售等服务场景中，支撑服务机器人的智能交互和任务执行。


* GitHub仓库：https://github.com/alibaba-damo-academy/RynnRCP


## Matrix-3D – 昆仑万维开源的3D世界模型

Matrix-3D是由昆仑万维Skywork AI团队开发的一个先进框架，旨在通过单张图像或文本提示生成可探索的大规模全景3D世界。它结合了全景视频生成与3D重建技术，旨在实现高保真、全向可探索的沉浸式3D场景。

![matrix-3d.png](https://free-img.400040.xyz/4/2025/08/12/689ada96c6197.png)

#### 核心功能
*   **全景3D世界生成：** 能够从单一图像或文本提示生成广阔且可探索的全景3D场景。
*   **图像/文本到3D场景转换：** 支持将输入的图像或文本描述直接转化为对应的3D世界内容。
*   **条件视频生成：** 具备基于特定条件生成全景视频的能力。
*   **全景3D重建：** 实现对全景图像或视频内容的3D重建。
*   **强大的泛化能力：** 基于自研的3D数据和视频模型先验，能够生成多样化且高质量的3D场景。

#### 技术原理
Matrix-3D的核心技术原理在于其对**全景表示（panoramic representation）**的利用，以实现广覆盖、全向可探索的3D世界生成。它融合了以下关键技术：
*   **条件视频生成（conditional video generation）：** 通过深度学习模型，根据输入条件（如图像或文本）生成符合要求的全景视频序列。
*   **全景3D重建（panoramic 3D reconstruction）：** 运用计算机视觉和图形学技术，从全景图像或视频中恢复场景的几何信息和结构。
*   **3D数据和视频模型先验（3D data and video model priors）：** 模型在大量自研的3D数据和视频数据上进行训练，学习到丰富的场景结构和动态规律，从而增强了生成结果的真实感和多样性。

![matrix-3d1.png](https://free-img.400040.xyz/4/2025/08/12/689ada9834889.png)

#### 应用场景
*   **虚拟现实（VR）/增强现实（AR）内容创作：** 快速生成沉浸式的VR/AR环境，用于游戏、教育、旅游等领域。
*   **元宇宙（Metaverse）构建：** 为元宇宙平台提供大规模、可探索的3D场景内容生成能力。
*   **影视动画制作：** 辅助制作人员快速生成复杂的3D场景背景或预览。
*   **虚拟漫游与规划：** 在房地产、城市规划、室内设计等领域，用于生成虚拟漫游体验。
*   **数字孪生（Digital Twin）：** 构建现实世界的虚拟副本，进行模拟和分析。
*   **游戏开发：** 提升游戏场景的生成效率和多样性，实现更加生动逼真的游戏世界。


Matrix-3D的项目地址
* 项目官网：https://matrix-3d.github.io/
* GitHub仓库：https://github.com/SkyworkAI/Matrix-3D
* HuggingFace模型库：https://huggingface.co/Skywork/Matrix-3D
* 技术论文：https://github.com/SkyworkAI/Matrix-3D/blob/main/asset/report.pdf


## Matrix-Game 2.0 – 昆仑万维推出的自研世界模型

Matrix-Game 2.0是由昆仑万维SkyWork AI发布的一款自研世界模型，被誉为业内首个开源的通用场景实时长序列交互式生成模型。它旨在推动交互式世界模型领域的发展，能够实现可控的游戏世界生成，并支持高质量、实时、长序列的视频生成。

![matrix-game.png](https://free-img.400040.xyz/4/2025/08/12/689ab9c15ab9e.png)

![matrix.png](https://free-img.400040.xyz/4/2025/08/12/689ab9b869577.png)

#### 核心功能

*   **交互式世界生成:** 能够根据指令生成和操控游戏世界，实现高度交互性。
*   **实时长序列视频生成:** 以25 FPS的超高速率生成分钟级的高质量视频，支持多样的场景。
*   **基础世界模型:** 作为交互式世界的基础模型，参数量达到17B，可用于构建复杂的虚拟环境。
*   **全面开源:** 提供模型权重和相关资源，促进社区共同发展和应用。

#### 技术原理

*   **生成对抗网络 (GAN) 或扩散模型 (Diffusion Models):** 用于高质量的图像和视频内容生成。
*   **序列建模:** 采用Transformer等架构处理长序列的交互和状态变化，以实现实时且连贯的世界演进。
*   **强化学习 (Reinforcement Learning) 或模仿学习 (Imitation Learning):** 用于训练模型理解用户意图并生成可控的交互行为。
*   **多模态融合:** 结合视觉、文本、动作等多种模态信息，以构建更丰富的世界表征。
*   **高效推理优化:** 实现25 FPS的实时生成速度，可能采用了量化、剪枝或并行计算等优化技术。

#### 应用场景

*   **具身AI训练:** 为具身智能体提供逼真且可控的训练环境。
*   **虚拟现实/元宇宙构建:** 快速生成和定制虚拟世界内容，提升用户体验。
*   **游戏开发:** 自动化游戏场景、角色行为和故事情节的生成，大幅提高开发效率。
*   **数字孪生:** 创建真实世界的虚拟复刻，用于模拟、预测和优化。
*   **内容创作:** 辅助艺术家和设计师进行概念设计、动画制作和电影预可视化。


* 项目官网：https://matrix-game-v2.github.io/
* GitHub仓库：https://github.com/SkyworkAI/Matrix-Game
* HuggingFace模型库：https://huggingface.co/Skywork/Matrix-Game-2.0
* 技术报告：https://github.com/SkyworkAI/Matrix-Game/blob/main/Matrix-Game-2/assets/pdf/report.pdf


## GLM-4.5V – 智谱开源的最新一代视觉推理模型

GLM-4.5V是由智谱AI开发并开源的领先视觉语言模型（VLM），它基于智谱AI新一代旗舰文本基座模型GLM-4.5-Air（总参数1060亿，活跃参数120亿）。该模型继承并发展了GLM-4.1V-Thinking的技术路线，旨在提升多模态感知之上的高级推理能力，以解决复杂AI任务，并支持长上下文理解和多模态智能体应用。

![glm-4.5v.jpeg](https://free-img.400040.xyz/4/2025/08/12/689ab9c38823b.jpeg)

![glm.jpeg](https://free-img.400040.xyz/4/2025/08/12/689ab9c15be20.jpeg)



#### 核心功能
*   **多模态理解与感知：** 能够处理和理解图像、视频、文档等多源异构数据。
*   **高级推理能力：** 具备强大的长上下文理解、科学问题解决（STEM Reasoning）和代理（Agentic）能力。
*   **代码与GUI操作：** 支持代码理解、生成以及图形用户界面（GUI）的自动化操作。
*   **工具调用：** 支持函数调用、知识库检索和网络搜索等工具集成。
*   **混合推理模式：** 提供“思考模式”用于复杂推理和工具使用，以及“非思考模式”用于即时响应。

#### 技术原理
GLM-4.5V的技术核心在于其 **“思考模式”(Thinking Mode)** 和 **多模态强化学习（Multimodal Reinforcement Learning, RL）**。它基于大规模Transformer架构，以GLM-4.5-Air作为其文本基础模型。通过采用GLM-4.1V-Thinking的先进方法，模型在多模态数据上进行了大规模训练，并结合可扩展的强化学习策略，显著增强了其复杂问题解决、长上下文处理和多模态代理能力。模型响应中的边界框（Bounding Box）坐标通过特殊标记` <|begin_of_box|> `和` <|end_of_box|> `表示，坐标值通常在0到1000之间归一化，用于视觉定位。

#### 应用场景
*   **智能助理与Agent：** 构建能够执行复杂多模态任务的智能代理，如内容创作、信息检索、自动化流程。
*   **教育与研究：** 辅助科学、技术、工程、数学（STEM）领域的复杂问题求解。
*   **文档处理与分析：** 进行长文档理解、内容识别与提取。
*   **自动化测试与操作：** 实现基于GUI的应用程序自动化操作，如UI测试、任务执行。
*   **多媒体内容分析：** 应用于图像和视频内容理解、分析与生成。
*   **编码辅助：** 作为代码助手，进行代码理解和生成。


* GitHub仓库：https://github.com/zai-org/GLM-V/
* HuggingFace模型库：https://huggingface.co/collections/zai-org/glm-45v-68999032ddf8ecf7dcdbc102
* 技术论文：https://github.com/zai-org/GLM-V/tree/main/resources/GLM-4.5V_technical_report.pdf
* 桌面助手应用：https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App

## DreamVVT – 字节联合清华推出的视频虚拟试穿技术

DreamVVT是由字节跳动与清华大学（深圳）联合推出的一项视频虚拟试穿（Video Virtual Try-On, VVT）技术。该项目旨在通过先进的AI模型实现高保真、逼真的视频虚拟服装试穿效果，尤其强调在“野外”场景下（即非受控环境）的真实感和鲁棒性。
![dream.png](https://free-img.400040.xyz/4/2025/08/12/689abb14f0baf.png)



#### 核心功能
DreamVVT的核心功能是实现用户在视频中进行虚拟服装试穿。具体包括：
*   **高保真试穿效果**：生成逼真、细节丰富的虚拟试穿视频。
*   **视频流适配**：支持在视频内容中进行动态、连续的服装替换和试穿。
*   **“野外”场景适用性**：能够在复杂、非受控的真实视频环境中稳定运行，克服光照、姿态变化等挑战。
*   **服装风格转移**：将目标服装的样式和纹理精确地应用到视频中的人物身上。

![DreamVVT.png](https://free-img.400040.xyz/4/2025/08/12/689ab9bd34bbb.png)

#### 技术原理
DreamVVT技术基于扩散Transformer（DiTs）框架，并采用两阶段（或称为分阶段）方法实现。其主要技术原理包括：
*   **扩散Transformer (DiTs)**：利用Transformer架构的强大建模能力处理扩散过程，以生成高质量的图像和视频内容。扩散模型在生成逼真图像方面表现出色，能够逐步去噪生成目标图像。
*   **两阶段框架**：通过分解任务为不同阶段来提高生成质量和稳定性。这可能包括初步的姿态对齐/服装变形阶段和随后的高保真渲染阶段。
*   **LoRA (Low-Rank Adaptation) 适配器**：结合LoRA技术，以高效地微调预训练模型，使其适应视频虚拟试穿的特定任务，同时减少计算资源消耗和模型大小。
*   **利用无配对数据**：该框架能够有效利用无配对的人像和服装数据进行训练，这大大降低了数据采集的难度和成本，使其在实际应用中更具灵活性。

#### 应用场景
DreamVVT技术在多个领域具有广阔的应用前景，主要包括：
*   **在线零售与电商**：消费者可以在购买前通过视频观看服装在自己身上的虚拟试穿效果，提升购物体验和决策效率，减少退货率。
*   **时尚产业**：用于服装设计、展示和营销，设计师可以快速预览设计效果，品牌可以制作更具吸引力的虚拟宣传视频。
*   **影视制作与内容创作**：在电影、电视节目、广告或短视频中快速、高效地更换演员的服装，节省后期制作成本。
*   **虚拟形象与元宇宙**：为虚拟形象和元宇宙中的用户提供个性化的虚拟服装试穿服务，增强沉浸感和互动性。


DreamVVT的项目地址
*  项目官网：https://virtu-lab.github.io/
*  Github仓库：https://github.com/Virtu-Lab/DreamVVT
*  arXiv技术论文：https://arxiv.org/pdf/2508.02807v1

## Skywork UniPic 2.0 – 昆仑万维开源的统一多模态模型

#### 简介
Skywork UniPic 2.0 是昆仑万维开源的高效多模态模型，致力于实现统一的图像生成、编辑和理解能力。该模型旨在通过统一的架构处理视觉信息，提升多模态任务的效率和性能。

![unipicv2-pipeline.png](https://free-img.400040.xyz/4/2025/08/13/689c0dfff2c94.png)

#### 核心功能
*   **图像生成：** 能够根据文本描述或其他输入生成高质量图像。
*   **图像编辑：** 提供对图像内容的编辑和修改能力。
*   **图像理解：** 具备对图像进行语义理解和分析的能力。
*   **统一多模态处理：** 将图像生成、编辑和理解等功能集成在一个模型框架内，实现多任务处理。

#### 技术原理
Skywork UniPic 2.0 基于2B参数的SD3.5-Medium架构（部分资料提及UniPic为1.5B参数的自回归模型，但2.0版本主要强调SD3.5-Medium架构）。其核心技术原理包括：
*   **自回归模型（Autoregressive Model）：** 通过预测序列中的下一个元素来逐步生成内容，尤其在图像生成和理解中表现出强大能力。
*   **多模态预训练：** 模型通过大规模多模态数据进行预训练，学习图像与文本之间的深层关联。
*   **渐进式双向特征融合：** 采用先进的特征融合技术，有效整合不同模态的信息，增强模型的跨模态理解与生成能力。
*   **统一表示学习：** 旨在学习一种统一的视觉和文本表示，使得模型能够在一个共享的潜在空间中进行多模态任务处理。

#### 应用场景
*   **内容创作：** 辅助设计师、艺术家和营销人员快速生成创意图像、广告素材等。
*   **图像处理与分析：** 在图像编辑软件中集成，实现智能图像修复、风格迁移或内容修改；在安防、医疗等领域进行图像识别和分析。
*   **多模态交互系统：** 作为智能助手或聊天机器人的一部分，支持用户通过自然语言进行图像查询、生成和编辑。
*   **教育与研究：** 为多模态AI领域的研究人员提供开源模型和工具，推动技术发展和创新应用。

* 项目官网：https://unipic-v2.github.io/
* GitHub仓库：https://github.com/SkyworkAI/UniPic/tree/main/UniPic-2
* HuggingFace模型库：https://huggingface.co/collections/Skywork/skywork-unipic2-6899b9e1b038b24674d996fd
* 技术论文：https://github.com/SkyworkAI/UniPic/blob/main/UniPic-2/assets/pdf/UNIPIC2.pdf


##  Voost – 创新的双向虚拟试穿和试脱AI模型

Voost 是由 Seungyong Lee 和 Jeong-gi Kwak (来自 NXN Labs) 共同开发的一个统一且可扩展的扩散变换器 (Diffusion Transformer) 框架。它旨在解决虚拟试穿中服装与身体对应关系建模的挑战，并首次将虚拟试穿 (Virtual Try-On) 和虚拟脱衣 (Virtual Try-Off) 功能整合到单一模型中，实现了双向处理，显著提高了虚拟服装合成的真实性和泛化能力。

![voost.png](https://free-img.400040.xyz/4/2025/08/13/689c5ce65dca6.png)

![voost2.png](https://free-img.400040.xyz/4/2025/08/13/689c5ce79ab2f.png)

#### 核心功能
*   **统一的虚拟试穿与脱衣：** 在一个模型中同时支持虚拟试穿（将服装穿到人体上）和虚拟脱衣（将服装从人体上移除）功能。
*   **高保真图像合成：** 生成高度逼真的人体穿着或脱下目标服装的图像。
*   **鲁棒性：** 能够适应不同的人体姿态、服装类别、背景、光照条件和图像构图，保持高质量输出。
*   **双向监督学习：** 通过联合建模试穿和脱衣任务，利用服装-人体对在两个方向上进行监督，提高模型的准确性和灵活性。
*   **可扩展性：** 作为一个统一且可扩展的框架，具有良好的性能和应用潜力。

![Voost.jpg](https://free-img.400040.xyz/4/2025/08/13/689c5ce8c6942.jpg)

#### 技术原理
Voost 的核心技术是其提出的“统一且可扩展的扩散变换器 (Unified and Scalable Diffusion Transformer)”。该模型利用扩散模型 (Diffusion Model) 在图像生成方面的强大能力，结合变换器架构 (Transformer Architecture) 处理序列和长距离依赖的优势，以端到端的方式学习虚拟试穿和脱衣的复杂映射关系。通过一个单一的扩散变换器，Voost 能够：
*   **联合学习：** 同时编码和解码人体与服装之间的复杂交互，实现试穿和脱衣任务的协同优化。
*   **细节建模：** 扩散模型逐步去噪的特性使其能够生成高分辨率且细节丰富的图像，精确模拟服装的褶皱、纹理和与身体的贴合。
*   **变换器架构：** 使得模型能够捕捉图像中不同区域（如人体姿态、服装形状、背景信息）之间的全局依赖关系，增强了模型处理复杂场景的能力和泛化性。
*   **双向流：** 实现从服装到人体的合成 (Try-On) 和从人体到服装的逆向合成 (Try-Off)，从而在训练中提供更丰富的监督信号，提升模型对服装-身体对应关系的理解。

#### 应用场景
*   **在线虚拟试穿平台：** 消费者可以在线预览服装穿着效果，提高购物体验和决策效率。
*   **服装设计与制造：** 设计师快速验证服装设计在不同人体模型上的效果，加速设计迭代周期。
*   **时尚内容创作：** 快速生成高质量的时尚宣传图片或视频，用于广告、社交媒体等。
*   **虚拟现实/增强现实：** 为元宇宙、虚拟形象和游戏中的服装更换提供技术支持。
*   **个性化推荐系统：** 结合用户身体特征和偏好，推荐最适合的服装并展示虚拟试穿效果。

* 项目官网：https://nxnai.github.io/Voost/
* Github仓库：https://github.com/nxnai/Voost
* arXiv技术论文：https://arxiv.org/pdf/2508.04825

## Qwen-Image-Edit

Qwen-Image-Edit 是由阿里通义（Qwen）团队推出的全能图像编辑模型，其核心构建于200亿参数的Qwen-Image架构之上。该模型融合了语义与外观层面的双重编辑能力，旨在提供精确、高效的图像内容修改。

[![qwen.png](https://i.postimg.cc/6pFv5CHj/qwen.png)](https://postimg.cc/tstTBnGW)

[![qwen1.png](https://i.postimg.cc/nzJ9qpfq/qwen1.png)](https://postimg.cc/Mc9GSkTG)

#### 核心功能
*   **语义级图像编辑：** 能够理解并修改图像中的概念、对象或场景，实现高层次的内容调整。
*   **外观级图像编辑：** 支持对图像的低层次视觉细节进行编辑，如添加、删除元素或调整视觉风格。
*   **双语文本精准编辑：** 具备对图片内中英文文本进行精确修改的能力，同时保持原图风格一致性。

#### 技术原理
Qwen-Image-Edit 基于大型预训练的视觉-语言模型（VLMs）——Qwen-Image，该模型拥有200亿参数，使其具备强大的图像理解与生成能力。其实现双重编辑能力可能采用了多模态融合技术，结合扩散模型（Diffusion Models）进行高质量图像生成与编辑，并通过条件控制机制（如文本提示、掩码）来引导编辑过程。针对文本编辑，模型可能利用了其多语言理解能力，结合图像内容上下文进行文本嵌入、渲染及融合，以确保编辑的自然性和风格保持。

#### 应用场景
*   **专业内容创作：** 辅助设计师、营销人员快速修改图像素材，满足广告、社交媒体等需求。
*   **个性化图像处理：** 用户可以利用其进行高级的个人照片编辑，实现定制化效果。
*   **电子商务与产品展示：** 快速调整商品图片，如修改产品标签、文字说明或细节展示。
*   **智能图像辅助：** 在办公、教育等场景中，实现对图片信息的快速修改与更新。
*   **学术研究与开发：** 作为图像编辑领域的基础模型，可用于进一步的算法研究与应用探索。

* 项目官网：https://qwenlm.github.io/blog/qwen-image-edit/
* GitHub仓库：https://github.com/QwenLM/Qwen-Image
* HuggingFace模型库：https://huggingface.co/Qwen/Qwen-Image-Edit
* 在线体验Demo：https://huggingface.co/spaces/Qwen/Qwen-Image-Edit

##  ToonComposer – 腾讯联合港中文、北大推出的AI动画制作工具

ToonComposer是由腾讯ARC实验室开发的一款生成式AI工具，旨在彻底改变和简化传统的卡通及动漫制作流程。它主要通过自动化关键帧之间的中间帧生成（inbetweening）工作，极大地提高了动画制作效率，减少了人工工作量。

[![toon.png](https://i.postimg.cc/qBQ66PbC/toon.png)](https://postimg.cc/fktR6rhw)

[![toon1.png](https://i.postimg.cc/hPjzL26Z/toon1.png)](https://postimg.cc/rzBwcN85)

#### 核心功能
*   **自动化中间帧生成：** 能够根据用户输入的草图关键帧，智能地生成完整的动画帧序列，填补关键帧之间的空隙。
*   **高效率与成本节约：** 显著节省约70%的人工工作量，加速动画制作周期。
*   **精确控制：** 提供对草图关键帧的精确控制能力，允许用户精细调整动画效果。
*   **智能填充与区域控制：** 支持智能图像填充和特定区域的精细化控制，确保生成内容的质量和一致性。

#### 技术原理
ToonComposer采用先进的生成式人工智能（Generative AI）技术，特别是通过“生成式关键帧后处理”（Generative Post-Keyframing）方法来驱动动画帧的生成。其核心在于利用深度学习模型理解关键帧间的运动和形态变化，并自主合成中间帧，从而实现动画的平滑过渡。这一技术统一了传统的动画插帧过程，摆脱了对每一帧手动绘制的依赖。

#### 应用场景
*   **卡通动画制作：** 广泛应用于卡通片、动漫剧集的制作，特别是在需要大量中间帧的场景。
*   **电影与游戏动画：** 可用于影视特效和游戏动画的预制作或辅助制作，提高生产效率。
*   **个人创作者与小型工作室：** 赋能独立动画师和小型团队，降低动画制作的技术门槛和成本。
*   **教育与研究：** 作为AI在内容创作领域应用的案例，可用于相关领域的教学和研究。


ToonComposer的项目地址
* 项目官网：https://lg-li.github.io/project/tooncomposer/
* GitHub仓库：https://github.com/TencentARC/ToonComposer
* HuggingFace模型库：https://huggingface.co/TencentARC/ToonComposer
* arXiv技术论文：https://arxiv.org/pdf/2508.10881
* 在线体验Demo：https://huggingface.co/spaces/TencentARC/ToonComposer

## 混元3D世界模型1.0推出Lite版本

腾讯混元世界模型1.0（Hunyuan World Model 1.0）是腾讯发布的一款基于AI的开源3D场景生成模型。它能够将文本描述或单张图片快速转化为高质量、可探索、360度的沉浸式3D虚拟世界，极大地简化了传统3D内容创作的复杂流程，实现分钟级生成。

[![image.png](https://i.postimg.cc/288vHkbw/image.png)](https://postimg.cc/8s9jcVbJ)

#### 核心功能
*   **文本/图像到3D场景生成：** 能够根据文字指令或图片输入，自动生成完整的3D场景。
*   **360度可探索环境：** 生成的场景支持360度全景视图，并具有可探索性。
*   **标准格式输出：** 生成的3D内容可导出为标准网格文件（如Mesh），兼容主流游戏引擎和CG软件。
*   **高生成质量：** 在纹理细节、真实感、美学质量和指令遵循方面表现出色。
*   **内容编辑能力：** 支持对前景物体进行调整以及替换天空背景，满足个性化创作需求。

#### 技术原理
腾讯混元世界模型1.0的生成架构核心在于结合了多项先进技术：
*   **全景代理生成：** 实现全景图像的合成。
*   **语义分层：** 对场景中的元素进行语义理解和分层处理。
*   **分层3D重建：** 通过分层结构对场景进行高精度3D重建，确保场景的深度和结构准确性。
*   **AIGC技术：** 综合运用人工智能生成内容技术，自动化复杂的3D建模过程。

#### 应用场景
*   **游戏开发：** 快速生成游戏原型、关卡设计所需的3D场景、建筑、地形和植被等元素。
*   **虚拟现实（VR）/元宇宙内容创作：** 为VR体验和虚拟世界构建提供沉浸式3D环境。
*   **影视动画制作：** 作为辅助工具，加速场景的搭建和概念验证。
*   **数字艺术与设计：** 帮助设计师和艺术家快速实现创意，无需专业的3D建模技能。
*   **教育与模拟：** 创建逼真的3D环境用于教学、训练和模拟。
*   **传统CG工作流集成：** 支持与现有计算机图形工作流的无缝衔接，进行编辑和物理仿真。



* 官网地址：https://3d.hunyuan.tencent.com/sceneTo3D

* Github 项目地址：https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0

* Hugging Face模型地址：https://huggingface.co/tencent/HunyuanWorld-1

* 技术报告地址：https://arxiv.org/abs/2507.21809


## DINOv3 – Meta开源的通用视觉基础模型

DINOv3是Meta AI推出的一款通用、SOTA（State-of-the-Art）级视觉基础模型，通过大规模自监督学习（SSL）进行训练。它能够从无标注数据中学习并生成高质量的高分辨率视觉特征，旨在提供强大的通用视觉骨干网络，并在各种视觉任务和领域中实现突破性性能。DINOv3在DINOv2的基础上进一步扩展了模型规模和训练数据量，并支持商业许可。

[![dinov3.png](https://i.postimg.cc/02WSFT7q/dinov3.png)](https://postimg.cc/YhFjmyXX)

#### 核心功能
*   **高分辨率特征生成**: 能够产生高质量且分辨率高的图像特征，适用于需要精细视觉信息的任务。
*   **通用视觉骨干网络**: 提供一个强大的、适用于多种视觉任务的通用视觉骨干模型。
*   **自监督学习能力**: 利用海量无标注数据进行训练，无需人工标注即可学习强大的视觉表征。
*   **跨领域性能优越**: 在图像分类、语义分割、目标检测、视频目标跟踪等多种探测任务上超越传统弱监督模型。
*   **适应性强**: 预训练的DINOv3模型可以通过轻量级适配器在少量标注数据上进行定制，实现更广泛的应用。
*   **部署灵活性**: 包含蒸馏后的较小模型（如ViT-B、ViT-L）和ConvNeXt变体，便于在不同场景下部署。

#### 技术原理
DINOv3的核心技术原理在于**大规模自监督学习（SSL）**。它在DINOv2的基础上进行了显著的扩展，模型参数量达到7B，训练数据集规模达到1.7B图像，但相比弱监督方法，所需的计算资源更少。
1.  **自监督训练**: 通过构建无需人工标注的任务来学习图像的内在结构和特征，例如预测图像不同视图之间的关系。
2.  **视觉Transformer (ViT) 架构**: 采用ViT作为其特征提取器，能够有效处理高分辨率图像并捕捉全局依赖关系。
3.  **模型与数据规模化**: 通过将模型尺寸扩大6倍、训练数据量扩大12倍，显著提升了模型性能和泛化能力。
4.  **冻结骨干网络**: 在许多应用中，DINOv3的骨干网络可以保持冻结状态，无需额外微调即可应用于新任务，这得益于其强大的通用特征提取能力。
5.  **轻量级适配器**: 对于特定任务，可以通过训练一个轻量级适配器来微调模型，而非对整个大型模型进行重新训练，从而提高效率。

#### 应用场景
*   **图像分类**: 对各种图像进行准确的类别识别。
*   **语义分割**: 精确识别图像中每个像素所属的对象类别。
*   **目标检测**: 在图像中定位并识别出特定对象。
*   **视频目标跟踪**: 在视频序列中持续追踪特定目标。
*   **医学影像分析**: 处理和理解医学图像，辅助诊断。
*   **卫星图像分析**: 支持对卫星影像进行解析，例如冠层高度估计等。
*   **遥感图像处理**: 应用于土地利用、环境监测等领域的遥感数据分析。
*   **安防监控**: 进行智能视频监控中的行为识别与异常检测。
*   **自动驾驶**: 用于环境感知和目标识别。
*   **任何标注数据稀缺的视觉任务**: 由于其强大的自监督学习能力，特别适用于缺乏大量标注数据的领域。


* 项目官网：https://ai.meta.com/blog/dinov3-self-supervised-vision-model/
* HuggingFace模型库：https://huggingface.co/docs/transformers/main/en/model_doc/dinov3
* 技术论文：https://ai.meta.com/research/publications/dinov3/


## CombatVLA – 淘天3D动作游戏专用VLA模型

CombatVLA 是由淘天集团未来生活实验室团队开发的一种高效视觉-语言-动作（VLA）模型，专为3D动作角色扮演游戏（ARPG）中的战斗任务设计。该模型旨在通过整合视觉感知、语言理解和动作控制，提升AI在复杂游戏环境中的表现。

[![comatvla.png](https://i.postimg.cc/3wpDyWVK/comatvla.png)](https://postimg.cc/Jyrhv7Rg)

[![combatvla.png](https://i.postimg.cc/3xkGTtZ0/combatvla.png)](https://postimg.cc/NKwF87SQ)

#### 核心功能
CombatVLA 的核心功能在于对3D ARPG中战斗任务的优化。它能够：
*   **战斗理解与推理：** 识别敌方位置，并进行动作推理。
*   **高效执行：** 相比现有基于VLM的游戏智能体，显著提升执行速度。
*   **多模态控制：** 处理视觉输入，并生成一系列操作指令（包括键盘和鼠标操作）来控制游戏。
*   **性能超越：** 在战斗理解方面超越了GPT-4o和Qwen2.5-VL等现有模型，并在CUBench基准测试中取得了高分。

#### 技术原理
CombatVLA 基于一个3B参数规模的VLA模型，其技术原理涉及：
*   **视觉-语言-动作（VLA）集成：** 模型将视觉信息、自然语言指令和游戏内的动作输出进行统一学习。
*   **高效推理设计：** 专门设计以实现高效推理，大幅提升处理速度。
*   **基准测试构建：** 建立了名为CUBench的战斗理解基准，通过VQA（视觉问答）格式评估模型在识别敌方位置和动作推理任务中的表现。
*   **模型架构：** 能够处理视觉输入并生成控制游戏动作的序列。

#### 应用场景
CombatVLA 的主要应用场景集中在：
*   **3D动作角色扮演游戏（ARPG）：** 尤其适用于其中复杂的战斗任务，可以作为游戏AI或游戏测试工具。
*   **智能体训练：** 为在复杂动作游戏环境中训练多模态智能体提供新的见解和方向。
*   **游戏开发：** 有助于开发更智能、更具响应性的游戏内NPC或AI对手。
*   **虚拟环境模拟：** 可应用于其他需要视觉感知、语言理解和精准操作的虚拟环境。


* 项目官网：https://combatvla.github.io/
* GitHub仓库：https://github.com/ChenVoid/CombatVLA
* arXiv技术论文：https://arxiv.org/pdf/2503.09527



**[⬆ 返回README目录](../README.md#目录)**
**[⬆ Back to Contents](../README-EN.md#contents)**