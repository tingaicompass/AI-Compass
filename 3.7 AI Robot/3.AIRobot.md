## UnifoLM-WMA-0 – 宇树科技世界模型行动框架

UnifoLM-WMA-0是宇树科技开源的世界模型-动作（World-Model-Action, WMA）架构，旨在实现通用机器人学习，适用于多类机器人本体。其核心在于构建一个能够理解机器人与环境之间物理交互规律的世界模型，并具备交互式仿真引擎和策略增强两大功能，以优化机器人的决策性能并提供合成数据进行学习。该架构已在真实机器人上部署，能够实现动作的可控生成和长期交互生成，显著提升机器人在复杂环境中的学习与决策能力。

![宇树.png](https://free-img.mofashi.ltd/5/2025/09/16/68c964d6275e6.png)

#### 核心功能
*   **动作可控生成：** 根据当前图像和预期的机器人未来动作，生成交互可控的视频，辅助机器人进行行为预测与规划。
*   **长期交互生成：** 能够支持长时序任务的持续交互生成，适用于需要复杂序列操作的场景。
*   **策略增强：** 通过预测机器人与环境的未来交互过程，进一步优化决策策略，提升机器人在复杂环境中的适应性和性能。
*   **仿真引擎：** 作为交互式模拟器运行，生成大量合成数据，用于机器人模型的学习和训练，提高模型的泛化能力。

![宇树2.png](https://free-img.mofashi.ltd/5/2025/09/16/68c964d394ad8.png)

#### 技术原理
*   **世界模型（World Model）：** 利用传感器（如摄像头）获取环境状态和历史交互数据，通过深度学习模型（如Transformer或LSTM）预测未来的环境状态。该模型帮助机器人理解物理交互，并为决策模块提供前瞻性的环境信息，以制定更合理的动作规划。
*   **决策模块（Decision Module）：** 基于世界模型提供的预测信息，生成最优的决策策略。将这些策略转化为具体的机器人动作指令，确保机器人能够高效、准确地完成任务。
*   **仿真引擎（Simulation Engine）：** 运用先进的仿真技术，生成高保真的合成数据，用于世界模型和决策模块的训练。它提供丰富的环境反馈，使机器人能够更好地学习和适应真实世界。
*   **微调视频生成模型（Fine-tuned Video Generation Model）：** 在特定机器人作业数据集（如Open-X）上进行微调，使其能够根据指令生成与未来动作对应的视频。模型能够结合当前视觉输入和未来动作指令，生成可控的交互视频，从而协助机器人进行动作预测与规划。

#### 应用场景
*   **智能制造：** 协助机器人预测设备状态，优化生产流程，提高工厂的自动化水平和生产效率。
*   **货物搬运：** 在物流仓储环境中，机器人能够预测其他机器人位置、货物动态等环境变化，优化路径规划和搬运策略。
*   **库存管理：** 通过长期交互生成能力，机器人可更高效地管理库存，优化补货和存储策略。
*   **酒店服务：** 服务型机器人可在酒店环境中提供如送餐、清洁等服务，优化服务流程，提升客户体验。
*   **家庭服务：** 机器人能够执行家务劳动，如打扫、烹饪等，提供个性化的家庭辅助服务。


* 项目官网：https://unigen-x.github.io/unifolm-world-model-action.github.io/
* GitHub仓库：https://github.com/unitreerobotics/unifolm-world-model-action


## Genie Envisioner – 智元机器人平台

Genie Envisioner（GE）是智元（Zhiyuan Robotics / AgiBotTech）推出的首个面向真实世界机器人操控的统一世界模型开源平台。它旨在通过一个统一的视频生成框架，集成策略学习、评估和仿真功能，打破传统机器人学习系统分阶段开发的模式，从而实现更高效、更智能的机器人操作。

#### 核心功能
*   **统一世界模型构建:** 通过视频生成技术构建统一的机器人世界模型。
*   **策略学习与生成:** 支持机器人行为策略的自动化学习与生成。
*   **评估与仿真:** 提供功能强大的评估工具和仿真环境，用于测试和验证机器人策略。
*   **指令驱动操作:** 实现可伸缩的、指令驱动的机器人操作。
*   **开放平台:** 提供开源代码库和相关数据集，促进社区协作和研究。

#### 技术原理
Genie Envisioner 的核心技术原理是构建一个统一的**视频生成世界模型**（Unified Video-Generative World Model）。该平台整合了**策略学习**（Policy Learning）、**评估**（Evaluation）和**仿真**（Simulation）机制，形成一个**闭环系统**（Closed-loop System）。它利用大规模数据集（如约3000小时的机器人操作数据）进行训练，以学习和预测机器人与环境的交互。通过生成未来的视频帧，该模型能够模拟不同操作指令下的机器人行为和环境变化，从而支持**强化学习**（Reinforcement Learning）和**模型预测控制**（Model Predictive Control）等高级控制策略，最终实现指令到动作的精确转化，并克服传统**感知-规划-执行**（Perception-Planning-Execution）范式的局限性。

#### 应用场景
*   **通用机器人操作:** 适用于各种通用型机器人的精细操作和任务执行。
*   **工业自动化:** 在工厂、仓库等场景中，提高机器人抓取、组装、分拣等任务的效率和鲁棒性。
*   **家庭服务机器人:** 为家庭服务机器人提供更强的环境感知和任务执行能力。
*   **具身智能研究:** 作为具身智能领域的研究平台，推动机器人感知、决策和行动一体化的发展。
*   **教育与科研:** 为研究人员和学生提供一个开放、统一的实验平台，加速机器人技术创新。


* 项目官网：https://genie-envisioner.github.io/
* GitHub仓库：https://github.com/AgibotTech/Genie-Envisioner
* arXiv技术论文：https://arxiv.org/pdf/2508.05635


## RoboBrain 2.0 – 智谱


#### 简介  
RoboBrain 2.0是由北京智源人工智能研究院（BAAI）开发的开源具身视觉语言基础模型，旨在统一物理环境中复杂具身任务的感知、推理与规划能力。模型包含轻量级7B和全尺寸32B两种变体，采用异构架构（视觉编码器+语言模型），在空间推理（如可达性预测、空间指称）和时间决策（如闭环交互、多智能体长程规划）等任务中表现优异，超越多数开源及专有模型，是当前最强大的开源具身智能模型之一。

 ![](https://article-images.zsxq.com/Ftv7qykPwTQDrLkQtipgjrgRbsQA)

![](https://article-images.zsxq.com/FjH7luIg62kDZUvLR5DErmP4z1fX)  

![](https://article-images.zsxq.com/FieAwDZOXd75A9DL4FnvrZpK-drt)  

#### 核心功能  
1. **空间理解**：支持精确的空间指称（点、边界框预测）、可达性预测（如抓取杯子的手柄）、轨迹预测及场景推理（实时场景图构建与更新）。  
2. **时间决策**：具备长程规划与闭环反馈能力，支持多智能体长程协作任务（如超市补货、餐厅服务）及实时场景记忆更新。  
3. **多模态处理**：支持多图像、长视频、高分辨率视觉输入，结合复杂任务指令与结构化场景图，输出结构化计划、空间关系及绝对/相对坐标。  
4. **推理与规划**：通过思维链（CoT）推理生成多步决策轨迹，支持任务分解与动态环境适应（如中断调整、场景快速适配）。  

#### 技术原理  
1. **模型架构**：采用模块化编解码架构，包含视觉编码器（处理高分辨率图像/视频）、MLP投影器（映射视觉特征至语言模型空间）及解码器（语言模型，支持长链推理）。视觉输入经编码器处理后与文本输入统一为多模态令牌流，由解码器生成结构化输出（如坐标、计划）。  
2. **训练数据**：覆盖通用多模态（VQA、视觉对话）、空间（视觉定位、指称、可达性）及时间（自我视角规划、多机器人协作）三类数据，通过合成与标注构建大规模高质量数据集（如空间数据合成流水线、多机器人协作模板）。  
3. **训练策略**：分三阶段训练：基础时空学习（通用感知与理解）、具身时空增强（多视角/视频数据强化长程依赖）、具身场景思维链推理（监督微调+强化微调，提升因果推理能力）。  
4. **基础设施**：基于FlagScale（分布式训练框架）和FlagEvalMM（多模态评估框架），支持混合并行训练、内存预分配及故障恢复，优化训练与推理效率。  

#### 应用场景  
1. **机器人操作**：如物体抓取（定位手柄）、室内导航（识别空闲区域）、桌面操作（物体排列）等。  
2. **多机器人协作**：家庭、超市、餐厅场景下的任务分解与协同（如补货、送餐、礼品包装）。  
3. **实时交互**：支持语音中断调整、动态场景适配（如识别物体距离/方向）及闭环任务执行（如咖啡机操作）。  
4. **智能规划**：长程任务分解（如准备咖啡、烹饪）、多步骤空间指称（如“将杯子放在笔架和键盘之间”）及轨迹生成（如机器人手臂移动路径）。


- 项目官网：https://superrobobrain.github.io/
- GitHub仓库：https://github.com/FlagOpen/RoboBrain2.0
- HuggingFace模型库：https://huggingface.co/collections/BAAI/robobrain20-6841eeb1df55c207a4ea0036
- arXiv技术论文：https://arxiv.org/pdf/2507.02029


##  RoboOS 2.0 – 智谱

#### 简介
RoboOS是首个开源具身操作系统，基于大脑-小脑分层架构，旨在解决多智能体协作中跨实体适应性差、任务调度低效及动态纠错不足等问题。其核心通过具身大脑模型（多模态大语言模型）、小脑技能库（模块化即插即用工具包）和实时共享内存（时空同步机制）的协同，支持长程任务的规划、调度与纠错，以及多智能体高效协作，并优化了边缘-云通信与分布式推理，适用于餐厅、家庭、超市等多场景的异构实体协作。

 ![](https://article-images.zsxq.com/Fi10-tdZhsvEXPqKIvnbAdzU4AxJ)

#### 核心功能
1. **全局感知与决策**：具身大脑模型（如RoboBrain）通过多模态大语言模型实现全局场景感知（3D重建、历史状态追踪）、多智能体任务分解及轨迹生成，支持动态纠错与实时重规划。
2. **模块化技能执行**：小脑技能库提供操作（VLA/专家工具）、导航（VLN/SLAM）及特殊技能（接触交互、可变形物体处理）的模块化工具，适配单臂、双臂、人形等异构实体。
3. **多智能体状态同步**：实时共享内存通过空间记忆（动态场景图）、时间记忆（任务反馈、工具调用日志）和机器人记忆（运动约束、电池状态），实现多智能体的时空协同与负载均衡。
4. **可扩展部署**：基于FlagScale框架优化边缘-云通信与分布式推理，支持高频交互与大规模云推理。

#### 技术原理
RoboOS采用大脑-小脑分层架构：
- **具身大脑模型**：以多模态大语言模型（如RoboBrain）为核心，通过三阶段训练（通用VLM、机器人专项、系统增强）强化多智能体任务规划、工具调用及记忆更新能力，结合检索增强生成（RAG）融合场景、任务、机器人状态等信息生成子任务图。
- **小脑技能库**：标准化工具与机器人配置文件实现异构实体的即插即用，支持操作（如抓握）、导航（如SLAM）及特殊技能（如灵巧手控制）的低延迟执行。
- **实时共享内存**：空间记忆通过多视角RGB-D输入构建场景图（楼层-房间-物体分层节点）；时间记忆记录任务历史；机器人记忆存储实时状态，三者协同支持任务分配与动态调整。
- **边缘-云通信**：基于FlagScale框架，采用发布-订阅机制实现低延迟（<0.001s）指令响应，结合内存优化数据引擎支持TB级历史数据访问，并行推理与多任务调度提升系统扩展性。

#### 应用场景
1. **服务机器人**：餐厅场景中，人形与双臂机器人协作完成汉堡制作与配送；家庭场景中，单臂与双臂机器人协同取递水果、刀具。
2. **零售与仓储**：超市场景下，机器人协作完成礼品挑选、包装及货架补货。
3. **工业自动化**：支持多类型工业机器人（如机械臂、轮式平台）在装配线中的任务分解与协同执行。
4. **智能制造**：通过多智能体协作优化生产流程，实现动态任务调度与错误纠正。

- 项目官网：https://github.com/FlagOpen/RoboOS
- GitHub仓库：https://github.com/FlagOpen/RoboOS
- arXiv技术论文：https://arxiv.org/pdf/2505.03673

