# 2.å¤§æ¨¡åž‹è®­ç»ƒæ¡†æž¶

LLMè®­ç»ƒæ¡†æž¶æ¨¡å—æž„å»ºäº†è¦†ç›–å…¨æ ˆçš„å¤§æ¨¡åž‹è®­ç»ƒç”Ÿæ€ç³»ç»Ÿï¼Œé›†æˆ20+ä¸“ä¸šè®­ç»ƒæ¡†æž¶å’Œå·¥å…·ã€‚**æ ¸å¿ƒæ¡†æž¶**åŒ…æ‹¬ï¼š**é­”å¡”ms-swift**ï¼ˆæ”¯æŒ500+ LLMså’Œ200+ MLLMsçš„å…¨å‚æ•°/PEFTè®­ç»ƒï¼‰ã€**Unsloth**ï¼ˆ2-5å€åŠ é€Ÿï¼Œ80%å†…å­˜èŠ‚çœï¼‰ã€**è‹±ä¼Ÿè¾¾Megatron-LM**ï¼ˆè¶…å¤§è§„æ¨¡transformerè®­ç»ƒï¼‰ã€**å¾®è½¯DeepSpeed**ï¼ˆZeROä¼˜åŒ–å™¨ï¼‰ã€**ColossalAI**ï¼ˆé«˜æ€§èƒ½åˆ†å¸ƒå¼è®­ç»ƒï¼‰ã€**Meta FairScale**ã€**LLaMA-Factory**ï¼ˆWebUIç•Œé¢ï¼Œæ”¯æŒ100+æ¨¡åž‹ï¼‰ã€**ä¹¦ç”ŸXTuner**ç­‰ã€‚**å…ˆè¿›ç®—æ³•**æ¶µç›–GaLoreæ¢¯åº¦ä½Žç§©æŠ•å½±ã€BAdamå†…å­˜é«˜æ•ˆä¼˜åŒ–ã€APOLLOã€Adam-miniã€Muonç­‰å‰æ²¿ä¼˜åŒ–å™¨ã€‚**å®žéªŒç›‘æŽ§**æä¾›MLflowã€WandBã€SwanLabç­‰ä¸“ä¸šå·¥å…·ã€‚é…å¥—**Flash Attention**ã€**Liger Kernel**ç­‰åŠ é€ŸæŠ€æœ¯ï¼Œä»¥åŠ**Easy Dataset**æ•°æ®æž„é€ å·¥å…·ï¼Œå½¢æˆä»Žæ•°æ®å‡†å¤‡ã€æ¨¡åž‹è®­ç»ƒåˆ°å®žéªŒç®¡ç†çš„å®Œæ•´é—­çŽ¯ã€‚

- [huggingface/peft: ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)
- [huggingface/transformers: ðŸ¤— Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.](https://github.com/huggingface/transformers)
- [å‚æ•°é‡è®¡ç®—å™¨ï¼ˆNvidia GPU å’Œ Apple Siliconï¼‰ --- Can You Run This LLM? VRAM Calculator (Nvidia GPU and Apple Silicon)](https://apxml.com/tools/vram-calculator)
- [pytorch/torchtune: PyTorch native post-training library](https://github.com/pytorch/torchtune)

------------------------------------------------------------

1. 2.å¤§æ¨¡åž‹è®­ç»ƒæ¡†æž¶.md
2. 0.ms-swift-é­”å¡”è®­ç»ƒæ¡†æž¶
3. 0.unsloth
4. 1. Megatronè‹±ä¼Ÿè¾¾
5. 1.ColossalAI
6. 1.DeepSpeed-å¾®è½¯
7. 1.FairScale-meta
8. 1.Horovod
9. 1.LLaMA-Factory
10. 1.LLaMA-Factory/easy-data
11. 1.xtuner-ä¹¦ç”Ÿæµ¦æº
12. 1.å®žéªŒç›‘æŽ§
13. 1.å®žéªŒç›‘æŽ§/SwanLab
14. 1.æ¨¡åž‹è®­ç»ƒ-å®žç”¨æŠ€å·§
15. 1.è®­ç»ƒå…ˆè¿›ç®—æ³•
16. 2.Firefly
17. 2.MMEngine
18. 2.fastAI
19. 3.openai-åœ¨çº¿å¾®è°ƒ



# 0.ms-swift-é­”å¡”è®­ç»ƒæ¡†æž¶

- [Swift DOCUMENTATION â€” swift 2.5.0.dev0 æ–‡æ¡£](https://swift.readthedocs.io/zh-cn/latest/)
- [swift](https://github.com/modelscope/swift/blob/main/README_CN.md#-%E6%96%87%E6%A1%A3)
- [modelscope/ms-swift: Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, GLM4, Mistral, Yi1.5, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, MiniCPM-V-2.6, GLM4v, Xcomposer2.5, DeepSeek-VL2, Phi4, GOT-OCR2, ...).](https://github.com/modelscope/ms-swift)
- [modelscope-classroom/LLM-tutorial/A.æ·±åº¦å­¦ä¹ å…¥é—¨ä»‹ç».md at main Â· modelscope/modelscope-classroom](https://github.com/modelscope/modelscope-classroom/blob/main/LLM-tutorial/A.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md)

------------------------------------------------------------

# 0.unsloth

- [unslothai/unsloth: Finetune Llama 3.1, Mistral, Phi & Gemma LLMs 2-5x faster with 80% less memory](https://github.com/unslothai/unsloth)
- [Fine-tuning Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-guide)

------------------------------------------------------------

# 1. Megatronè‹±ä¼Ÿè¾¾

- [NVIDIA/Megatron-LM: Ongoing research training transformer models at scale](https://github.com/nvidia/megatron-lm)

------------------------------------------------------------

# 1.ColossalAI

- [Colossal-AI](https://colossalai.org/)
- [hpcaitech/ColossalAI: Making large AI models cheaper, faster and more accessible](https://github.com/hpcaitech/ColossalAI)
- [ColossalAI/docs/README-zh-Hans.md at main Â· hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md)

------------------------------------------------------------

# 1.DeepSpeed-å¾®è½¯

- [microsoft/DeepSpeed:](https://github.com/microsoft/DeepSpeed/tree/master)
- [DeepSpeed](https://www.deepspeed.ai/training/)
- [DeepSpeed/blogs/deepspeed-chat/chinese at master Â· microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/chinese)
- [microsoft/DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples)
- [DeepSpeed](https://github.com/microsoft/DeepSpeed)
- [Deepspeedå¹¶è¡Œæ¡†æž¶ä»‹ç»](https://github.com/wzzzd/LLM_Learning_Note/blob/main/Parallel/deepspeed.md)

------------------------------------------------------------

# 1.FairScale-meta

- [facebookresearch/fairscale: PyTorch extensions for high performance and large scale training.](https://github.com/facebookresearch/fairscale)

------------------------------------------------------------

# 1.Horovod

- [horovod/horovod: Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.](https://github.com/horovod/horovod)

------------------------------------------------------------

# 1.LLaMA-Factory

- [hiyouga/LLaMA-Factory: å®˜æ–¹A WebUI for Efficient Fine-Tuning of 100+ LLMs (ACL 2024)](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file)
- [hiyouga/ChatGLM-Efficient-Tuning: Fine-tuning ChatGLM-6B with PEFT | åŸºäºŽ PEFT çš„é«˜æ•ˆ ChatGLM å¾®è°ƒ](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)
- [LLaMA Factoryä¸­æ–‡æ‰‹å†Œ](https://llamafactory.readthedocs.io/zh-cn/latest/)
- [åŸºäºŽ Amazon SageMaker å’Œ LLaMA-Factory æ‰“é€ ä¸€ç«™å¼æ— ä»£ç æ¨¡åž‹å¾®è°ƒéƒ¨ç½²å¹³å° Model Hub | äºšé©¬é€ŠAWSå®˜æ–¹åšå®¢](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)
- [åŸºäºŽLLaMA-Factoryå’ŒEasy Datasetçš„Qwen3å¾®è°ƒå®žæˆ˜ï¼šä»Žæ•°æ®å‡†å¤‡åˆ°LoRAå¾®è°ƒæŽ¨ç†è¯„ä¼°çš„å…¨æµç¨‹æŒ‡å—_llamafactory qwen3-CSDNåšå®¢](https://blog.csdn.net/sinat_39620217/article/details/148159347?spm=1011.2415.3001.5331)

------------------------------------------------------------

# easy-data

- [ï¼easy-dataset: ä¾¿æ·æ•°æ®æž„é€ ](https://github.com/ConardLi/easy-dataset)
- [easy-dataset/README.zh-CN.ä¾¿æ·æ•°æ®æž„é€ ](https://github.com/ConardLi/easy-dataset/blob/main/README.zh-CN.md)
- [â€â¡â€¬â€¬â€¬â€¬â€Œâ€‹â€¬â¤â€‹ï»¿â£â¤ï»¿â¢â€¬â€Œâ¢â€¬â¡â¤â€â¡ï»¿â¤ï»¿â¡â¡â¢ï»¿â£â¢ï»¿â€‹â€¬â£â€Œâ£â¢ï»¿â¡â€‹â€¬â£â¤â€Œâ€¬â¡â£Easy Dataset Ã— LLaMA Factory: è®©å¤§æ¨¡åž‹é«˜æ•ˆå­¦ä¹ é¢†åŸŸçŸ¥è¯† - é£žä¹¦äº‘æ–‡æ¡£](https://buaa-act.feishu.cn/wiki/KY9xwTGs1iqHrRkjXBwcZP9WnL9)

------------------------------------------------------------

# 1.xtuner-ä¹¦ç”Ÿæµ¦æº

- [InternLM/xtuner: An efficient, flexible and full-featured toolkit for fine-tuning LLM (InternLM2, Llama3, Phi3, Qwen, Mistral, ...)](https://github.com/InternLM/xtuner)
- [å§‹æ™ºAI-wisemodel-ä¸­ç«‹å¼€æ”¾çš„AIå¼€æºç¤¾åŒº](https://www.wisemodel.cn/organization/xtuner)

------------------------------------------------------------

# 1.å®žéªŒç›‘æŽ§

- [mlflow/mlflow: Open source platform for the machine learning lifecycle](https://github.com/mlflow/mlflow)
- [wandb/wandb: The AI developer platform. Use Weights & Biases to train and fine-tune models, and manage models from experimentation to production.](https://github.com/wandb/wandb)

------------------------------------------------------------

# SwanLab

- [SwanLab - AGIæ—¶ä»£å…ˆè¿›æ¨¡åž‹è®­ç»ƒç ”å‘å·¥å…·](https://swanlab.cn/)
- [SwanLabå®˜æ–¹æ–‡æ¡£ | å…ˆè¿›çš„AIå›¢é˜Ÿåä½œä¸Žæ¨¡åž‹åˆ›æ–°å¼•æ“Ž](https://docs.swanlab.cn/)
- [SwanHubX/SwanLab: âš¡ï¸SwanLab - an open-source, modern-design AI training tracking and visualization tool. Supports Cloud / Self-hosted use. Integrated with PyTorch / Transformers / LLaMA Factory / Swift / Ultralytics / veRL / MMEngine / Keras etc.](https://github.com/SwanHubX/SwanLab)
- [OpenBenchmark åŸºçº¿ç¤¾åŒº Â· æœ‰è¿‡ç¨‹ï¼Œæ‰æ˜¯çœŸå¼€æº | SwanLab](https://swanlab.cn/benchmarks)

------------------------------------------------------------

# 1.æ¨¡åž‹è®­ç»ƒ-å®žç”¨æŠ€å·§

- [Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention)
- [linkedin/Liger-Kernel: Efficient Triton Kernels for LLM Training](https://github.com/linkedin/Liger-Kernel)

------------------------------------------------------------

# 1.è®­ç»ƒå…ˆè¿›ç®—æ³•

- [jiaweizzhao/GaLore: GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://github.com/jiaweizzhao/GaLore)
- [Ledzy/BAdam: [NeurIPS 2024] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models](https://github.com/Ledzy/BAdam)
- [zhuhanqing/APOLLO: APOLLO: SGD-like Memory, AdamW-level Performance](https://github.com/zhuhanqing/APOLLO)
- [zyushun/Adam-mini: Code for Adam-mini: Use Fewer Learning Rates To Gain More https://arxiv.org/abs/2406.16793](https://github.com/zyushun/Adam-mini)
- [KellerJordan/Muon: Muon optimizer: +>30% sample efficiency with <3% wallclock overhead](https://github.com/KellerJordan/Muon)

------------------------------------------------------------

# 2.Firefly

- [yangjianxin1/Firefly: Firefly: å¤§æ¨¡åž‹è®­ç»ƒå·¥å…·ï¼Œæ”¯æŒè®­ç»ƒQwen2ã€Yi1.5ã€Phi-3ã€Llama3ã€Gemmaã€MiniCPMã€Yiã€Deepseekã€Orionã€Xverseã€Mixtral-8x7Bã€Zephyrã€Mistralã€Baichuan2ã€Llma2ã€Llamaã€Qwenã€Baichuanã€ChatGLM2ã€InternLMã€Ziya2ã€Vicunaã€Bloomç­‰å¤§æ¨¡åž‹](https://github.com/yangjianxin1/Firefly)

------------------------------------------------------------

# 2.MMEngine

- [open-mmlab/mmengine: OpenMMLab Foundational Library for Training Deep Learning Models](https://github.com/open-mmlab/mmengine)
- [æ¬¢è¿Žæ¥åˆ° MMEngine çš„ä¸­æ–‡æ–‡æ¡£ï¼ â€” mmengine 0.10.7 æ–‡æ¡£](https://mmengine.readthedocs.io/zh-cn/latest/)

------------------------------------------------------------

# 2.fastAI

- [fastai/fastai: The fastai deep learning library](https://github.com/fastai/fastai)

------------------------------------------------------------

# 3.openai-åœ¨çº¿å¾®è°ƒ

- [OpenAI ä½¿ç”¨äº‘æŽ§åˆ¶å°è¿›è¡Œçº¿ä¸Šè®­ç»ƒå¾®è°ƒæ¨¡åž‹-è¯¦ç»†å…¥é—¨ç‰ˆ_openaiå¾®è°ƒè®­ç»ƒæ¨¡åž‹-CSDNåšå®¢](https://blog.csdn.net/chrnhao/article/details/136797372)
- [Fine-tuning - OpenAI API](https://platform.openai.com/finetune)
- [Overview - OpenAI API](https://platform.openai.com/docs/overview)

------------------------------------------------------------

**[â¬† è¿”å›žREADMEç›®å½•](../README.md#ç›®å½•)**

**[â¬† Back to Contents](../README-EN.md#contents)**