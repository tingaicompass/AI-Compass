# 2.大模型训练框架

LLM训练框架模块构建了覆盖全栈的大模型训练生态系统，集成20+专业训练框架和工具。**核心框架**包括：**魔塔ms-swift**（支持500+ LLMs和200+ MLLMs的全参数/PEFT训练）、**Unsloth**（2-5倍加速，80%内存节省）、**英伟达Megatron-LM**（超大规模transformer训练）、**微软DeepSpeed**（ZeRO优化器）、**ColossalAI**（高性能分布式训练）、**Meta FairScale**、**LLaMA-Factory**（WebUI界面，支持100+模型）、**书生XTuner**等。**先进算法**涵盖GaLore梯度低秩投影、BAdam内存高效优化、APOLLO、Adam-mini、Muon等前沿优化器。**实验监控**提供MLflow、WandB、SwanLab等专业工具。配套**Flash Attention**、**Liger Kernel**等加速技术，以及**Easy Dataset**数据构造工具，形成从数据准备、模型训练到实验管理的完整闭环。

- [huggingface/peft: 🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)
- [huggingface/transformers: 🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.](https://github.com/huggingface/transformers)
- [参数量计算器（Nvidia GPU 和 Apple Silicon） --- Can You Run This LLM? VRAM Calculator (Nvidia GPU and Apple Silicon)](https://apxml.com/tools/vram-calculator)
- [pytorch/torchtune: PyTorch native post-training library](https://github.com/pytorch/torchtune)

------------------------------------------------------------

1. 2.大模型训练框架.md
2. 0.ms-swift-魔塔训练框架
3. 0.unsloth
4. 1. Megatron英伟达
5. 1.ColossalAI
6. 1.DeepSpeed-微软
7. 1.FairScale-meta
8. 1.Horovod
9. 1.LLaMA-Factory
10. 1.LLaMA-Factory/easy-data
11. 1.xtuner-书生浦源
12. 1.实验监控
13. 1.实验监控/SwanLab
14. 1.模型训练-实用技巧
15. 1.训练先进算法
16. 2.Firefly
17. 2.MMEngine
18. 2.fastAI
19. 3.openai-在线微调



# 0.ms-swift-魔塔训练框架

- [Swift DOCUMENTATION — swift 2.5.0.dev0 文档](https://swift.readthedocs.io/zh-cn/latest/)
- [swift](https://github.com/modelscope/swift/blob/main/README_CN.md#-%E6%96%87%E6%A1%A3)
- [modelscope/ms-swift: Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, GLM4, Mistral, Yi1.5, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, MiniCPM-V-2.6, GLM4v, Xcomposer2.5, DeepSeek-VL2, Phi4, GOT-OCR2, ...).](https://github.com/modelscope/ms-swift)
- [modelscope-classroom/LLM-tutorial/A.深度学习入门介绍.md at main · modelscope/modelscope-classroom](https://github.com/modelscope/modelscope-classroom/blob/main/LLM-tutorial/A.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md)

------------------------------------------------------------

# 0.unsloth

- [unslothai/unsloth: Finetune Llama 3.1, Mistral, Phi & Gemma LLMs 2-5x faster with 80% less memory](https://github.com/unslothai/unsloth)
- [Fine-tuning Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-guide)

------------------------------------------------------------

# 1. Megatron英伟达

- [NVIDIA/Megatron-LM: Ongoing research training transformer models at scale](https://github.com/nvidia/megatron-lm)

------------------------------------------------------------

# 1.ColossalAI

- [Colossal-AI](https://colossalai.org/)
- [hpcaitech/ColossalAI: Making large AI models cheaper, faster and more accessible](https://github.com/hpcaitech/ColossalAI)
- [ColossalAI/docs/README-zh-Hans.md at main · hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md)

------------------------------------------------------------

# 1.DeepSpeed-微软

- [microsoft/DeepSpeed:](https://github.com/microsoft/DeepSpeed/tree/master)
- [DeepSpeed](https://www.deepspeed.ai/training/)
- [DeepSpeed/blogs/deepspeed-chat/chinese at master · microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/chinese)
- [microsoft/DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples)
- [DeepSpeed](https://github.com/microsoft/DeepSpeed)
- [Deepspeed并行框架介绍](https://github.com/wzzzd/LLM_Learning_Note/blob/main/Parallel/deepspeed.md)

------------------------------------------------------------

# 1.FairScale-meta

- [facebookresearch/fairscale: PyTorch extensions for high performance and large scale training.](https://github.com/facebookresearch/fairscale)

------------------------------------------------------------

# 1.Horovod

- [horovod/horovod: Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.](https://github.com/horovod/horovod)

------------------------------------------------------------

# 1.LLaMA-Factory

- [hiyouga/LLaMA-Factory: 官方A WebUI for Efficient Fine-Tuning of 100+ LLMs (ACL 2024)](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file)
- [hiyouga/ChatGLM-Efficient-Tuning: Fine-tuning ChatGLM-6B with PEFT | 基于 PEFT 的高效 ChatGLM 微调](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)
- [LLaMA Factory中文手册](https://llamafactory.readthedocs.io/zh-cn/latest/)
- [基于 Amazon SageMaker 和 LLaMA-Factory 打造一站式无代码模型微调部署平台 Model Hub | 亚马逊AWS官方博客](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)
- [基于LLaMA-Factory和Easy Dataset的Qwen3微调实战：从数据准备到LoRA微调推理评估的全流程指南_llamafactory qwen3-CSDN博客](https://blog.csdn.net/sinat_39620217/article/details/148159347?spm=1011.2415.3001.5331)

------------------------------------------------------------

# easy-data

- [！easy-dataset: 便捷数据构造](https://github.com/ConardLi/easy-dataset)
- [easy-dataset/README.zh-CN.便捷数据构造](https://github.com/ConardLi/easy-dataset/blob/main/README.zh-CN.md)
- [‍⁡‬‬‬‬‌​‬⁤​﻿⁣⁤﻿⁢‬‌⁢‬⁡⁤‍⁡﻿⁤﻿⁡⁡⁢﻿⁣⁢﻿​‬⁣‌⁣⁢﻿⁡​‬⁣⁤‌‬⁡⁣Easy Dataset × LLaMA Factory: 让大模型高效学习领域知识 - 飞书云文档](https://buaa-act.feishu.cn/wiki/KY9xwTGs1iqHrRkjXBwcZP9WnL9)

------------------------------------------------------------

# 1.xtuner-书生浦源

- [InternLM/xtuner: An efficient, flexible and full-featured toolkit for fine-tuning LLM (InternLM2, Llama3, Phi3, Qwen, Mistral, ...)](https://github.com/InternLM/xtuner)
- [始智AI-wisemodel-中立开放的AI开源社区](https://www.wisemodel.cn/organization/xtuner)

------------------------------------------------------------

# 1.实验监控

- [mlflow/mlflow: Open source platform for the machine learning lifecycle](https://github.com/mlflow/mlflow)
- [wandb/wandb: The AI developer platform. Use Weights & Biases to train and fine-tune models, and manage models from experimentation to production.](https://github.com/wandb/wandb)

------------------------------------------------------------

# SwanLab

- [SwanLab - AGI时代先进模型训练研发工具](https://swanlab.cn/)
- [SwanLab官方文档 | 先进的AI团队协作与模型创新引擎](https://docs.swanlab.cn/)
- [SwanHubX/SwanLab: ⚡️SwanLab - an open-source, modern-design AI training tracking and visualization tool. Supports Cloud / Self-hosted use. Integrated with PyTorch / Transformers / LLaMA Factory / Swift / Ultralytics / veRL / MMEngine / Keras etc.](https://github.com/SwanHubX/SwanLab)
- [OpenBenchmark 基线社区 · 有过程，才是真开源 | SwanLab](https://swanlab.cn/benchmarks)

------------------------------------------------------------

# 1.模型训练-实用技巧

- [Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention)
- [linkedin/Liger-Kernel: Efficient Triton Kernels for LLM Training](https://github.com/linkedin/Liger-Kernel)

------------------------------------------------------------

# 1.训练先进算法

- [jiaweizzhao/GaLore: GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://github.com/jiaweizzhao/GaLore)
- [Ledzy/BAdam: [NeurIPS 2024] BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models](https://github.com/Ledzy/BAdam)
- [zhuhanqing/APOLLO: APOLLO: SGD-like Memory, AdamW-level Performance](https://github.com/zhuhanqing/APOLLO)
- [zyushun/Adam-mini: Code for Adam-mini: Use Fewer Learning Rates To Gain More https://arxiv.org/abs/2406.16793](https://github.com/zyushun/Adam-mini)
- [KellerJordan/Muon: Muon optimizer: +>30% sample efficiency with <3% wallclock overhead](https://github.com/KellerJordan/Muon)

------------------------------------------------------------

# 2.Firefly

- [yangjianxin1/Firefly: Firefly: 大模型训练工具，支持训练Qwen2、Yi1.5、Phi-3、Llama3、Gemma、MiniCPM、Yi、Deepseek、Orion、Xverse、Mixtral-8x7B、Zephyr、Mistral、Baichuan2、Llma2、Llama、Qwen、Baichuan、ChatGLM2、InternLM、Ziya2、Vicuna、Bloom等大模型](https://github.com/yangjianxin1/Firefly)

------------------------------------------------------------

# 2.MMEngine

- [open-mmlab/mmengine: OpenMMLab Foundational Library for Training Deep Learning Models](https://github.com/open-mmlab/mmengine)
- [欢迎来到 MMEngine 的中文文档！ — mmengine 0.10.7 文档](https://mmengine.readthedocs.io/zh-cn/latest/)

------------------------------------------------------------

# 2.fastAI

- [fastai/fastai: The fastai deep learning library](https://github.com/fastai/fastai)

------------------------------------------------------------

# 3.openai-在线微调

- [OpenAI 使用云控制台进行线上训练微调模型-详细入门版_openai微调训练模型-CSDN博客](https://blog.csdn.net/chrnhao/article/details/136797372)
- [Fine-tuning - OpenAI API](https://platform.openai.com/finetune)
- [Overview - OpenAI API](https://platform.openai.com/docs/overview)

------------------------------------------------------------

**[⬆ 返回README目录](../README.md#目录)**

**[⬆ Back to Contents](../README-EN.md#contents)**