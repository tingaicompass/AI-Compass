# LLM测评榜

主要围绕大语言模型（LLM）和多模态模型的评测榜单展开。涵盖了多个评测平台，如LiveCodeBench、OpenCompass司南、AGI - Eval评测社区等。涉及众多模型，如Gemini - 2.5系列、Qwen系列、InternVL系列等，使用多种评测数据集，包括OlympicArena、RM - Bench、UGMathBench等，从代码能力、语言理解、视觉处理、多模态推理等多维度对模型进行评估。

OpenCompass是LLM评估平台，支持众多模型在超100个数据集上评测；SuperCLUE关注中文原生检索增强生成测评；LLM Leaderboard 2024展示2024年4月后模型公开基准性能；MMBench用于多模态模型评测。

#### 核心功能
- **模型排名展示**：为不同模型提供综合或特定维度的排名，如LiveCodeBench对模型代码能力排名，OpenCompass司南、OpenCompass和MMBench、有大语言模型、多模态模型等不同类型排名。
- **评测数据支持**：采用多种评测数据集对模型进行全面评估，如AGI - Eval评测社区的OlympicArena涵盖七大学科领域，RM - Bench评估奖励模型能力，SuperCLUE开展中文原生检索增强生成基准测评。
- **信息参考**：为行业和研究提供全面、客观、中立的评估参考，帮助用户了解模型优缺点，做出选择。

#### 技术原理
通过构建不同的评测数据集和评测指标体系，对模型在特定任务上的表现进行量化评估。如OlympicArena通过一系列学科挑战任务评估人工智能高阶能力；RM - Bench从敏感度和鲁棒性两方面评估奖励模型。依据这些评测结果，按照一定规则生成模型的排名。OpenCompass等平台通过构建多类型评测数据集，让模型在这些数据集上完成任务，根据任务表现进行量化评分。如SuperCLUE针对检索增强生成能力构建特定评测体系，以评估模型在知识获取和生成内容准确性等方面的表现。

#### 应用场景
- **模型选择**：企业和开发者在选择合适的大语言模型或多模态模型时，可参考榜单排名和评测信息。
- **技术研究**：研究人员可根据评测数据了解模型在不同维度的性能，为模型的改进和优化提供方向。
- **行业标准制定**：有助于推动行业建立统一的评测标准和规范，促进人工智能技术的健康发展。 

- [Overview Leaderboard | LMArena](https://lmarena.ai/leaderboard)
- [AI Model & API Providers Analysis | Artificial Analysis](https://artificialanalysis.ai/)
- [LiveCodeBench Leaderboard](https://livecodebench.github.io/leaderboard.html)
- [OpenCompass司南 - 评测榜单](https://rank.opencompass.org.cn/home)
- [AGI-Eval评测社区](https://agi-eval.cn/mvp/topRanking)
- [open-compass/opencompass: OpenCompass is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.](https://github.com/open-compass/opencompass)
- [SuperCLUE](https://cluebenchmarks.com/superclue_rag.html)
- [LLM Leaderboard 2024](https://www.vellum.ai/llm-leaderboard)
- [MMBench](https://mmbench.opencompass.org.cn/leaderboard)

**[⬆ 返回README目录](../README.md#目录)**
**[⬆ Back to Contents](../README-EN.md#contents)**