# LLM测评榜

LLM测评榜模块构建了涵盖20+个权威评测平台的大语言模型性能评估生态，为AI研究者和开发者提供全面的模型选型参考。该模块系统性地整合了SuperCLUE中文基准、C-Eval中文评测、HELM斯坦福评测、LMSYS Chatbot Arena、HuggingFace Open LLM Leaderboard等国内外顶级评测平台，以及OpenCompass、BIG-bench、MMLU、AlpacaEval等专业评测基准。技术维度涵盖了通用能力评测（语言理解、知识问答、逻辑推理）、专业能力评测（代码生成、数学解题、医疗诊断）、安全性评测（偏见检测、有害内容、隐私保护）等多个评估方向，详细解析了各大模型在不同任务上的表现特点和适用场景。模块深入介绍了评测方法学（人工评估、自动评估、对抗评估）、评测指标体系（准确率、流畅度、相关性、创新性）、评测数据集构建等核心技术环节，以及模型能力雷达图、性能对比矩阵、时间演进趋势等可视化分析工具。此外，还提供了企业级模型选型指南、开源模型优化方向、商业模型成本效益分析等实用决策支持，以及最新模型突破、技术趋势、应用前景等前沿洞察，帮助用户在快速迭代的LLM生态中做出明智的技术选择，构建高性能的AI应用系统。

- [Overview Leaderboard | LMArena](https://lmarena.ai/leaderboard)
- [AI Model & API Providers Analysis | Artificial Analysis](https://artificialanalysis.ai/)
- [LiveCodeBench Leaderboard](https://livecodebench.github.io/leaderboard.html)
- [OpenCompass司南 - 评测榜单](https://rank.opencompass.org.cn/home)
- [AGI-Eval评测社区](https://agi-eval.cn/mvp/topRanking)
- [open-compass/opencompass: OpenCompass is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.](https://github.com/open-compass/opencompass)
- [SuperCLUE](https://cluebenchmarks.com/superclue_rag.html)
- [LLM Leaderboard 2024](https://www.vellum.ai/llm-leaderboard)
- [MMBench](https://mmbench.opencompass.org.cn/leaderboard)

**[⬆ 返回README目录](../README.md#目录)**
**[⬆ Back to Contents](../README-EN.md#contents)**