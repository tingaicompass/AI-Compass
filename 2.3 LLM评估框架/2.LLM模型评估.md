# 2.LLM模型评估+RAG评估框架

LLM评估框架模块建立了多维度、全覆盖的大模型评估生态系统，涵盖通用能力测评、RAG系统评估和隐私安全检测。**权威评测平台**：**CLiB中文大模型榜单**（128个模型全覆盖，包含ChatGPT、GPT-4o、Gemini、文心一言、通义千问等商用模型，以及Qwen2.5、Llama3.1、GLM4、InternLM2.5等开源模型）、**OpenCompass司南**（全方位能力评估）、**魔塔EvalScope**（流水线式评测框架）。**RAG专项评估**：**RAGas**（RAG Assessment专业框架）、**Arize Phoenix**（AI可观测性与评估）、**DeepEval**（LLM评估框架）、**ChainForge**（Prompt对战测试）等。**多模态评估**集成**谷歌LMEval**跨模型评估框架。**隐私安全**提供**微软Presidio**（PII敏感数据检测、编辑、掩码和匿名化），支持文本、图像和结构化数据的全方位隐私保护，确保模型应用的合规性和安全性。

1. 1.CLiB中文大模型能力评测榜单
2. 1.opencompass司南
3. 2.魔塔evalscope
4. 3.1.Arize Phoenix
5. 3.1.DeepEval
6. 3.1.RAGas（RAG Assessment）
7. 3.RAG评估框架
8. 4.多模态AI评估框架-谷歌
9. 5.PII隐私保护

# 1.CLiB中文大模型能力评测榜单

- [jeinlee1991/chinese-llm-benchmark: 中文大模型能力评测榜单：目前已囊括128个大模型，覆盖chatgpt、gpt-4o、谷歌gemini、百度文心一言、阿里通义千问、百川、讯飞星火、商汤senseChat、minimax等商用模型， 以及qwen2.5、llama3.1、glm4、书生internLM2.5、openbuddy、AquilaChat等开源大模型。不仅提供能力评分排行榜，也提供所有模型的原始输出结果！](https://github.com/jeinlee1991/chinese-llm-benchmark)

------------------------------------------------------------

# 1.opencompass司南

- [opencompass测评/README_zh-CN.md at main · open-compass/opencompass](https://github.com/open-compass/opencompass/blob/main/README_zh-CN.md)

------------------------------------------------------------

# 2.魔塔evalscope

- [modelscope/eval-scope: A streamlined and customizable framework for efficient large model evaluation and performance benchmarking](https://github.com/modelscope/eval-scope)
- [modelscope/evalscope: A streamlined and customizable framework for efficient large model evaluation and performance benchmarking](https://github.com/modelscope/evalscope)

------------------------------------------------------------

# 1.Arize Phoenix

- [Arize Phoenix | Phoenix](https://docs.arize.com/phoenix/zh)
- [Arize-ai/phoenix: AI Observability & Evaluation](https://github.com/Arize-ai/phoenix?tab=readme-ov-file)

------------------------------------------------------------

# 1.DeepEval

- [Quick Introduction | DeepEval - The Open-Source LLM Evaluation Framework](https://docs.confident-ai.com/docs/getting-started?utm_source=GitHub)
- [confident-ai/deepeval: The LLM Evaluation Framework](https://github.com/confident-ai/deepeval)
- [Arize-ai/phoenix: AI Observability & Evaluation](https://github.com/Arize-ai/phoenix)

------------------------------------------------------------

# 1.RAGas（RAG Assessment）

- [RAGas（RAG Assessment)](https://github.com/explodinggradients/ragas)
- [🚀 Get Started - Ragas](https://docs.ragas.io/en/stable/getstarted/)
- [🛠️ How-to Guides - Ragas](https://docs.ragas.io/en/stable/howtos/)
- [explodinggradients/ragas: Supercharge Your LLM Application Evaluations 🚀](https://github.com/explodinggradients/ragas)

------------------------------------------------------------

# 3.RAG评估框架

- [Arize-ai/phoenix: AI Observability & Evaluation](https://github.com/Arize-ai/phoenix)
- [ianarawjo/ChainForge: An open-source visual programming environment for battle-testing prompts to LLMs.](https://github.com/ianarawjo/ChainForge)

------------------------------------------------------------

# 4.多模态AI评估框架-谷歌

- [Announcing LMEval: An Open Source Framework for Cross-Model Evaluation | Google Open Source Blog](https://opensource.googleblog.com/2025/05/announcing-lmeval-an-open-ource-framework-cross-model-evaluation.html)
- [google/lmeval](https://github.com/google/lmeval)

------------------------------------------------------------

# 5.PII隐私保护

- [microsoft/presidio: An open-source framework for detecting, redacting, masking, and anonymizing sensitive data (PII) across text, images, and structured data. Supports NLP, pattern matching, and customizable pipelines.](https://github.com/microsoft/presidio?tab=readme-ov-file)
- [Presidio Demo - a Hugging Face Space by presidio](https://huggingface.co/spaces/presidio/presidio_demo)
- [microsoft.github.io](https://microsoft.github.io/presidio/)

------------------------------------------------------------

**[⬆ 返回README目录](../README.md#目录)**
**[⬆ Back to Contents](../README-EN.md#contents)**