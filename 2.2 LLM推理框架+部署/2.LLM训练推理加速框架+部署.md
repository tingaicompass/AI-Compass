# 2.LLMè®­ç»ƒæ¨ç†åŠ é€Ÿæ¡†æ¶+éƒ¨ç½²

LLMæ¨ç†æ¡†æ¶+éƒ¨ç½²æ¨¡å—æ‰“é€ äº†å…¨æ–¹ä½çš„å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿä¸éƒ¨ç½²ç”Ÿæ€ï¼Œæ•´åˆ21+é«˜æ€§èƒ½æ¨ç†å¼•æ“å’Œéƒ¨ç½²å¹³å°ã€‚**é¡¶çº§åŠ é€Ÿæ¡†æ¶**ï¼š**vLLMä¼¯å…‹åˆ©**ï¼ˆä¸šç•Œæ ‡æ†ï¼‰ã€**SGLang**ï¼ˆè¶…è¶ŠTensorRT-LLMæ€§èƒ½ï¼‰ã€**LMDeployä¹¦ç”Ÿ**ï¼ˆå·¥ä¸šçº§éƒ¨ç½²ï¼‰ã€**DeepSpeed-MII**ï¼ˆå¾®è½¯æ¨ç†ä¼˜åŒ–ï¼‰ç­‰ã€‚**ä¾¿æ·éƒ¨ç½²å·¥å…·**ï¼š**Ollama**ï¼ˆæœ¬åœ°æ¨¡å‹ç®¡ç†ï¼‰ã€**LM Studio**ï¼ˆå›¾å½¢åŒ–ç•Œé¢ï¼‰ã€**FastChat+vLLM**ï¼ˆåˆ†å¸ƒå¼æœåŠ¡ï¼‰ã€**Xinference**ï¼ˆå¤šæ¨¡å‹ç»Ÿä¸€æ¥å£ï¼‰ã€**OpenLLM**ï¼ˆäº‘ç«¯éƒ¨ç½²ï¼‰ç­‰ã€‚**APIç½‘å…³æœåŠ¡**ï¼š**LiteLLM**ï¼ˆ100+ LLM APIsç»Ÿä¸€æ ¼å¼ï¼‰ã€**One-API**ï¼ˆæ¥å£ç®¡ç†åˆ†å‘ï¼‰ã€**Xi-API**ç­‰ã€‚**æ‰˜ç®¡å¹³å°**åŒ…æ‹¬**Together AI**ã€**Replicate**ã€**SiliconFlowç¡…åŸºæµåŠ¨**ç­‰ã€‚é…å¥—**Huggingface Accelerate**ã€**llama-cpp-python**ç­‰åº•å±‚åŠ é€Ÿåº“ï¼Œä»¥åŠ**Jan.ai**ã€**LocalAI**ã€**text-generation-webui**ç­‰ç”¨æˆ·å‹å¥½ç•Œé¢ï¼Œå®ç°ä»æœ¬åœ°éƒ¨ç½²åˆ°äº‘ç«¯æœåŠ¡çš„å…¨åœºæ™¯è¦†ç›–ã€‚

- [self-llm/datawhaleå¤§æ¨¡å‹ä½¿ç”¨æŒ‡å—(å«è®­ç»ƒ)](https://github.com/datawhalechina/self-llm/blob/master/models/Qwen2.5/05-Qwen2.5-7B-Instruct%20Lora%20.ipynb)
- [Together AI â€“ The AI Acceleration Cloud - Fast Inference, Fine-Tuning & Training](https://www.together.ai/)

------------------------------------------------------------

1. 2.LLMè®­ç»ƒæ¨ç†åŠ é€Ÿæ¡†æ¶+éƒ¨ç½².md
2. 0.FastChat-åˆ†å¸ƒå¼éƒ¨ç½²ä¸åŠ é€Ÿï¼Œéœ€è¦é…åˆvllm
3. 0.LM Studio
4. 0.OpenLLM
5. 0.Xorbits Inferenceï¼šæ¨¡å‹æ¨ç†
6. 0.Xorbits Inferenceï¼šæ¨¡å‹æ¨ç†/issueè§£å†³
7. 0.litellm
8. 0.ollama
9. 0.one-api|Xi-api
10. 1.Jan.ai
11. 1.LocalAI
12. 1.Replicateå¤§æ¨¡å‹æ‰˜ç®¡å¹³å°
13. 1.SiliconFlow (åŒ—äº¬ç¡…åŸºæµåŠ¨)
14. 1.text-generation-webui
15. 2.DeepSpeed-MII
16. 2.SGLang
17. 2.fluxgym
18. 2.huggingface-accelerate
19. 2.llama-cpp-python
20. 2.lmdeploy-ä¹¦ç”Ÿæµ¦æº
21. 2.vLLM-ä¼¯å…‹åˆ©åŠ é€Ÿåº“



# 0.FastChat-åˆ†å¸ƒå¼éƒ¨ç½²ä¸åŠ é€Ÿï¼Œéœ€è¦é…åˆvllm

- [lm-sys/FastChat: An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.](https://github.com/lm-sys/FastChat/tree/main)
- [æœ¬åœ°åŒ–éƒ¨ç½²å¤§æ¨¡å‹æ–¹æ¡ˆäºŒï¼šfastchat+llm(vllm)-CSDNåšå®¢](https://blog.csdn.net/huiguo_/article/details/135766850)

------------------------------------------------------------

# 0.LM Studio

- [LM Studio - Discover, download, and run local LLMs](https://lmstudio.ai/)
- [LM Studio](https://github.com/lmstudio-ai)
- [LM Studio æ‰‹å†Œ](https://lmstudio.ai/blog/lmstudio-v0.3.0)

------------------------------------------------------------

# 0.OpenLLM

- [bentoml/OpenLLM: Run any open-source LLMs, such as Llama 3.1, Gemma, as OpenAI compatible API endpoint in the cloud.](https://github.com/bentoml/OpenLLM)
- [bentoml/openllm-models](https://github.com/bentoml/openllm-models)

------------------------------------------------------------

# 0.Xorbits Inferenceï¼šæ¨¡å‹æ¨ç†

- [xorbitsai/inference: Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.](https://github.com/xorbitsai/inference)
- [inference/README_zh_CN.md at main Â· xorbitsai/inference](https://github.com/xorbitsai/inference/blob/main/README_zh_CN.md#%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2)
- [å†…ç½®æ¨¡å‹ â€” Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/index.html)
- [åµŒå…¥æ¨¡å‹ â€” Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/embedding/index.html)

------------------------------------------------------------

# issueè§£å†³

- ['bge-reranker-v2-minicpm-layerwise æ— æ³•æŠ¥é”™ä½¿ç”¨ï¼Œå…¶ä»–rerankeræ¨¡å‹æ²¡æœ‰ç›¸å…³é—®é¢˜ Â· Issue #2217 Â· xorbitsai/inference](https://github.com/xorbitsai/inference/issues/2217)
- [å¯åŠ¨qwen2.5 14B vllm int8 ç‰ˆæœ¬ValueError: [address=0.0.0.0:37961, pid=352437] Marlin does not support weight_bits = uint8b128. Only types = [] are supported (for group_size = 128, min_capability = 70, zp = False) Â· Issue #2350 Â· xorbitsai/inference](https://github.com/xorbitsai/inference/issues/2350)
- [SGlangéƒ¨ç½²qwen2.5å¤±è´¥Exception: Capture cuda graph failed: BatchPrefillWithPagedKVCache failed with error code no kernel image is available for execution on the device Â· Issue #2351 Â· xorbitsai/inference](https://github.com/xorbitsai/inference/issues/2351)

------------------------------------------------------------

# 0.litellm

- [BerriAI/litellm: Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]](https://github.com/BerriAI/litellm)
- [LiteLLM - Getting Started | liteLLM](https://docs.litellm.ai/docs/)

------------------------------------------------------------

# 0.ollama

- [ollamaå®˜ç½‘-æ¨¡å‹ä¸‹è½½](https://ollama.com/library)
- [ollama/ollama: Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.](https://github.com/ollama/ollama)

------------------------------------------------------------

# 0.one-api|Xi-api

- [One APIæ¼”ç¤º](https://openai.justsong.cn/)
- [one-api: OpenAI æ¥å£ç®¡ç† & åˆ†å‘ç³»ç»Ÿï¼Œæ”¯æŒ Azureã€Anthropic Claudeã€Google PaLM 2 & Geminiã€æ™ºè°± ChatGLMã€ç™¾åº¦æ–‡å¿ƒä¸€è¨€ã€è®¯é£æ˜Ÿç«è®¤çŸ¥ã€é˜¿é‡Œé€šä¹‰åƒé—®ã€360 æ™ºè„‘ä»¥åŠè…¾è®¯æ··å…ƒï¼Œå¯ç”¨äºäºŒæ¬¡åˆ†å‘ç®¡ç† keyï¼Œä»…å•å¯æ‰§è¡Œæ–‡ä»¶ï¼Œå·²æ‰“åŒ…å¥½ Docker é•œåƒï¼Œä¸€é”®éƒ¨ç½²ï¼Œå¼€ç®±å³ç”¨. OpenAI key management & redistribution system, using a single API for all LLMs, and features an English UI.](https://github.com/songquanpeng/one-api)
- [Xi-Api](https://api.xi-ai.cn/)

------------------------------------------------------------

# 1.Jan.ai

- [janhq/jan: Jan is an open source alternative to ChatGPT that runs 100% offline on your computer. Multiple engine support (llama.cpp, TensorRT-LLM)](https://github.com/janhq/jan)

------------------------------------------------------------

# 1.LocalAI

- [Quickstart | LocalAI documentation](https://localai.io/basics/getting_started/)
- [mudler/LocalAI: :robot: The free, Open Source OpenAI alternative. Self-hosted, community-driven and local-first. Drop-in replacement for OpenAI running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. It allows to generate Text, Audio, Video, Images. Also with voice cloning capabilities.](https://github.com/mudler/LocalAI)

------------------------------------------------------------

# 1.Replicateå¤§æ¨¡å‹æ‰˜ç®¡å¹³å°

- [Replicate â€” Run AI with an API](https://replicate.com/)
- [Get embeddings â€“Â Replicate](https://replicate.com/collections/embedding-models)
- [Use a language model â€“Â Replicate](https://replicate.com/collections/language-models)

------------------------------------------------------------

# 1.SiliconFlow (åŒ—äº¬ç¡…åŸºæµåŠ¨)

- [SiliconFlow, Accelerate AGI to Benefit Humanity](https://siliconflow.cn/zh-cn/)

------------------------------------------------------------

# 1.text-generation-webui

- [oobabooga/text-generation-webui: A Gradio web UI for Large Language Models.](https://github.com/oobabooga/text-generation-webui)

------------------------------------------------------------

# 2.DeepSpeed-MII

- [DeepSpeed-MIIæ¨ç†åŠ é€Ÿ](https://github.com/microsoft/DeepSpeed-MII)
- [deepspeedai/DeepSpeed: DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.](https://github.com/deepspeedai/DeepSpeed)

------------------------------------------------------------

# 2.SGLang

- [sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models.](https://github.com/sgl-project/sglang)
- [Achieving Faster Open-Source Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) | LMSYS Org](https://lmsys.org/blog/2024-07-25-sglang-llama3/)

------------------------------------------------------------

# 2.fluxgym

- [cocktailpeanut/fluxgym: Dead simple FLUX LoRA training UI with LOW VRAM support](https://github.com/cocktailpeanut/fluxgym)

------------------------------------------------------------

# 2.huggingface-accelerate

- [huggingface/accelerate: ğŸš€ A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support](https://github.com/huggingface/accelerate)

------------------------------------------------------------

# 2.llama-cpp-python

- [abetlen/llama-cpp-python: Python bindings for llama.cpp](https://github.com/abetlen/llama-cpp-python)

------------------------------------------------------------

# 2.lmdeploy-ä¹¦ç”Ÿæµ¦æº

- [InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.](https://github.com/InternLM/lmdeploy)

------------------------------------------------------------

# 2.vLLM-ä¼¯å…‹åˆ©åŠ é€Ÿåº“

- [1.ä¼¯å…‹åˆ©å¤§å­¦vLLM](https://github.com/vllm-project/vllm)

------------------------------------------------------------

**[â¬† è¿”å›READMEç›®å½•](../README.md#ç›®å½•)**
**[â¬† Back to Contents](../README-EN.md#contents)**