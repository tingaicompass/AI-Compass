# 2.LLM训练推理加速框架+部署

LLM推理框架+部署模块打造了全方位的大模型推理加速与部署生态，整合21+高性能推理引擎和部署平台。**顶级加速框架**：**vLLM伯克利**（业界标杆）、**SGLang**（超越TensorRT-LLM性能）、**LMDeploy书生**（工业级部署）、**DeepSpeed-MII**（微软推理优化）等。**便捷部署工具**：**Ollama**（本地模型管理）、**LM Studio**（图形化界面）、**FastChat+vLLM**（分布式服务）、**Xinference**（多模型统一接口）、**OpenLLM**（云端部署）等。**API网关服务**：**LiteLLM**（100+ LLM APIs统一格式）、**One-API**（接口管理分发）、**Xi-API**等。**托管平台**包括**Together AI**、**Replicate**、**SiliconFlow硅基流动**等。配套**Huggingface Accelerate**、**llama-cpp-python**等底层加速库，以及**Jan.ai**、**LocalAI**、**text-generation-webui**等用户友好界面，实现从本地部署到云端服务的全场景覆盖。

- [self-llm/datawhale大模型使用指南(含训练)](https://github.com/datawhalechina/self-llm/blob/master/models/Qwen2.5/05-Qwen2.5-7B-Instruct%20Lora%20.ipynb)
- [Together AI – The AI Acceleration Cloud - Fast Inference, Fine-Tuning & Training](https://www.together.ai/)

------------------------------------------------------------

1. 2.LLM训练推理加速框架+部署.md
2. 0.FastChat-分布式部署不加速，需要配合vllm
3. 0.LM Studio
4. 0.OpenLLM
5. 0.Xorbits Inference：模型推理
6. 0.Xorbits Inference：模型推理/issue解决
7. 0.litellm
8. 0.ollama
9. 0.one-api|Xi-api
10. 1.Jan.ai
11. 1.LocalAI
12. 1.Replicate大模型托管平台
13. 1.SiliconFlow (北京硅基流动)
14. 1.text-generation-webui
15. 2.DeepSpeed-MII
16. 2.SGLang
17. 2.fluxgym
18. 2.huggingface-accelerate
19. 2.llama-cpp-python
20. 2.lmdeploy-书生浦源
21. 2.vLLM-伯克利加速库



# 0.FastChat-分布式部署不加速，需要配合vllm

- [lm-sys/FastChat: An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.](https://github.com/lm-sys/FastChat/tree/main)
- [本地化部署大模型方案二：fastchat+llm(vllm)-CSDN博客](https://blog.csdn.net/huiguo_/article/details/135766850)

------------------------------------------------------------

# 0.LM Studio

- [LM Studio - Discover, download, and run local LLMs](https://lmstudio.ai/)
- [LM Studio](https://github.com/lmstudio-ai)
- [LM Studio 手册](https://lmstudio.ai/blog/lmstudio-v0.3.0)

------------------------------------------------------------

# 0.OpenLLM

- [bentoml/OpenLLM: Run any open-source LLMs, such as Llama 3.1, Gemma, as OpenAI compatible API endpoint in the cloud.](https://github.com/bentoml/OpenLLM)
- [bentoml/openllm-models](https://github.com/bentoml/openllm-models)

------------------------------------------------------------

# 0.Xorbits Inference：模型推理

- [xorbitsai/inference: Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.](https://github.com/xorbitsai/inference)
- [inference/README_zh_CN.md at main · xorbitsai/inference](https://github.com/xorbitsai/inference/blob/main/README_zh_CN.md#%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2)
- [内置模型 — Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/index.html)
- [嵌入模型 — Xinference](https://inference.readthedocs.io/zh-cn/latest/models/builtin/embedding/index.html)

------------------------------------------------------------

# issue解决

- ['bge-reranker-v2-minicpm-layerwise 无法报错使用，其他reranker模型没有相关问题 · Issue #2217 · xorbitsai/inference](https://github.com/xorbitsai/inference/issues/2217)
- [启动qwen2.5 14B vllm int8 版本ValueError: [address=0.0.0.0:37961, pid=352437] Marlin does not support weight_bits = uint8b128. Only types = [] are supported (for group_size = 128, min_capability = 70, zp = False) · Issue #2350 · xorbitsai/inference](https://github.com/xorbitsai/inference/issues/2350)
- [SGlang部署qwen2.5失败Exception: Capture cuda graph failed: BatchPrefillWithPagedKVCache failed with error code no kernel image is available for execution on the device · Issue #2351 · xorbitsai/inference](https://github.com/xorbitsai/inference/issues/2351)

------------------------------------------------------------

# 0.litellm

- [BerriAI/litellm: Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]](https://github.com/BerriAI/litellm)
- [LiteLLM - Getting Started | liteLLM](https://docs.litellm.ai/docs/)

------------------------------------------------------------

# 0.ollama

- [ollama官网-模型下载](https://ollama.com/library)
- [ollama/ollama: Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.](https://github.com/ollama/ollama)

------------------------------------------------------------

# 0.one-api|Xi-api

- [One API演示](https://openai.justsong.cn/)
- [one-api: OpenAI 接口管理 & 分发系统，支持 Azure、Anthropic Claude、Google PaLM 2 & Gemini、智谱 ChatGLM、百度文心一言、讯飞星火认知、阿里通义千问、360 智脑以及腾讯混元，可用于二次分发管理 key，仅单可执行文件，已打包好 Docker 镜像，一键部署，开箱即用. OpenAI key management & redistribution system, using a single API for all LLMs, and features an English UI.](https://github.com/songquanpeng/one-api)
- [Xi-Api](https://api.xi-ai.cn/)

------------------------------------------------------------

# 1.Jan.ai

- [janhq/jan: Jan is an open source alternative to ChatGPT that runs 100% offline on your computer. Multiple engine support (llama.cpp, TensorRT-LLM)](https://github.com/janhq/jan)

------------------------------------------------------------

# 1.LocalAI

- [Quickstart | LocalAI documentation](https://localai.io/basics/getting_started/)
- [mudler/LocalAI: :robot: The free, Open Source OpenAI alternative. Self-hosted, community-driven and local-first. Drop-in replacement for OpenAI running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. It allows to generate Text, Audio, Video, Images. Also with voice cloning capabilities.](https://github.com/mudler/LocalAI)

------------------------------------------------------------

# 1.Replicate大模型托管平台

- [Replicate — Run AI with an API](https://replicate.com/)
- [Get embeddings – Replicate](https://replicate.com/collections/embedding-models)
- [Use a language model – Replicate](https://replicate.com/collections/language-models)

------------------------------------------------------------

# 1.SiliconFlow (北京硅基流动)

- [SiliconFlow, Accelerate AGI to Benefit Humanity](https://siliconflow.cn/zh-cn/)

------------------------------------------------------------

# 1.text-generation-webui

- [oobabooga/text-generation-webui: A Gradio web UI for Large Language Models.](https://github.com/oobabooga/text-generation-webui)

------------------------------------------------------------

# 2.DeepSpeed-MII

- [DeepSpeed-MII推理加速](https://github.com/microsoft/DeepSpeed-MII)
- [deepspeedai/DeepSpeed: DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.](https://github.com/deepspeedai/DeepSpeed)

------------------------------------------------------------

# 2.SGLang

- [sgl-project/sglang: SGLang is a fast serving framework for large language models and vision language models.](https://github.com/sgl-project/sglang)
- [Achieving Faster Open-Source Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) | LMSYS Org](https://lmsys.org/blog/2024-07-25-sglang-llama3/)

------------------------------------------------------------

# 2.fluxgym

- [cocktailpeanut/fluxgym: Dead simple FLUX LoRA training UI with LOW VRAM support](https://github.com/cocktailpeanut/fluxgym)

------------------------------------------------------------

# 2.huggingface-accelerate

- [huggingface/accelerate: 🚀 A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support](https://github.com/huggingface/accelerate)

------------------------------------------------------------

# 2.llama-cpp-python

- [abetlen/llama-cpp-python: Python bindings for llama.cpp](https://github.com/abetlen/llama-cpp-python)

------------------------------------------------------------

# 2.lmdeploy-书生浦源

- [InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.](https://github.com/InternLM/lmdeploy)

------------------------------------------------------------

# 2.vLLM-伯克利加速库

- [1.伯克利大学vLLM](https://github.com/vllm-project/vllm)

------------------------------------------------------------

**[⬆ 返回README目录](../README.md#目录)**
**[⬆ Back to Contents](../README-EN.md#contents)**