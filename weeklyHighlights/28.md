# AI Compass前沿速览：Open-AutoGLM智能体框架、Z-Image图像生成、GLM-4.6V多模态理解与可灵2.6音画同步技术


**AI-Compass** 致力于构建最全面、最实用、最前沿的AI技术学习和实践生态，通过六大核心模块的系统化组织，为不同层次的学习者和开发者提供从完整学习路径。

* github地址：[AI-Compass👈：https://github.com/tingaicompass/AI-Compass](https://github.com/tingaicompass/AI-Compass)
* gitee地址：[AI-Compass👈：https://gitee.com/tingaicompass/ai-compass](https://gitee.com/tingaicompass/ai-compass)


🌟 如果本项目对您有所帮助，请为我们点亮一颗星！🌟

# 1.每周项目推荐
## Open-AutoGLM：智谱AI开源手机端智能体框架

Open-AutoGLM是智谱AI开源的手机端智能助理框架，基于AutoGLM大模型构建。它旨在通过自然语言指令实现手机操作的自动化，将用户的口头或文本指令转化为实际的手机交互行为，如点击、滑动和输入。该框架通过其Phone Use能力保障隐私安全，并支持广泛的中文主流应用。

#### 核心功能
*   **自然语言理解与任务执行：** 能够解析用户自然语言指令，并将其转化为手机上的具体操作以完成任务。
*   **自动化操作模拟：** 支持模拟真实用户在手机上的多样化操作，包括点击、滑动、文本输入、长按和双击等。
*   **隐私与安全保障：** 在执行敏感操作时，提供人工确认或接管机制，同时借助云手机技术确保用户隐私安全。
*   **远程调试与控制：** 支持通过WiFi或网络进行远程ADB（Android Debug Bridge）调试，无需物理连接即可控制设备。
*   **广泛应用支持：** 兼容50多款主流中文手机应用，涵盖社交、电商、外卖、娱乐等多个领域。

#### 技术原理
Open-AutoGLM的核心技术原理是构建在**AutoGLM大模型**之上，结合了**多模态感知能力**和**智能规划机制**。它利用**Phone Use能力框架**，将高层级的自然语言指令（例如“帮我订外卖”）拆解为一系列低层级的原子操作。具体实现包括：
1.  **视觉语言模型（Vision-Language Model, VLM）：** 用于理解手机屏幕的当前UI状态和内容，从而实现对界面的感知。
2.  **智能规划（Intelligent Planning）：** 根据用户意图和当前屏幕状态，生成并优化操作序列以达成目标。
3.  **ADB (Android Debug Bridge) 控制：** 通过ADB协议与手机设备进行通信，执行屏幕点击、滑动、文本输入等底层操作，模拟用户行为。
4.  **模型客户端：** 采用与OpenAI兼容的客户端，便于接入和调用AI模型。

#### 应用场景
*   **外卖点餐：** 用户通过自然语言指令，实现自动打开外卖应用、搜索特定商家、选择商品并完成下单。
*   **社交媒体互动：** 自动化执行点赞、评论、分享等社交应用内的操作，如在微信、微博或抖音上与内容互动。
*   **办公自动化：** 在WPS、Microsoft Office等办公应用中，根据指令创建文档、编辑内容或处理其他办公任务。
*   **智能家居控制：** 通过智能家居应用，AI能够精准识别并控制相应的智能设备，实现场景切换或设备操作。
*   **交通出行：** 在地图或打车应用中，自动规划路线、叫车或执行其他出行相关操作。


* GitHub仓库：https://github.com/zai-org/Open-AutoGLM


## LongCat-Image：美团开源6B参数文生图与图像编辑模型

LongCat-Image是美团开源的高性能图像生成模型，以仅6B的参数规模在文生图和图像编辑方面达到开源顶尖水平。该模型采用创新架构和训练策略，尤其在高质量中文文字渲染方面表现出色，覆盖8105个常用汉字，旨在为创意设计、广告等领域提供强大的视觉生成能力。

![](https://i.postimg.cc/cHBcykv1/Long-Cat-Image.jpg)

![](https://i.postimg.cc/hGxrMtgw/Long-Cat-Image-edit-gallery.jpg)

![](https://i.postimg.cc/W3v7ycbY/Long-Cat-Image-gallery.jpg)

#### 核心功能
*   **文生图 (Text-to-Image)**：根据文本描述生成高质量图像，支持多种风格和场景。
*   **图像编辑 (Image Editing)**：提供强大的图像编辑能力，实现风格迁移、属性编辑和构图调整。
*   **中文文字渲染**：优化中文文本生成，支持复杂笔画和生僻字，确保文本准确性和背景融合自然度。
*   **真实感与纹理细节提升**：通过系统性数据筛选和对抗训练，生成图像具有更高真实感，避免“塑料感”纹理。
*   **低门槛开发与应用**：提供从预训练模型到微调代码的完整工具链，支持SFT、LoRA等功能，便于二次开发和定制。

#### 技术原理
LongCat-Image的核心扩散架构采用混合MM-DiT和Single-DiT结构，并利用Qwen2.5VL-7B作为其文本编码器，为生成和编辑任务提供统一且强大的条件空间。模型训练采用渐进式学习策略，包括：
1.  **预训练阶段**：使用多源数据和指令改写策略，提升模型对多样化指令的理解。
2.  **SFT阶段 (Supervised Fine-Tuning)**：引入人工精标数据和真实世界文本图像数据，提高指令遵循精准度、泛化能力及对齐大众审美。
3.  **RL阶段 (Reinforcement Learning)**：融入OCR（光学字符识别）与美学双奖励模型，并创新性引入AIGC内容检测器作为奖励模型，通过对抗信号引导模型学习物理纹理和光影效果，进一步优化文本准确性和背景融合自然度。

#### 应用场景
*   **海报设计与广告创作**：根据文案快速生成高质量海报和广告图，支持中文文字渲染和风格定制。
*   **教学辅助**：生成与教学内容相关的图像，如历史场景、科学实验图示等，辅助学生理解知识。
*   **艺术创作与设计**：为艺术家和设计师提供创意生成和图像编辑工具。
*   **社交媒体与营销**：快速生成社交媒体内容和营销素材。
*   **个性化图像处理**：对照片进行风格转换、背景替换、人物美化等。



* GitHub仓库：https://github.com/meituan-longcat/LongCat-Image


## GLM-4.6V：智谱AI开源128K长上下文多模态视觉理解模型


GLM-4.6V是智谱AI与清华大学联合推出的多模态大模型系列，旨在实现高保真视觉理解和长上下文推理。该系列包含基础版GLM-4.6V（106B）和轻量版GLM-4.6V-Flash（9B），支持长达128K tokens的上下文，并首次将原生多模态函数调用能力融入视觉模型，实现了从视觉感知到可执行行动的闭环。

![](https://i.postimg.cc/2SgG4ZpZ/GLM-4-6V-9b.jpg)

![](https://i.postimg.cc/pTHBSMLH/GLM-4-6V-bench-46v.jpg)

#### 核心功能
*   **高保真视觉理解与长上下文推理：** 能够处理图像、文档和混合媒体，进行精确的视觉分析和跨多页的复杂推理。
*   **原生多模态函数调用：** 允许将图像、截图、文档页面等视觉资产直接作为参数传递给外部工具，实现视觉感知与工具执行的无缝连接。
*   **图文交错内容生成：** 从多模态输入（如混合文本/图片论文、报告、幻灯片）自动生成高质量、结构化的图文交错内容。
*   **UI重建与视觉编辑：** 能从UI截图像素级重建HTML/CSS代码，并支持自然语言驱动的迭代视觉编辑和代码生成。
*   **多版本部署支持：** 提供面向云端高性能场景的基础版和面向本地部署、低延迟应用的轻量版。

#### 技术原理
GLM-4.6V系列模型基于大规模多模态Transformer架构，其技术亮点包括：
*   **长上下文窗口：** 在训练中将上下文窗口扩展至128K tokens，大幅提升模型处理长文档、多页报告和长时间视频的能力。
*   **原生函数调用集成：** 首次将函数调用能力设计为模型的核心组成部分，允许模型直接将视觉输入（如图像、屏幕截图）作为工具调用的参数，避免了信息损失。
*   **视觉编程接口：** 模型能够通过对屏幕截图的原生理解，在布局、设计意图和输出代码之间进行迭代，实现端到端的视觉编程。
*   **模型规模与效率：** 拥有106B参数的基础版（可能采用MoE架构以优化效率），以及9B参数的Flash版本，在同等参数规模下达到领先的视觉理解性能，并实现成本优化。

#### 应用场景
*   **智能图文创作：** 自动生成高质量的图文混排内容，如新闻稿、报告和演示文稿。
*   **识图购物与导购：** 通过图片搜索同款商品，进行比价，并生成导购清单。
*   **前端复刻与开发：** 根据UI截图生成像素级准确的HTML/CSS代码，并支持通过自然语言进行修改和迭代。
*   **长文档与视频理解：** 能够处理多达150页的文本、200张幻灯片或1小时的视频，进行内容摘要、信息抽取和复杂问答。
*   **多模态代理：** 作为多模态智能体的核心，连接视觉感知与外部工具执行，赋能更智能的自动化工作流。

* GitHub仓库：https://github.com/zai-org/GLM-V



## MemMachine：开源跨模型AI持久化记忆系统

MemMachine 是一个开源的、跨模型的人工智能记忆层，专为高级AI智能体设计，特别是针对大型语言模型（LLM）和代理式AI应用。它使AI应用能够学习、存储并召回跨会话、跨智能体和跨LLM的数据及偏好，从而构建复杂、不断演进的用户画像，将传统AI聊天机器人转变为个性化、上下文感知的AI助手，以提供更精准和深入的响应。

![](https://i.postimg.cc/pTHBSMLp/memmachine.png)

#### 核心功能
*   **持久化记忆：** 实现AI代理在多个会话和不同代理间的数据、偏好及用户配置的长期存储与快速召回。
*   **跨模型兼容：** 支持与各种AI代理和大型语言模型的无缝集成与协作。
*   **智能体状态管理：** 优化AI智能体状态的存储和检索，提升自主系统的运行效率。
*   **个性化交互：** 赋能AI系统提供基于历史互动和用户特征的定制化、情境感知型体验。
*   **开源生态系统：** 提供开源项目，并伴随企业级解决方案，促进社区协作和创新。

#### 技术原理
*   **分层记忆架构：** 作为AI智能体的通用记忆层，提供可扩展、可扩展且可互操作的记忆存储与检索机制。
*   **知识图谱构建：** 通过持续学习和关联数据，隐式或显式地构建和维护复杂的用户画像及知识结构。
*   **持久化数据存储：** 利用后端数据库（如文档中提及的Databases）确保记忆内容的跨会话持久性。
*   **代理式记忆支持：** 专注于代理工作流，使AI智能体能够基于过往经验进行记忆和决策。
*   **长短期记忆管理：** 具备管理和利用LLM上下文信息的能力，支持在长时间交互中保持连贯性和相关性。
*   **API与SDK接口：** 提供便捷的API和SDK，方便开发者集成和构建基于MemMachine的AI应用。

#### 应用场景
*   **个性化AI助手：** 用于开发能够记住用户偏好、历史对话和特定需求的智能客服或个人助理。
*   **金融服务：** AI代理可记住用户的投资组合、风险偏好，提供个性化的金融咨询和市场洞察。
*   **内容创作与编辑：** 辅助内容创作者，记忆专属风格指南、术语和历史文档，确保内容一致性。
*   **自动化与自主系统：** 在需要跨时间或跨任务保持状态和决策连续性的自动驾驶、机器人等领域。
*   **教育与培训：** 构建能够跟踪学生学习进度和偏好的个性化辅导系统。


* 项目官网：https://memmachine.ai/
* GitHub仓库：https://github.com/MemMachine/


## Gen-4.5：Runway电影级视频生成与多模态世界模型

当前AI领域涌现出一批代表新一代技术水平的“4.5”系列模型，它们在多模态理解与生成方面取得显著进展。这些模型包括Runway的Gen-4.5视频生成模型、百度的文心大模型4.5（Ernie 4.5）以及Anthropic的Claude Haiku 4.5等。它们共同特点是致力于提升AI的运动质量、视觉逼真度、多模态处理能力以及对话的连贯性与深度理解，旨在为用户提供更智能、更高效、更具表现力的AI体验。

![](https://i.postimg.cc/FR4x6XHY/runway.png)

#### 核心功能
1.  **高质量视频生成与编辑**：能够生成高运动质量、物理模拟精确、视觉逼真且具有电影级质感的视频内容，支持通过自然语言指令进行视频增删、风格重绘和镜头延展等操作。
2.  **统一多模态理解与生成**：具备集成处理文本、图像、音频和视频信息的能力，实现跨模态内容的深度理解、关联和生成，例如文档解析和对互联网模因的理解。
3.  **高级语言与推理能力**：显著提升语言理解、生成、逻辑推理和记忆能力，能够更好地理解上下文，维持长时间对话的连贯性，并提供个性化服务。
4.  **实时生成与3D一致性**：支持实时生成新的2D图像，并能在不显式构建3D表示的情况下模拟3D几何和反射，实现3D一致性。
5.  **模型性能与效率优化**：通过架构优化和参数精简，提高推理速度，降低运行成本，同时支持多种控制模式和思考长度调节，以平衡效果与效率。

#### 技术原理
1.  **大一统多模态架构 (Unified Multimodal Architecture)**：采用整合不同模态数据处理模块的统一框架，如Transformer或更先进的混合专家模型（MoE），实现文本、图像、音频、视频数据的深层融合与协同理解生成。
2.  **生成对抗网络 (GANs) 与扩散模型 (Diffusion Models)**：作为核心生成技术，驱动视频和图像内容的高保真度合成，并通过先进的采样与优化技术提升生成内容的视觉质量和动态连贯性。
3.  **时空注意力机制 (Spatio-Temporal Attention Mechanisms)**：在视频生成中，引入复杂机制以捕捉时间维度上的连续性和空间维度上的细节，确保运动流畅性和场景构建的复杂性。
4.  **因果语言模型与长上下文窗口 (Causal Language Models & Long Context Windows)**：通过优化Attention机制和位置编码，扩展模型对历史对话信息的记忆和理解能力，从而实现“长记忆”和更具情境感的交互。
5.  **参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT) 与模型蒸馏 (Model Distillation)**：应用于优化模型结构和规模，实现“lite”版本模型的轻量化，在保持性能的同时降低计算资源消耗，提升部署效率。
6.  **端到端学习 (End-to-End Learning) 与隐式3D表示 (Implicit 3D Representation)**：对于世界模型，通过大规模视频数据训练，模型能够直接从2D输入学习并模拟3D几何及物理特性，而无需显式中间表示。

#### 应用场景
1.  **数字内容创作**：艺术家、设计师和内容创作者可利用其生成高质量视频、图像和动画，加速影视制作、广告创意及数字艺术品的创作流程。
2.  **智能助理与客户服务**：通过具备“长记忆”和多模态理解能力的对话系统，提供更人性化、个性化、高效的智能客服、教育辅导及个人助理服务。
3.  **跨媒体信息处理**：应用于智能办公、新闻媒体等领域，实现文档的智能识别、解析与摘要，以及跨图像、视频、文本内容的快速检索与分析。
4.  **虚拟现实与游戏开发**：构建实时、逼真的虚拟世界和游戏场景，生成动态环境和智能NPC行为，提升沉浸式体验。
5.  **AI模型开发与部署**：作为基础模型和开发平台，为开发者提供强大的多模态能力，加速各种AI应用的构建和迭代，如ChatHub这类集成多模型的应用。


* https://runwayml.com/research/introducing-runway-gen-4.5




## Vidi：字节跳动多模态视频理解与时空定位模型

Vidi是由字节跳动开发的一系列多模态大语言模型，专注于视频理解和创作。它旨在通过整合文本、音频和视觉信息，实现对视频内容的深度分析、编辑和生成，并在多个视频理解任务中达到行业领先水平。

![](https://i.postimg.cc/gJWK7b2r/vidi2.png)

![](https://i.postimg.cc/hj10WZXD/vidi21.png)



#### 核心功能
*   **多模态时间检索 (Multimodal Temporal Retrieval, TR)**：高效精准地从视频内容中检索特定时间段的信息，结合多种模态数据进行匹配。
*   **时空定位 (Spatio-Temporal Grounding, STG)**：准确识别并定位视频中特定对象或事件在时间和空间上的发生位置。
*   **视频问答 (Video Question Answering, Video QA)**：根据用户提出的问题，从视频内容中提取信息并给出准确答案。
*   **视频编辑 (Video Editing)**：支持对视频内容进行高级编辑操作，可能涉及内容生成、修改等。

#### 技术原理
Vidi模型基于大型多模态预训练模型架构，融合了Transformer等深度学习技术，能够处理和理解跨模态数据（如视频帧、音频波形和文本描述）。其核心技术在于构建一个统一的表示空间，将不同模态的信息映射到该空间中进行语义对齐和交互学习。通过自注意力机制和跨模态注意力机制，模型可以捕捉视频中复杂的时空依赖关系和语义信息，从而实现高级的视频理解和生成任务。

#### 应用场景
*   **智能视频内容管理与检索**：应用于媒体库、在线视频平台，实现高效的内容分类、搜索和推荐。
*   **视频创作与编辑工具**：为专业人士和普通用户提供智能化的视频剪辑、特效添加、内容生成等辅助功能。
*   **教育与培训**：通过对教学视频的深度理解，辅助学习者进行知识获取和问答。
*   **安防监控与事件检测**：自动识别视频中的异常行为或特定事件，提高监控效率和响应速度。
*   **机器人与自动化**：赋能机器人通过视觉和听觉理解环境，执行复杂任务。


* 项目官网：https://bytedance.github.io/vidi-website/
* Github仓库：https://github.com/bytedance/vidi


## Z-Image：阿里通义6B参数高效图像生成模型


Z-Image（造相）是阿里巴巴通义实验室推出的一款高效的图像生成模型。它包括一个参数量为6B的基础模型，以及一个从Z-Image蒸馏而来的极速版Z-Image-Turbo。Z-Image系列模型旨在提供高质量、逼真的图像生成能力，并以其高效率和快速生成速度为特点。


![](https://i.postimg.cc/ZnYxTPNP/z-image.png)

![](https://i.postimg.cc/cHBcykv8/Z-Image-arena.jpg)

#### 核心功能
*   **高效率图像生成：** 能够快速生成高质量图像，Z-Image-Turbo版本更是达到了亚秒级生成速度。
*   **逼真图像效果：** 生成的图像具有令人惊叹的真实感。
*   **参数规模适中：** 6B参数量使其在保持高性能的同时，兼顾了模型的轻量化与部署效率。

#### 技术原理
Z-Image模型基于新颖的架构设计，虽然具体细节需查阅相关技术报告（如Z_Image_Report.pdf和Decoupled_DMD.pdf），但已知其核心在于一个高效的6B参数图像生成模型。Z-Image-Turbo版本则通过模型蒸馏（Model Distillation）技术，从更大的Z-Image模型中提炼而来，旨在优化推理速度和效率，实现亚秒级的生成响应，同时保持视觉效果的高度逼真。这通常涉及到知识蒸馏、模型剪枝、量化等技术，以减小模型体积并提升运行效率。


![](https://i.postimg.cc/vBtvF249/Z-Image-architecture.webp)

#### 应用场景
*   **创意内容生成：** 艺术家、设计师、内容创作者可用于生成草图、概念图、营销素材等。
*   **虚拟现实/增强现实：** 快速生成高质量的虚拟场景和对象纹理。
*   **游戏开发：** 用于快速迭代游戏内的环境、角色、道具纹理等视觉资产。
*   **电子商务：** 生成商品展示图、广告图等，提高营销效率。
*   **多媒体编辑：** 作为图像处理和编辑工具的底层生成能力，辅助用户进行图像创作和修改。


* 项目官网：https://tongyi-mai.github.io/Z-Image-blog/
* GitHub仓库：https://github.com/Tongyi-MAI/Z-Image


## Depth Anything 3：字节跳动统一多视图深度估计与空间重建模型

Depth Anything 3 (DA3) 是字节跳动Seed团队推出的一款先进的视觉空间重建模型。它旨在从任意数量的视觉输入中预测出空间一致的几何结构，无论是否已知相机姿态。DA3简化了AI模型理解多图像空间几何的方式，并通过单一Transformer架构实现了这一目标。

![](https://i.postimg.cc/0yhnpwgG/depth-anything.png)
![](https://i.postimg.cc/N0T4bjYX/depth-anything3.png)

#### 核心功能
*   **空间几何重建：** 能够从任意视角输入恢复出精确的三维空间几何信息。
*   **多视图输入处理：** 支持处理任意数量的视觉输入，并能从中生成对齐的深度和光线预测。
*   **灵活的相机姿态支持：** 无论相机姿态已知或未知，模型均能有效工作。
*   **卓越的性能：** 在单目深度估计、多视图深度估计和姿态估计方面显著超越前代DA2及VGGT模型。
*   **多样化模型系列：** 提供DA3 Main Series（如Giant、Large、Base、Small）和DA3 Metric Series（如DA3Metric-Large），分别满足统一深度-光线表示和单目指标深度估计的需求。

#### 技术原理
DA3的核心技术基于**单一Transformer架构**，利用**输入自适应的跨视图自注意力机制（input-adaptive cross-view self-attention mechanism）**，实现了在所有图像之间动态共享信息。这使得模型能够为每个视图生成对齐的深度和光线预测。其训练采用**教师-学生方法**，通过合成数据生成高质量的伪标签来优化真实世界的深度图，确保几何细节的准确性，避免了复杂的多任务设置。模型直接预测深度而非依赖视差，提升了几何精度。此外，研究发现模型更新趋向于在预训练模型的特定参数区域内进行，表明了一种深层的、模型引导的优化模式。

#### 应用场景
*   **三维重建：** 从多张图像或视频中重建出精确的三维场景模型。
*   **机器人导航与感知：** 为机器人提供精确的环境深度信息，辅助路径规划和避障。
*   **增强现实 (AR) / 虚拟现实 (VR)：** 实现更逼真的虚拟内容与真实世界的融合，提升沉浸感。
*   **自动驾驶：** 实时感知周围环境的深度信息，辅助车辆进行决策和避险。
*   **电影与游戏制作：** 快速生成高质量的场景深度图，用于特效渲染和三维资产创建。
*   **计算机视觉研究：** 作为基础模型，推动深度估计、场景理解等领域的研究进展。


* 项目官网：https://depth-anything-3.github.io/
* GitHub仓库：https://github.com/ByteDance-Seed/depth-anything-3


## DeepSeek-Math-V2：DeepSeek开源MoE架构数学推理大模型

DeepSeek Math V2 是一个强大的数学推理大型语言模型 (LLM)，基于 DeepSeek-V2 架构开发，旨在高效且准确地解决复杂的数学问题，包括奥林匹克级别的证明题。它具有经济高效的训练和推理特点，在保持高性能的同时显著降低了成本。


![](https://i.postimg.cc/s2ScKgy5/Deep-Seek-Math-V2.png)

#### 核心功能
*   **高精度数学问题求解：** 能够以近99%的准确率解决困难的证明题和奥林匹克级别的数学问题。
*   **多步骤推理与证明生成：** 能够生成详细的、符合逻辑的数学证明步骤。
*   **符号推理与逻辑分析：** 支持复杂的符号推理和逻辑步骤，避免随机快捷方式。
*   **答案验证与迭代优化：** 利用多遍推理和验证器机制，迭代优化证明草稿，直到通过验证。

#### 技术原理
DeepSeek Math V2 构建于 DeepSeek-V2 之上，其核心技术原理包括：
*   **Mixture-of-Experts (MoE) 架构：** DeepSeek-V2 采用 MoE 架构，拥有 236B 总参数，每个 token 激活 21B 参数，实现了训练成本的降低和推理效率的提升。
*   **多遍推理 (Multi-pass Inference) 与验证器 (Verifier)：** 模型生成多个候选证明草稿，并通过一个独立的验证器对每个草稿进行检查。
*   **蒙特卡洛树搜索 (MCTS) 式探索：** 在证明过程中，模型能进行 MCTS 风格的搜索，探索不同的证明路径，并淘汰低分路径，迭代优化。
*   **迭代自举 (Iterative Bootstrapping)：** 通过持续重写和验证其工作，直到验证器批准，实现性能的不断提升。
*   **长上下文处理与高效推理：** 结合了长上下文扩展能力和优化的KV缓存机制，提升了生成吞吐量和效率。
*   **对齐技术：** 采用了监督微调 (SFT) 和强化学习 (RL) 等对齐方法，以确保模型输出的质量和准确性。

#### 应用场景
*   **数学竞赛与学术研究：** 用于竞赛训练、定理证明验证、生成研究辅助内容。
*   **教育与学习辅助：** 生成数学问题的分步解决方案，用于课堂教学解释、辅助学生学习和理解概念。
*   **自动化评估与辅导系统：** 支持自动化数学作业批改、检查长证明的正确性，并构建智能辅导系统。
*   **AI驱动的问题解决：** 赋能AI系统进行精确的数学问题解决和逻辑推理。

* GitHub仓库：https://github.com/deepseek-ai/DeepSeek-Math-V2


## GLM-ASR：智谱AI开源端云协同语音识别模型

智谱AI发布并开源了GLM-ASR系列语音识别模型，旨在提供行业领先的云端及端侧语音识别解决方案。该系列包含GLM-ASR-2512（云端模型）和GLM-ASR-Nano-2512（端侧模型），其中Nano版本为1.5B参数的SOTA开源模型，强调对真实复杂环境的适应性，包括多噪声、多口音、低音量及方言场景，并支持本地部署以增强隐私和降低延迟。

#### 核心功能
*   **高精度识别:** 云端模型GLM-ASR-2512的字符错误率（CER）低至0.0717，达到国际领先水平；端侧模型GLM-ASR-Nano-2512在中文基准测试中表现优于OpenAI Whisper V3，平均错误率4.10。
*   **多场景鲁棒性:** 针对真实复杂环境优化，如嘈杂环境、重叠语音、会议场景以及低音量/耳语语音的识别能力。
*   **方言支持优化:** 专门对中文方言和粤语进行了增强优化，旨在弥补方言识别能力的空白。
*   **自定义词典:** 支持用户导入专业词汇、项目代码、人名地名等，提高特定领域的识别准确率。
*   **云端与端侧部署:** 提供云端API服务和轻量级端侧模型，满足不同部署需求。

#### 技术原理
GLM-ASR系列模型基于深度学习架构，针对语音识别任务进行设计和优化。其中，GLM-ASR-Nano-2512采用1.5B参数，通过特定的训练策略，使其不仅关注理想环境下的低错误率，更注重“从实际使用场景往回推需求”的设计理念。该模型在训练中专门覆盖了多噪声、多口音、低音量（如耳语）以及中文方言（特别是粤语）等复杂语音样本，以增强其在真实世界复杂声学环境下的鲁棒性。其推理支持Hugging Face transformers，并计划支持vLLM和SGLang等推理框架，结合自定义解码逻辑进行前处理和后处理，形成完整的语音识别管线。

#### 应用场景
*   **实时会议纪要:** 实时转录在线会议内容，自动整理结构化摘要，提升办公效率。
*   **客户服务质检与工单管理:** 高精度转录客服通话内容，提升质检效率，支持多场景分析。
*   **直播视频字幕:** 为直播内容提供实时字幕，提升内容可访问性。
*   **智能AI输入法:** 作为智谱AI输入法的核心，实现语音任务化交互，支持语音输入进行翻译、改写、代码编写等。
*   **移动端与远距离拾音应用:** 针对手机、远距离麦克风等设备，解决低音量、弱信号下语音识别的难题。


* GitHub仓库：https://github.com/zai-org/GLM-ASR

## VoxCPM 1.5：面壁智能开源无分词器端到端语音合成模型

VoxCPM 1.5是由面壁智能（ModelBest）推出的先进的端到端文本到语音（TTS）模型。它专注于上下文感知的语音生成和逼真的零样本语音克隆，实现了无分词器（tokenizer-free）的语音合成技术。

![](https://i.postimg.cc/3RFZ5fkk/voxcpm-model.png)

#### 核心功能
*   **上下文感知语音生成**：能够根据文本内容智能推断语调和情感风格。
*   **零样本语音克隆**：实现高度逼真的声音克隆，仅需少量参考音频即可复制目标音色。
*   **跨语言合成**：支持中英双语之间的跨语言语音合成。
*   **端到端语音合成**：提供从文本到语音的完整、流畅的转换过程。
*   **高效推理**：具备RTF 0.17的高效推理性能，确保快速生成高质量语音。

#### 技术原理
VoxCPM 1.5基于MiniCPM-4大语言模型架构，采用层级语言建模（hierarchical language modeling）技术，实现了无分词器的端到端语音合成。该模型通过有效整合文本语义理解和语音特征提取，以支持上下文感知的语音生成。它融合了扩散模型（diffusion models）和Transformer架构的优势，通过局部扩散机制（local diffusion mechanisms）保障音频质量，并确保高效的推理表现。模型在180万小时的双语语料库上进行训练，并针对边缘部署进行了优化。

#### 应用场景
*   **跨语言语音克隆**：适用于需要将特定音色应用于不同语言文本的场景。
*   **情感表达丰富的语音合成**：在需要语音带有情感或特定语气的应用中。
*   **上下文感知内容创作**：如智能助手、有声读物、教育内容等需要语音自然流畅、符合语境的领域。
*   **个性化语音定制**：为用户或品牌提供独特的、高保真的定制化语音。


* GitHub仓库：https://github.com/OpenBMB/VoxCPM

## GLM-TTS：智谱AI开源多奖励强化学习语音合成系统

GLM-TTS是由智谱（Zhipu AI）开发并开源的工业级语音合成系统。它旨在提供高质量、富有表现力的语音输出，并支持音色复刻和多情感表达，是一款基于强化学习的先进文本到语音（TTS）解决方案。

![](https://i.postimg.cc/fbXK5RZq/glm-tts-architecture.png)

#### 核心功能
*   **高质量语音合成：** 能够将文本转换为自然、清晰的语音。
*   **音色复刻（Voice Cloning）：** 支持复刻特定音色，实现个性化语音输出。
*   **多情感表达：** 能够合成带有不同情感（如喜悦、悲伤、愤怒等）的语音，增强表现力。
*   **高精度文本理解：** 具备对文本内容进行深度理解的能力，以生成更准确、语调自然的语音。
*   **零样本语音合成（Zero-shot TTS）：** 能够在没有特定说话者数据的情况下，通过少量提示直接合成新音色语音。

#### 技术原理
GLM-TTS的核心技术基于**多奖励强化学习（Multi-reward Reinforcement Learning）**框架，通过优化多个奖励信号来提升语音合成的自然度和表现力。它可能结合了**深度学习模型**（如Transformer或Diffusion模型）进行声学建模和声码器设计，以实现端到端的高质量语音生成。同时，系统支持**零样本（Zero-shot）**能力，暗示其模型能够从少量语音提示中学习并泛化到未见过的新音色。

#### 应用场景
*   **智能助手与机器人：** 为AI助手、智能客服机器人提供更自然、富有情感的语音交互能力。
*   **有声读物与播客：** 批量生成高质量的有声内容，降低制作成本。
*   **导航系统与公告：** 提供清晰、多变的语音指引和信息播报。
*   **个性化语音定制：** 用于品牌声音、虚拟形象或个人定制的音色复刻服务。
*   **无障碍辅助：** 将文字内容转换为语音，帮助视障人士获取信息。
*   **内容创作与配音：** 为视频、游戏、动漫等提供高效、灵活的配音解决方案。


* GitHub仓库：https://github.com/zai-org/GLM-TTS



# 2.每周大新闻

## Seedream 4.5：字节跳动/火山引擎商业级电影4K图像生成模型

Seedream 4.5（豆包图像创作模型 Doubao-Seedream-4.5）是字节跳动推出、火山引擎发布的新一代AI图像创作模型，现已开启公测。该模型融合了文本生成图像（T2I）和通用编辑功能，在主体一致性、指令遵循精准度、空间逻辑理解和美学表现力方面进行了全面升级，尤其在生成高品质电影级4K视觉效果方面表现突出，推理速度较前代提升超10倍，旨在聚焦商业生产力场景，为广告营销、电商运营、影视制作等行业提供高效智能的视觉创作解决方案。

#### 核心功能
*   **高品质图像生成：** 支持生成电影级4K超高清图像，提升一次成功率，减少重复生成。
*   **主体一致性强化：** 在多图融合与复杂编辑场景下，实现像素级元素识别与提取，确保主体细节、色调高度统一，避免AI合成的拼贴感，支持3D渲染、微缩景观和人像风格转换等。
*   **精确的文本渲染：** 能够准确渲染图像中的小尺寸文字、海报和排版设计中的文本。
*   **指令遵循精准度：** 基于深度语义理解，能精准响应复杂指令，包括艺术风格、技术规格及抽象构图要求，并支持构图、风格及元素位置的精细化调控。
*   **空间逻辑理解：** 内置丰富的世界知识与空间逻辑，能准确把控物体空间落位与透视关系，处理专业需求如物理受力分析图、标准书法篆刻等。
*   **多模态输入与创作：** 支持文本、图像组合输入，实现多图融合创作和复杂图像编辑。
*   **多图组合生成与排版优化：** 强化多源素材融合时的自然感与一致性，优化海报排版与Logo设计功能，支持高精度图文混排。

#### 技术原理
Seedream 4.5 基于多模态大模型架构，其核心技术包括：
*   **高效扩散Transformer与强大VAE：** 构建高效的扩散Transformer（Diffusion Transformer），并结合强大的VAE（Variational AutoEncoder），显著减少图像Token数量，实现高效训练和快速生成原生高分辨率图像。
*   **深度语义理解：** 允许模型精确解析用户输入的复杂文本指令，将其转换为详细的视觉生成参数，从而实现对艺术风格、技术标准和抽象构图等高阶指令的精准响应。
*   **像素级主体识别与提取：** 在多模态融合任务中，模型能够进行精细化的图像元素分析，确保不同源素材在合并时能保持高度的一致性。
*   **空间逻辑推理：** 模型基于对物理世界规则的理解，准确模拟物体的空间位置、透视关系、光影效果和材质纹理，使生成的超现实创意更具真实感。
*   **多模态后训练：** 在数十亿文本-图像对上进行预训练，涵盖多样化分类和知识密集型概念，并通过精心微调的VLM模型进行多模态后训练，以同时支持T2I和图像编辑任务。

#### 应用场景
*   **广告营销：** 生成"成品级"海报、活动物料、波普风杂志封面、活动票务排版等，高效产出视觉素材，减少修改成本。
*   **电商运营：** 商家无需专业影棚即可一键生成媲美商业摄影的产品图，通过多图融合能力，智能合成情景匹配的视觉内容，提升转化率。
*   **影视制作：** 将抽象剧本描述快速可视化为具体的角色设定、场景构图及分镜草图，大幅提升前期开发效率。
*   **虚拟现实与游戏开发：** 生成高分辨率、高真实感的场景、角色和物品纹理。
*   **数字教育：** 将抽象知识可视化，辅助教学内容创作。
*   **建筑设计：** 辅助生成设计效果图，降低视觉创作门槛。



## 可灵2.6：快手首创音画同步生成的AI视频模型

可灵2.6（Kling 2.6）是快手AI团队推出的一款创新AI视频生成模型。它能够将文本描述或静态图片转化为高质量的电影级短视频，并首次实现了音画同步生成，为用户提供了一站式的视频内容创作解决方案。

#### 核心功能
*   **文生视频与图生视频：** 支持通过文本提示或上传图片直接生成视频内容。
*   **音画同步生成：** 首次集成原生音频功能，在一次生成中同时输出画面、自然语音、匹配音效与环境音，告别无声视频。
*   **高保真度与真实感：** 具备更逼真的运动、改进的角色一致性和增强的图像到视频质量。
*   **多模态输入：** 打通了“音”与“画”两个世界，实现了端到端的多模态内容创作。

#### 技术原理
可灵2.6的核心技术原理在于其**音画同步生成**能力，这标志着从传统视觉优先的视频生成模式向**多模态深度语义对齐**的转变。模型能够通过对输入的文本或图像进行**深度语义理解**，进而**端到端**地生成包含视觉元素（如场景、人物动作）和听觉元素（如对话、配乐、环境音效）的完整视频。它利用先进的生成对抗网络（GANs）或扩散模型（Diffusion Models）架构，结合多模态数据训练，实现视频帧与音频波形的精确同步和内容连贯性。

#### 应用场景
*   **商品展示与直播：** 快速生成带解说和背景音乐的商品介绍视频。
*   **生活Vlog与短剧：** 制作具有故事情节、对话和音效的个人Vlog或搞笑短剧。
*   **新闻播报与纪录片：** 生成配有专业解说和背景音的报道或纪实内容。
*   **音乐表演：** 创作带有歌唱、说唱或乐器演奏的音乐视频。
*   **创意广告与影视特效：** 用于品牌宣传、ASMR内容制作或电影片段的快速原型。


## Gemini 3 Deep Think：Google DeepMind并行推理超强逻辑模型

Gemini 3 Deep Think 是 Google DeepMind 推出的一款超强推理模型，旨在解决复杂的数学、科学和逻辑问题。它代表了Gemini模型在推理能力上的重大飞跃，目前已在Gemini应用中面向Ultra订阅用户开放。该模型在多项严格基准测试中表现出色，显著超越了现有最先进的模型，标志着通用人工智能（AGI）发展的重要一步。

#### 核心功能
*   **并行推理能力：** 能够同时探索并处理多个假设，从而在高难度问题中找到最优解决方案。
*   **高级逻辑推理：** 在如ARC-AGI-2等复杂逻辑推理测试中表现卓越，准确率显著领先。
*   **创意编程与生成：** 具备生成复杂程序化内容的能力，包括高保真度3D场景和交互式3D模型。
*   **复杂场景复现：** 能根据简单草图生成精确的3D场景，并模拟真实的光影和物理效果。
*   **多领域专家级处理：** 适用于科学、技术、工程、数学（STEM）等领域的复杂任务，提供专家级处理能力。

#### 技术原理
Gemini 3 Deep Think 的核心技术原理在于其**先进的并行推理能力**。该模型能够并行思考，同时分析和评估多种可能的解决方案路径，而非线性地进行单一路径探索。这种机制使其在处理需要多步逻辑推导和复杂决策的问题时，能够更有效地识别和选择最佳策略。其卓越的性能，如在Humanity’s Last Exam和ARC-AGI-2等基准测试中的高准确率，印证了其强大的逻辑推理和知识整合能力。

#### 应用场景
*   **科学研究与工程设计：** 解决物理、化学、生物学等领域的复杂计算和模拟问题，加速科研进程。
*   **教育与学习辅导：** 辅助学生理解和解决高难度数学、物理和编程问题，提供个性化学习支持。
*   **创意内容生成：** 自动生成复杂的3D模型、程序代码和交互式场景，赋能游戏开发、影视制作和虚拟现实等领域。
*   **高级自动化系统：** 在需要复杂决策和逻辑推理的自动化任务中发挥作用，例如机器人路径规划、智能系统故障诊断等。

## PixVerse V5.5：爱诗科技多模态视频生成与编辑模型

PixVerse V5.5 是一款先进的AI视频生成器，能够将文本、图像或现有视频片段转化为高质量、富有创意且具有流畅动态的短视频。该版本在视频生成质量、功能丰富度和用户控制方面进行了显著提升，旨在为用户提供更强大的视频创作能力。

#### 核心功能
*   **文本到视频生成 (Text-to-Video):** 根据文本提示生成视频片段。
*   **图像到视频生成 (Image-to-Video):** 将静态图片转化为具有自然运动的视频。
*   **视频融合与效果 (Video Fusion & AI Effects):** 提供视频融合能力和多种AI特效。
*   **关键帧控制 (Keyframe Control):** 允许用户对视频生成过程进行更精细的控制。
*   **音频生成与多片段生成 (Audio & Multi-Clip Generation):** 支持生成视频音频和创建多个视频片段。
*   **视频内容延伸 (Video Extension):** 能够分析视频末尾场景并无缝地延续故事内容，扩展视频长度。

#### 技术原理
PixVerse V5.5 核心技术基于深度学习领域的生成式人工智能模型。它可能采用了扩散模型（Diffusion Models）或其他先进的视频生成架构，通过对海量视频数据进行训练，学习如何从文本描述、图像特征或视频上下文信息中合成出逼真的动态画面。
*   **文本/图像编码器：** 将输入的文本提示或图像编码为潜在空间中的向量表示。
*   **视频扩散模型：** 基于编码后的信息，通过迭代去噪过程从随机噪声中逐步生成视频帧序列，确保时间上的一致性和流畅性。
*   **运动合成模块：** 精细控制生成视频中的物体运动、摄像机运镜等，实现自然的动态效果。
*   **上下文感知生成：** 在视频内容延伸功能中，模型会分析现有视频的帧序列和语义信息，预测并生成符合上下文逻辑的后续内容。
*   **多模态融合：** 整合文本、图像、音频等多种输入模态，实现更丰富的视频生成控制和效果。

#### 应用场景
*   **短视频内容创作：** 快速生成社交媒体、短视频平台的创意内容。
*   **广告与营销：** 制作吸引人的产品宣传片或品牌故事视频。
*   **娱乐产业：** 用于游戏开发中的过场动画、电影预可视化或概念验证。
*   **教育与培训：** 制作教学演示或解释性视频。
*   **创意设计：** 帮助设计师和艺术家将静态创意转化为动态视觉作品。
*   **个性化定制：** 根据用户需求快速生成定制化的视频内容。


## 可灵O1：快手全球首个统一多模态视频生成模型


可灵AI是由快手推出的一系列AI创作工具，其中包含“可灵AI国际版”和“可灵O1”模型。可灵AI国际版是一个专注于视频和图像创作的AI工具，提供动态、美学和提示遵循优化，旨在帮助用户快速生成创意内容。可灵O1是可灵AI推出的全球首个统一多模态视频生成模型，通过创新的多模态视觉语言（MVL）架构，实现视频生成、编辑与理解的无缝融合，支持多模态输入，解决视频一致性难题，并提供多种创意组合。

#### 核心功能
*   **统一多模态视频生成与编辑：** 可灵O1提供一站式视频生成、编辑和修改全流程，无需切换工具。
*   **多模态输入与理解：** 支持图片、视频、文字等多种形式的输入，并通过深层语义理解生成或编辑内容。
*   **创意内容生成：** 可灵AI国际版能生成AI图像、视频和声音作品，满足多样化的创意需求。
*   **智能组合与交互：** 支持技能组合使用，如同时增加主体和修改背景，实现高自由度交互编辑。
*   **AI模板与效果：** 可灵AI国际版提供丰富的AI模板和效果，简化创作过程。
*   **虚拟模型与AI换装：** 提供自定义模型、虚拟模型、AI换装等高级功能。

#### 技术原理
可灵O1基于全新的视频生成模型，打破传统视频功能割裂，构建生成式底座，融合了**多模态理解的Multimodal Transformer**和**多模态长上下文（Multimodal Long Context）**。核心技术引入**多模态视觉语言（MVL）**作为交互媒介，通过Transformer实现文本语义与多模态信号的深层融合，支持单一输入框内灵活调用并无缝融合多种任务。模型还结合了**Chain-of-thought（思维链）技术**，具备常识推理与事件推演能力，从而展现出视频生成的智能化表现，在图片参考任务和指令变换任务上均表现出色。

#### 应用场景
*   **社交媒体内容制作：** 快速生成适用于抖音、Instagram等平台的短视频，用于个人分享或品牌营销。
*   **企业宣传与演示：** 制作高质量的企业宣传片、产品展示和活动报道视频，增强企业形象。
*   **专业内容创作：** 帮助创作者在短视频、广告、动画等领域快速实现想法，节省创作时间和精力。
*   **虚拟试穿与购物体验：** 在服装、饰品等行业，用户可通过虚拟试穿功能查看效果，提升购物体验和满意度。
*   **虚拟角色与互动：** 结合虚拟模型、AI换装等功能，应用于虚拟主播、虚拟偶像、游戏角色定制等领域。

# 3. AI-Compass

**AI-Compass** 致力于构建最全面、最实用、最前沿的AI技术学习和实践生态，通过六大核心模块的系统化组织，为不同层次的学习者和开发者提供从完整学习路径。

* github地址：[AI-Compass👈：https://github.com/tingaicompass/AI-Compass](https://github.com/tingaicompass/AI-Compass)
* gitee地址：[AI-Compass👈：https://gitee.com/tingaicompass/ai-compass](https://gitee.com/tingaicompass/ai-compass)


🌟 如果本项目对您有所帮助，请为我们点亮一颗星！🌟

### 📋 核心模块架构：
- **🧠 基础知识模块**：涵盖AI导航工具、Prompt工程、LLM测评、语言模型、多模态模型等核心理论基础
- **⚙️ 技术框架模块**：包含Embedding模型、训练框架、推理部署、评估框架、RLHF等技术栈
- **🚀 应用实践模块**：聚焦RAG+workflow、Agent、GraphRAG、MCP+A2A等前沿应用架构
- **🛠️ 产品与工具模块**：整合AI应用、AI产品、竞赛资源等实战内容
- **🏢 企业开源模块**：汇集华为、腾讯、阿里、百度飞桨、Datawhale等企业级开源资源
- **🌐 社区与平台模块**：提供学习平台、技术文章、社区论坛等生态资源

### 📚 适用人群：
- **AI初学者**：提供系统化的学习路径和基础知识体系，快速建立AI技术认知框架
- **技术开发者**：深度技术资源和工程实践指南，提升AI项目开发和部署能力
- **产品经理**：AI产品设计方法论和市场案例分析，掌握AI产品化策略
- **研究人员**：前沿技术趋势和学术资源，拓展AI应用研究边界
- **企业团队**：完整的AI技术选型和落地方案，加速企业AI转型进程
- **求职者**：全面的面试准备资源和项目实战经验，提升AI领域竞争力