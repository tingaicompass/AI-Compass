# 2.RLHF

RLHF模块构建了完整的人类反馈强化学习技术栈，集成前沿的偏好优化和人类对齐框架。**核心框架**包括：**Huggingface TRL**（Transformer强化学习标准库，PPO训练详解）、**OpenRLHF**（易用可扩展RLHF框架，支持70B+ PPO全量微调、迭代DPO、LoRA和RingAttention）、**字节veRL**（火山引擎强化学习框架，工业级部署）、**EasyR1**（基于veRL的高效多模态RL训练框架）。**创新技术**融入**通义WorldPM**（72B参数的世界偏好模型，引领偏好建模新范式）等前沿研究成果。技术覆盖从PPO（Proximal Policy Optimization）算法实现、DPO（Direct Preference Optimization）直接偏好优化，到GRPO等先进算法，支持全参数微调、LoRA高效微调等多种训练模式，为大模型的人类价值对齐提供从理论到实践的完整解决方案。

- [PPO训练详解](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html)
- [TRL-huggingface-使用强化学习训练transformer语言模型。](https://github.com/huggingface/trl)

------------------------------------------------------------

1. 2.RLHF.md
2. 1.EasyR1
3. 1.OpenRLHF
4. 1.WorldPM
5. 1.verl


# 1.EasyR1

- [hiyouga/EasyR1: EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework based on veRL](https://github.com/hiyouga/EasyR1)

------------------------------------------------------------

# 1.OpenRLHF

- [OpenRLHF/OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework (70B+ PPO Full Tuning & Iterative DPO & LoRA & RingAttention)](https://github.com/OpenRLHF/OpenRLHF)

------------------------------------------------------------

# 1.WorldPM

- [WorldPM/README_CN.md at main · QwenLM/WorldPM](https://github.com/QwenLM/WorldPM/blob/main/README_CN.md)
- [QwenLM/WorldPM](https://github.com/QwenLM/WorldPM)
- [Qwen/WorldPM-72B · Hugging Face](https://huggingface.co/Qwen/WorldPM-72B)
- [2505.10527](https://arxiv.org/pdf/2505.10527)

------------------------------------------------------------

# 1.verl

- [volcengine/verl: verl: Volcano Engine Reinforcement Learning for LLMs](https://github.com/volcengine/verl)
- [Welcome to verl’s documentation! — verl documentation](https://verl.readthedocs.io/en/latest/)

------------------------------------------------------------

**[⬆ 返回README目录](../README.md#目录)**
**[⬆ Back to Contents](../README-EN.md#contents)**