# 2.RLHF

RLHF模块构建了完整的人类反馈强化学习技术栈，集成前沿的偏好优化和人类对齐框架。**核心框架**包括：**Huggingface TRL**（Transformer强化学习标准库，PPO训练详解）、**OpenRLHF**（易用可扩展RLHF框架，支持70B+ PPO全量微调、迭代DPO、LoRA和RingAttention）、**字节veRL**（火山引擎强化学习框架，工业级部署）、**EasyR1**（基于veRL的高效多模态RL训练框架）。**创新技术**融入**通义WorldPM**（72B参数的世界偏好模型，引领偏好建模新范式）等前沿研究成果。技术覆盖从PPO（Proximal Policy Optimization）算法实现、DPO（Direct Preference Optimization）直接偏好优化，到GRPO等先进算法，支持全参数微调、LoRA高效微调等多种训练模式，为大模型的人类价值对齐提供从理论到实践的完整解决方案。

- [PPO训练详解](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html)
- [TRL-huggingface-使用强化学习训练transformer语言模型。](https://github.com/huggingface/trl)

------------------------------------------------------------


# 1.EasyR1

#### 简介
EasyR1是一个高效、可扩展的多模态强化学习（RL）训练框架，基于veRL项目改进以支持视觉语言模型。它借助HybirdEngine设计和vLLM的SPMD模式实现高效扩展，支持多种模型、算法、数据集及训练技巧。

#### 核心功能
- **支持多类型模型**：涵盖Llama3、Qwen2等语言模型，Qwen2-VL等视觉语言模型及DeepSeek-R1蒸馏模型。
- **多种算法支持**：支持GRPO、Reinforce++、ReMax、RLOO等多种RL算法。
- **适配多格式数据集**：可处理特定格式的文本、图像-文本和多图像-文本数据集。
- **训练技巧丰富**：提供Padding-free训练、从检查点恢复、并支持Wandb、SwanLab、Mlflow和Tensorboard等训练过程跟踪工具。

#### 技术原理
EasyR1的核心技术原理在于其对原有veRL项目的继承与优化，特别体现在以下两点：
- HybridEngine架构：该框架得益于veRL的HybridEngine设计，实现了高效的训练流程。HybridEngine是一种混合引擎，旨在提高强化学习训练的灵活性和效率。
- vLLM的SPMD模式：结合了vLLM 0.7最新版本的SPMD（Single Program, Multiple Data）模式，有效提升了大规模语言模型和多模态模型的训练性能和可扩展性，尤其对于内存管理和并行计算提供了优化。

#### 应用场景
- 多模态大模型的强化学习训练：特别适用于训练Qwen2.5-VL等多模态模型，以提升其在特定任务上的表现。
- 数学问题求解：可用于训练数学模型，例如在Geometry3k数据集上进行几何问题求解。
- 视觉问答与推理：可应用于图像-文本相关的视觉推理任务，如CLEVR-70k-Counting和GeoQA-8k数据集上的任务。
- 研究与开发：为研究人员和开发者提供一个强大的平台，探索新的RL算法和多模态模型的训练方法，并集成实验跟踪和可视化工具。


- [hiyouga/EasyR1: EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework based on veRL](https://github.com/hiyouga/EasyR1)

------------------------------------------------------------

# 1.OpenRLHF

#### 简介
OpenRLHF是首个基于Ray、vLLM、ZeRO - 3和HuggingFace Transformers构建的易于使用、高性能的开源RLHF框架，具有分布式架构、推理加速、内存高效训练等特点，支持多种算法和功能。

#### 核心功能
- 分布式训练：利用Ray进行高效分布式调度，支持多模型分离到不同GPU。
- 推理加速：结合vLLM和AutoTP实现高吞吐量、内存高效的样本生成。
- 多算法支持：实现分布式PPO、REINFORCE++等多种算法。
- 数据处理：提供多种数据处理方法，支持混合数据集。
- 模型训练：支持监督微调、奖励模型训练、PPO训练等。
- 异步训练：支持异步RLHF和基于代理的RLHF。

#### 技术原理
- 分布式架构：借助Ray进行任务调度，分离不同模型到不同GPU，支持混合引擎调度以提高GPU利用率。
- 推理加速：基于vLLM和AutoTP，减少样本生成时间，与HuggingFace Transformers集成实现快速生成。
- 内存优化：基于DeepSpeed的ZeRO - 3、deepcompile和AutoTP，直接与HuggingFace配合进行大模型训练。
- 算法优化：采用优化的PPO实现，结合实用技巧提升训练稳定性和奖励质量。

#### 应用场景
- 大语言模型微调：对大型语言模型进行监督微调、奖励模型训练和强化学习微调。
- 多智能体系统训练：如MARTI利用其训练基于LLM的多智能体系统。
- 多模态任务：为多模态任务提供高性能RL基础设施，如LMM - R1。 


- [OpenRLHF/OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework (70B+ PPO Full Tuning & Iterative DPO & LoRA & RingAttention)](https://github.com/OpenRLHF/OpenRLHF)

------------------------------------------------------------

# 1.WorldPM

#### 简介
WorldPM（世界偏好建模）证明了偏好建模遵循与语言建模类似的扩展规律，通过对1500万条来自StackExchange的偏好数据进行大规模训练，让偏好模型学习统一的偏好表示。在对抗性和客观评估中表现出明显扩展趋势，对抗性评估测试损失幂律下降，客观指标有涌现现象；主观评估无明显扩展趋势，可能受风格偏好影响。

#### 核心功能
- **偏好学习**：从大规模偏好数据中学习统一的人类偏好表示。
- **对抗评估**：提升识别包含意图错误、不相关或不完整回复的能力。
- **客观指标评估**：助力大模型获取客观知识偏好，展现出测试损失的幂律下降。
- **基础与微调**：提供基础模型WorldPM - 72B，并支持在不同规模数据集上微调。

#### 技术原理
- **数据收集**：从StackExchange、Reddit、Quora等公共论坛收集偏好数据，经评估选StackExchange数据为代表。
- **训练方法**：遵循人类偏好建模框架，用偏好模型预测奖励并优化BT - loss，不同规模模型保持一致超参数。
- **评估方法**：用BT - loss计算测试性能，使用多个RM基准的不同领域测试集评估。

#### 应用场景
- **模型微调**：作为基础助力不同规模人类偏好数据集的偏好模型微调。
- **语言模型对齐**：通过Best - of - N采样使语言模型输出符合人类偏好，在Arena Hard和Alpaca Eval等基准评估。 


- [WorldPM/README_CN.md at main · QwenLM/WorldPM](https://github.com/QwenLM/WorldPM/blob/main/README_CN.md)
- [QwenLM/WorldPM](https://github.com/QwenLM/WorldPM)
- [Qwen/WorldPM-72B · Hugging Face](https://huggingface.co/Qwen/WorldPM-72B)
- [2505.10527](https://arxiv.org/pdf/2505.10527)

------------------------------------------------------------

# 1.verl

#### 简介
verl是由字节跳动Seed团队发起、verl社区维护的强化学习训练库，是HybridFlow论文的开源版本。它灵活高效、适用于生产，用于大语言模型（LLM）的后训练，能与多种现有LLM框架集成，支持多种强化学习算法。

#### 核心功能
- **算法扩展**：可轻松扩展多种强化学习算法，如PPO、GRPO等。
- **框架集成**：通过模块化API与现有LLM框架无缝集成，支持FSDP、Megatron - LM等训练框架，vLLM、SGLang等推理框架。
- **设备映射**：支持将模型灵活放置在不同GPU集上，实现高效资源利用和集群扩展。
- **多类型支持**：支持基于模型和函数的奖励、视觉语言模型（VLM）和多模态强化学习、多轮对话及工具调用等。
- **性能优化**：具有先进的吞吐量，通过3D - HybridEngine实现高效的actor模型重分片。

#### 技术原理
- **混合编程模型**：结合单控制器和多控制器范式的优势，灵活表示和高效执行复杂的后训练数据流。
- **3D - HybridEngine**：在训练和生成阶段的转换中消除内存冗余，显著减少通信开销。
- **模块化设计**：解耦计算和数据依赖，便于与现有LLM框架集成。

#### 应用场景
- **大语言模型训练**：如对DeepSeek - 671b、Qwen3 - 236b等大模型进行强化学习训练。
- **代码生成与数学推理**：在编码、数学等领域进行模型训练，提升模型在这些任务上的表现。
- **视觉语言模型**：支持Qwen2.5 - vl、Kimi - VL等视觉语言模型的多模态强化学习。
- **多轮对话与工具调用**：实现大语言模型的多轮对话及工具调用功能。 


- [volcengine/verl: verl: Volcano Engine Reinforcement Learning for LLMs](https://github.com/volcengine/verl)
- [Welcome to verl’s documentation! — verl documentation](https://verl.readthedocs.io/en/latest/)



# Skywork-Reward-V2

#### 简介
Skywork-Reward-V2 是昆仑万维（SkyworkAI）开源的第二代奖励模型系列，旨在为大型语言模型（LLMs）提供卓越的人类偏好评估能力。该系列包含八个不同参数规模（从6亿到80亿）的模型，通过大规模、高质量的偏好数据训练，在多项主流奖励模型评测榜单上取得了领先的性能，刷新了State-of-the-Art (SOTA) 记录，成为目前最强的人类偏好感应器之一。

#### 核心功能
*   **人类偏好感知与量化：** 精准识别、理解并量化人类对LLM生成内容的偏好程度，为模型输出质量提供客观依据。
*   **强化学习奖励信号生成：** 作为强化学习从人类反馈中学习（RLHF）的关键组件，提供高质量、稳定的奖励信号，以有效指导LLMs的行为对齐人类价值观和指令。
*   **多任务通用性评估：** 能够评估LLMs在广泛任务（如对话、摘要、写作等）中的表现，支持多领域和多场景的应用需求。

#### 技术原理
Skywork-Reward-V2系列模型核心基于**Bradley-Terry模型**进行训练，该模型擅长处理配对比较数据，以推断个体偏好。其技术亮点在于：
*   **大规模高质量偏好数据：** 模型在高达2600万对经过精心策划的高质量人类偏好数据上进行训练，显著提升了模型的泛化能力和准确性。
*   **数据驱动的性能优化：** 通过强调数据规模和质量的重要性，采用了先进的数据选择和过滤策略，确保训练数据的有效性和代表性。
*   **模型架构多样性：** 提供不同参数量级的模型，以满足不同应用场景下对计算资源和性能的需求平衡。
*   **强化学习与偏好学习结合：** 通过将人类反馈转化为奖励信号，驱动LLM在迭代优化中逐步学习并适应人类的复杂偏好模式，实现与人类意图的高度对齐。

#### 应用场景
*   **大型语言模型（LLMs）对齐：** 在RLHF流程中作为奖励函数，用于微调LLMs，使其生成内容更符合人类偏好、更安全、更无害。
*   **内容生成质量评估：** 自动评估由LLMs生成的文本内容（如对话回复、文章摘要、创意文案等）的质量、连贯性和相关性。
*   **对话系统优化：** 提高聊天机器人和虚拟助手的对话质量和用户满意度，使其能够生成更自然、更具吸引力的回复。
*   **个性化推荐系统：** 根据用户偏好对生成的内容或信息进行排序和过滤，提升推荐的准确性和用户体验。
*   **模型效果迭代与对比：** 作为衡量不同LLM版本或训练策略效果的基准，指导模型持续改进。


[Skywork-Reward-V2](https://github.com/SkyworkAI/Skywork-Reward-V2)

[huggingface](https://huggingface.co/collections/Skywork/skywork-reward-v2-685cc86ce5d9c9e4be500c84)

[arxiv](https://arxiv.org/pdf/2507.01352)


------------------------------------------------------------

**[⬆ 返回README目录](../README.md#目录)**
**[⬆ Back to Contents](../README-EN.md#contents)**